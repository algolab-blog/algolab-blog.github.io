<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ALGO GEEKS</title>
    <link>http://blog.algolab.jp/</link>
    <description>Recent content on ALGO GEEKS</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Wed, 31 Aug 2016 14:39:10 +0900</lastBuildDate>
    <atom:link href="http://blog.algolab.jp/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Kaldiで音声を学習させる 〜ディープラーニングを用いた音声認識ツールキット〜</title>
      <link>http://blog.algolab.jp/post/2016/08/31/kaldi/</link>
      <pubDate>Wed, 31 Aug 2016 14:39:10 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/08/31/kaldi/</guid>
      <description>

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/31/kaldi//kaldi_text_and_logo.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;h2 id=&#34;kaldiとは&#34;&gt;Kaldiとは&lt;/h2&gt;

&lt;p&gt;C++で書かれた音声認識ツールキットで、Apache Licence 2.0で公開されています。&lt;br /&gt;
音響モデルにDNN (Deep Neural Network) を用いているのが特長です。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://kaldi-asr.org/&#34;&gt;http://kaldi-asr.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;今回はKaldiを動作させ、yesかnoの音声を判別するモデルを学習させてみます。&lt;/p&gt;

&lt;h2 id=&#34;環境&#34;&gt;環境&lt;/h2&gt;

&lt;p&gt;Vagrant上のUbuntu 16.04 LTSを用いています。
&lt;pre&gt;&lt;code class=&#34;language-sh hljs bash&#34;&gt;$ cat /etc/lsb-release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=&lt;span class=&#34;hljs-number&#34;&gt;16.04&lt;/span&gt;
DISTRIB_CODENAME=xenial
DISTRIB_DESCRIPTION=&lt;span class=&#34;hljs-string&#34;&gt;&#34;Ubuntu 16.04.1 LTS&#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh hljs bash&#34;&gt;$ uname &lt;span class=&#34;hljs-operator&#34;&gt;-a&lt;/span&gt;
Linux vagrant &lt;span class=&#34;hljs-number&#34;&gt;4.4&lt;/span&gt;.&lt;span class=&#34;hljs-number&#34;&gt;0&lt;/span&gt;-&lt;span class=&#34;hljs-number&#34;&gt;31&lt;/span&gt;-generic &lt;span class=&#34;hljs-comment&#34;&gt;#50-Ubuntu SMP Wed Jul 13 00:07:12 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/p&gt;

&lt;h2 id=&#34;kaldiのダウンロード&#34;&gt;Kaldiのダウンロード&lt;/h2&gt;

&lt;p&gt;Githubよりダウンロードします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ git clone https://github.com/kaldi-asr/kaldi.git
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;インストール&#34;&gt;インストール&lt;/h2&gt;

&lt;p&gt;インストール方法は&lt;code&gt;INSTALL&lt;/code&gt;ファイルに最新情報が記載されているので、それに従います。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ cd kaldi
$ cat INSTALL
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;This is the official Kaldi INSTALL. Look also at INSTALL.md for the git mirror installation.&lt;br /&gt;
[for native Windows install, see windows/INSTALL]&lt;/p&gt;

&lt;p&gt;(1)&lt;br /&gt;
go to tools/  and follow INSTALL instructions there.&lt;/p&gt;

&lt;p&gt;(2)&lt;br /&gt;
go to src/ and follow INSTALL instructions there.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code&gt;tools&lt;/code&gt;および&lt;code&gt;src&lt;/code&gt;フォルダの&lt;code&gt;INSTALL&lt;/code&gt;を見れば良いようなので、まず&lt;code&gt;tools&lt;/code&gt;から確認していきます。&lt;/p&gt;

&lt;h2 id=&#34;toolsのインストール&#34;&gt;toolsのインストール&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ cd tools
$ cat INSTALL
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;To install the most important prerequisites for Kaldi:&lt;/p&gt;

&lt;p&gt;&amp;nbsp;first do&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&amp;nbsp;extras/check_dependencies.sh&lt;/p&gt;

&lt;p&gt;to see if there are any system-level installations or modifications you need to do.&lt;br /&gt;
Check the output carefully: there are some things that will make your life a lot&lt;br /&gt;
easier if you fix them at this stage.&lt;/p&gt;

&lt;p&gt;Then run&lt;/p&gt;

&lt;p&gt;&amp;nbsp;make&lt;/p&gt;

&lt;p&gt;If you have multiple CPUs and want to speed things up, you can do a parallel&lt;br /&gt;
build by supplying the &amp;ldquo;-j&amp;rdquo; option to make, e.g. to use 4 CPUs:&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&amp;nbsp;make -j 4&lt;/p&gt;

&lt;p&gt;By default, Kaldi builds against OpenFst-1.3.4. If you want to build against&lt;br /&gt;
OpenFst-1.4, edit the Makefile in this folder. Note that this change requires&lt;br /&gt;
a relatively new compiler with C++11 support, e.g. gcc &amp;gt;= 4.6, clang &amp;gt;= 3.0.&lt;/p&gt;

&lt;p&gt;In extras/, there are also various scripts to install extra bits and pieces that&lt;br /&gt;
are used by individual example scripts.  If an example script needs you to run&lt;br /&gt;
one of those scripts, it will tell you what to do.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;概要は以下の通りです。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;extras/check_dependencies.sh&lt;/code&gt;で依存関係をチェックする&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make&lt;/code&gt;コマンドでインストールを行う

&lt;ul&gt;
&lt;li&gt;マルチコアのCPUの場合は&lt;code&gt;j&lt;/code&gt;オプションをつけることでインストールが並列化できる (早くなる)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;依存関係のチェック&#34;&gt;依存関係のチェック&lt;/h3&gt;

&lt;p&gt;スクリプトを用いて依存関係をチェックします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ extras/check_dependencies.sh

extras/check_dependencies.sh: automake is not installed.
extras/check_dependencies.sh: autoconf is not installed.
extras/check_dependencies.sh: neither libtoolize nor glibtoolize is installed
extras/check_dependencies.sh: subversion is not installed
extras/check_dependencies.sh: default or create an bash alias for kaldi scripts to run correctly
extras/check_dependencies.sh: we recommend that you run (our best guess):
 sudo apt-get install  automake autoconf libtool subversion
You should probably do:
 sudo apt-get install libatlas3-base
/bin/sh is linked to dash, and currently some of the scripts will not run
properly.  We recommend to run:
 sudo ln -s -f bash /bin/sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;サジェストされた通りに進めます。&lt;br /&gt;
(環境によって出てくるメッセージが異なるのでご注意下さい)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ sudo apt-get install automake autoconf libtool subversion
$ sudo apt-get install -y libatlas3-base
$ sudo ln -s -f bash /bin/sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再度依存関係をチェックすると、OKとなりました。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ extras/check_dependencies.sh
extras/check_dependencies.sh: all OK.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;インストール-1&#34;&gt;インストール&lt;/h3&gt;

&lt;p&gt;まず、手元の環境のCPUコア数を調べます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ nproc
4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;筆者の環境は4コアだったので、&lt;code&gt;j&lt;/code&gt;オプションを用いて並列インストールを行います。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ sudo make -j 4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下のライブラリがインストールされます。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;OpenFst

&lt;ul&gt;
&lt;li&gt;重み付き有限状態トランスデューサー (WFST) を扱うライブラリ&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;sph2pipe

&lt;ul&gt;
&lt;li&gt;SPHEREファイルのコンバータ&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;sclite

&lt;ul&gt;
&lt;li&gt;音声認識結果をスコアリングするためのライブラリ&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;ATLAS

&lt;ul&gt;
&lt;li&gt;線形代数ライブラリ&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;CLAPACK

&lt;ul&gt;
&lt;li&gt;線形代数ライブラリ&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;オプション-言語モデルツールキットのインストール&#34;&gt;(オプション) 言語モデルツールキットのインストール&lt;/h3&gt;

&lt;p&gt;また、言語モデルのツールキット (IRSTLM や SRILM) を使用する場合は追加でインストールします。&lt;/p&gt;

&lt;h4 id=&#34;irstlm&#34;&gt;IRSTLM&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ extras/install_irstlm.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;srlm&#34;&gt;SRLM&lt;/h4&gt;

&lt;p&gt;下記からファイルをダウンロードし、&lt;code&gt;srilm.tgz&lt;/code&gt;というファイル名にした上で、&lt;code&gt;tools/&lt;/code&gt;直下に配置します。&lt;br /&gt;
&lt;a href=&#34;http://www.speech.sri.com/projects/srilm/download.html&#34;&gt;http://www.speech.sri.com/projects/srilm/download.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;また、インストールにはGNU awkが必要なので導入します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ sudo apt-get install -y gawk
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;本体をインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ extras/install_srilm.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;srcのインストール&#34;&gt;srcのインストール&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ cd ../src
$ cat INSTALL
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;These instructions are valid for UNIX-like systems (these steps have&lt;br /&gt;
been run on various Linux distributions; Darwin; Cygwin).  For native Windows&lt;br /&gt;
compilation, see ../windows/INSTALL.&lt;/p&gt;

&lt;p&gt;You must first have completed the installation steps in ../tools/INSTALL&lt;br /&gt;
(compiling OpenFst; getting ATLAS and CLAPACK headers).&lt;/p&gt;

&lt;p&gt;The installation instructions are:&lt;br /&gt;
./configure&lt;br /&gt;
make depend&lt;br /&gt;
make&lt;/p&gt;

&lt;p&gt;Note that &amp;ldquo;make&amp;rdquo; takes a long time; you can speed it up by running make&lt;br /&gt;
in parallel if you have multiple CPUs, for instance&lt;br /&gt;
 make depend -j 8&lt;br /&gt;
 make -j 8&lt;br /&gt;
For more information, see documentation at &lt;a href=&#34;http://kaldi-asr.org/doc/&#34;&gt;http://kaldi-asr.org/doc/&lt;/a&gt;&lt;br /&gt;
and click on &amp;ldquo;The build process (how Kaldi is compiled)&amp;rdquo;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;以下の3つのコマンドを叩けば良いようなので、一つずつ叩いていきます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ ./configure
$ sudo make depend -j 4
$ sudo make -j 4
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;サンプルの動作確認&#34;&gt;サンプルの動作確認&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;egs&lt;/code&gt;以下にサンプルが公開されています。&lt;br /&gt;
ここでは、&lt;code&gt;yes&lt;/code&gt;と&lt;code&gt;no&lt;/code&gt;を判別する非常に小さなタスクを学習させてみます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd ../egs/yesno
cat README.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;The &amp;ldquo;yesno&amp;rdquo; corpus is a very small dataset of recordings of one individual&lt;br /&gt;
saying yes or no multiple times per recording, in Hebrew.  It is available from&lt;br /&gt;
&lt;a href=&#34;http://www.openslr.org/1&#34;&gt;http://www.openslr.org/1&lt;/a&gt;.&lt;br /&gt;
It is mainly included here as an easy way to test out the Kaldi scripts.&lt;/p&gt;

&lt;p&gt;The test set is perfectly recognized at the monophone stage, so the dataset is&lt;br /&gt;
not exactly challenging.&lt;/p&gt;

&lt;p&gt;The scripts are in s5/.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ヘブライ語で&lt;code&gt;yes&lt;/code&gt;と&lt;code&gt;no&lt;/code&gt;を喋っているコーパスを学習データとして用いるようです。&lt;br /&gt;
&lt;code&gt;s5&lt;/code&gt;フォルダに動作用のスクリプトがあるので、動かしてみます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ cd s5
$ sh run.sh
...
%WER 0.00 [ 0 / 232, 0 ins, 0 del, 0 sub ] exp/mono0a/decode_test_yesno/wer_10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;WER (単語誤り率) が 0% という結果となりました。&lt;/p&gt;

&lt;h2 id=&#34;次回予告&#34;&gt;次回予告&lt;/h2&gt;

&lt;p&gt;次回はサンプルのソースコードを追ってみたいと思います。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Alexa Skills KitをAWS Lamdaから使う</title>
      <link>http://blog.algolab.jp/post/2016/08/29/alexa-skills-kit/</link>
      <pubDate>Mon, 29 Aug 2016 16:10:04 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/08/29/alexa-skills-kit/</guid>
      <description>

&lt;p&gt;こちらの記事の続きとなります。&lt;br /&gt;
&lt;a href=&#34;http://blog.algolab.jp/post/2016/08/11/raspberry-pi-alexa/&#34;&gt;Amazon Echoを6,000円で自作する 〜Raspberry Pi 3 + Alexa Voice Services (AVS)〜&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;前回はRaspberry PiからAVS (Alexa Voice Services) を使ってみましたが、今回は、Alexa Skills Kitを使ってみたいと思います。&lt;/p&gt;

&lt;h2 id=&#34;alexa-skill-kitとは&#34;&gt;Alexa Skill Kitとは&lt;/h2&gt;

&lt;p&gt;AVSには好みの機能を追加できるSkillという機能があり、「カスタムスキル」と「スマートホームスキル」の2種類を登録することができます。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/understanding-the-different-types-of-skills&#34;&gt;https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/understanding-the-different-types-of-skills&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;カスタムスキル&#34;&gt;カスタムスキル&lt;/h3&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//custom-skill.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;ul&gt;
&lt;li&gt;ピザを注文したり、タクシーを呼んだり色々なことができる&lt;/li&gt;
&lt;li&gt;Invocation Name (スキルの呼び名) で呼び出す&lt;/li&gt;
&lt;li&gt;リクエストは「intent」としてマッピングされる

&lt;ul&gt;
&lt;li&gt;ピザの注文 &amp;rarr; OrderPizza intent&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;スマートホームスキル&#34;&gt;スマートホームスキル&lt;/h3&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//smart-home-skill.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;ul&gt;
&lt;li&gt;Smart home device (灯りやエアコンなど) を操作できる&lt;/li&gt;
&lt;li&gt;Invocation Nameで呼び出すのは不要&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;今回作るもの&#34;&gt;今回作るもの&lt;/h2&gt;

&lt;p&gt;公式の &lt;a href=&#34;https://developer.amazon.com/appsandservices/solutions/alexa/alexa-skills-kit/docs/developing-an-alexa-skill-as-a-lambda-function&#34;&gt;ドキュメント&lt;/a&gt; と &lt;a href=&#34;https://developer.amazon.com/public/community/post/TxDJWS16KUPVKO/New-Alexa-Skills-Kit-Template-Build-a-Trivia-Skill-in-under-an-Hour&#34;&gt;ポスト&lt;/a&gt; を参考に、今回は「Color Expert」のSkillを使ってみます。&lt;br /&gt;
Alexa SkillsはLambdaファンクション上で実行されるので、AWS LambdaとAlexa Skillsの設定が必要になります。&lt;/p&gt;

&lt;h2 id=&#34;aws-lambdaの作成&#34;&gt;AWS Lambdaの作成&lt;/h2&gt;

&lt;p&gt;AWSマネジメントコンソールにログインし、&lt;a href=&#34;https://console.aws.amazon.com/lambda/home&#34;&gt;Lambda&lt;/a&gt; のページを開きます。&lt;/p&gt;

&lt;p&gt;リージョンがバージニア北部(US East (N. Virginia))になっていることを確認し、なっていなければ変更します。Lambdaファンクションを利用してAlexa Skillsを使うのに、現在他のリージョンはサポートされていません。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-1.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;&lt;code&gt;Create a Lambda function&lt;/code&gt;をクリックするとBlueprint一覧画面になります。ここから&lt;code&gt;alexa-skills-kit-color-expert&lt;/code&gt;を選択します。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-2.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;Lambdaファンクションを呼び出すトリガーの選択画面になるので、灰色の点線のボックスをクリックし、&lt;code&gt;Alexa Skills Kit&lt;/code&gt;を選び&lt;code&gt;Next&lt;/code&gt;をクリックします。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-3.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;Lambdaファンクションの構成画面になります。Nameには「colorExpertTest」などと入力します。&lt;/p&gt;

&lt;p&gt;RoleにはLambdaを使うのが初めてであれば、&lt;code&gt;Create new role from template(s)&lt;/code&gt;から新しくRoleを作成し、Role Nameには「lambda_basic_execution」などと入力します。&lt;/p&gt;

&lt;p&gt;Policy templatesには&lt;code&gt;AMI read-only permissions&lt;/code&gt;などを選択すればOKです。&lt;/p&gt;

&lt;p&gt;Lambda function codeなど他の項目はデフォルトのままでも問題ありません。&lt;/p&gt;

&lt;p&gt;一通り入力・変更が終わったら&lt;code&gt;Next&lt;/code&gt;をクリックします。&lt;/p&gt;

&lt;p&gt;そうすると、下記のような確認画面になります。問題なければ&lt;code&gt;Create function&lt;/code&gt;をクリックします。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-4.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;トリガーのテスト画面になります。念のため&lt;code&gt;Test&lt;/code&gt;をクリックすると、デフォルトのテストであるAlexa Start Sessionが走ります。実行結果がSuceededとなること、ログ出力に先ほどのLambda function codeの出力結果が表示されていればOKです。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-5.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;これで作成は完了です。最後にLambdaファンクションの呼び出し先となるARNをメモしておきます。上記スクリーンショットで右上の一部灰色でマスクしている文字列です。&lt;/p&gt;

&lt;h2 id=&#34;alexa-skillの作成&#34;&gt;Alexa Skillの作成&lt;/h2&gt;

&lt;p&gt;Raspberry Piが登録されているアカウントでAmazon Developer Consoleにログインし、&lt;a href=&#34;https://developer.amazon.com/edw/home.html&#34;&gt;Alexa&lt;/a&gt; のページに進みます。&lt;/p&gt;

&lt;p&gt;Alexa Skills Kitの&lt;code&gt;Get Started&lt;/code&gt;をクリックします。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-6.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;&lt;code&gt;Add a New Skill&lt;/code&gt;から新規にSkillを登録します。実際に話しかけて呼び出すときの名前となるInvocation Nameには「color expert」と入力して、&lt;code&gt;Next&lt;/code&gt;をクリックします。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-7.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;Interaction Modelの定義画面になります。これがAlexaに話しかけてやり取りをする内容になります。今回は&lt;a href=&#34;https://developer.amazon.com/appsandservices/solutions/alexa/alexa-skills-kit/docs/developing-an-alexa-skill-as-a-lambda-function&#34;&gt;公式ドキュメント&lt;/a&gt;のとおりにIntent Schame, Custom Slot Types, Sample Utterancesを下記のようにします。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-8.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;Intent_Schema&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  &amp;quot;intents&amp;quot;: [
    {
      &amp;quot;intent&amp;quot;: &amp;quot;MyColorIsIntent&amp;quot;,
      &amp;quot;slots&amp;quot;: [
        {
          &amp;quot;name&amp;quot;: &amp;quot;Color&amp;quot;,
          &amp;quot;type&amp;quot;: &amp;quot;LIST_OF_COLORS&amp;quot;
        }
      ]
    },
    {
      &amp;quot;intent&amp;quot;: &amp;quot;WhatsMyColorIntent&amp;quot;
    },
    {
      &amp;quot;intent&amp;quot;: &amp;quot;AMAZON.HelpIntent&amp;quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Custom_Slot_Type&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;LIST_OF_COLORS
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Custom_Slot_Type_Values&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;green
red
blue
orange
gold
silver
yellow
black
white
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sample_Utterances&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;WhatsMyColorIntent what&#39;s my favorite color
WhatsMyColorIntent what is my favorite color
WhatsMyColorIntent what&#39;s my color
WhatsMyColorIntent what is my color
WhatsMyColorIntent my color
WhatsMyColorIntent my favorite color
WhatsMyColorIntent get my color
WhatsMyColorIntent get my favorite color
WhatsMyColorIntent give me my favorite color
WhatsMyColorIntent give me my color
WhatsMyColorIntent what my color is
WhatsMyColorIntent what my favorite color is
WhatsMyColorIntent yes
WhatsMyColorIntent yup
WhatsMyColorIntent sure
WhatsMyColorIntent yes please
MyColorIsIntent my favorite color is {Color}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;次にEndpointなどの設定画面になります。先ほどメモしておいたARNを入力します。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-9.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;次にTest画面になります。Enter Utteranceに先ほどSample Utteranceに定義した文章を入力して&lt;code&gt;Ask color expert&lt;/code&gt;をクリックします。するとLambdaで処理が実行されて返答される文章などを含んだレスポンスが返ってきます。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-10.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;残りの設定項目に Publishing infomation, Privacy &amp;amp; Compliance がありますが、これらはAlexa Skillをpubulishingするときに必要で、手元の実機での実行には必要ないので今回は割愛します。&lt;/p&gt;

&lt;h1 id=&#34;動作確認&#34;&gt;動作確認&lt;/h1&gt;

&lt;p&gt;まずAmazon Developer Consoleと同じアカウントでAmazon Alexaにログインして&lt;a href=&#34;http://alexa.amazon.com/spa/index.html#skills/your-skills&#34;&gt;Skill一覧画面&lt;/a&gt;から先ほど作成したSkillがあることを確認します。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-11.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;あとは下記の動画のように話しかけて動作するか確認します。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>麻雀カメラプロジェクト再始動？ 〜【導入編】テンプレートマッチングによる麻雀牌判定〜</title>
      <link>http://blog.algolab.jp/post/2016/08/25/mahjong-camera-introduction/</link>
      <pubDate>Thu, 25 Aug 2016 16:28:18 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/08/25/mahjong-camera-introduction/</guid>
      <description>

&lt;p&gt;2014年の終わり頃に「麻雀カメラ」というiPhoneアプリを作っていました。&lt;br /&gt;
カメラをかざすだけで麻雀の得点計算を行ってくれるアプリです。&lt;/p&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/livDDcygEDU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34; allowfullscreen frameborder=&#34;0&#34;&gt;&lt;/iframe&gt;
 &lt;/div&gt;


&lt;p&gt;動画を見ていただければ一目瞭然ですが、さすがに実用では使えない、ということでお蔵入りしました&amp;hellip;。&lt;br /&gt;
(時間がかかるだけでなく、精度もあまり良くありませんでした。)&lt;/p&gt;

&lt;p&gt;あれから一年半、技術の進歩は目覚ましく、いまならば良いものが作れるのでは？と思い立ち、再挑戦してみることにしました。&lt;/p&gt;

&lt;p&gt;今回は、導入編として、当時用いていた手法 (テンプレートマッチング) について説明します。&lt;/p&gt;

&lt;h2 id=&#34;画像判定プロセス&#34;&gt;画像判定プロセス&lt;/h2&gt;

&lt;p&gt;画像に何が写っているか判定するためには、大きく「検出 (Detection)」と「認識 (Recognition)」というプロセスに分かれます。&lt;/p&gt;

&lt;h3 id=&#34;検出&#34;&gt;検出&lt;/h3&gt;

&lt;p&gt;物体が画像内のどこにあるかを判定するプロセス。&lt;br /&gt;
下記の画像でいうと、赤枠をつけていくイメージです。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/25/mahjong-camera-introduction//detection_1.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;h3 id=&#34;認識&#34;&gt;認識&lt;/h3&gt;

&lt;p&gt;検出された領域に写っている物体が何であるかを判定するプロセス。&lt;br /&gt;
具体的には、下記の物体が「五萬」である、と識別することを言います。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/25/mahjong-camera-introduction//5m.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;h2 id=&#34;検出の難しさ&#34;&gt;検出の難しさ&lt;/h2&gt;

&lt;p&gt;麻雀牌判定においては、特に「検出」のプロセスが困難です。&lt;br /&gt;
横並びの複数牌に対し、どこが牌の境界かを判断することが非常に難しいのです。&lt;br /&gt;
例えば、下記の画像を考えてみましょう。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/25/mahjong-camera-introduction//detection_2.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;人間の目には、3つの牌の境界を判断することはできますが、&lt;br /&gt;
機械では、下記の赤枠の部分を1つの牌として検出してしまう、ということが起こり得てしまいます。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/25/mahjong-camera-introduction//detection_3.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;牌の境界には溝らしきものが存在するので、画像処理で溝を判定し境界とみなす、というような実装も考えられますが、光の具合や撮影角度などによって溝が見えなくなってしまうこともあるため、このような特定のルールを設ける方法では一筋縄ではいきません。&lt;/p&gt;

&lt;h2 id=&#34;当時用いていたアルゴリズム&#34;&gt;当時用いていたアルゴリズム&lt;/h2&gt;

&lt;p&gt;当時はどのように判定していたかというと、前処理を加えた画像に対し、テンプレートマッチングという手法を用いていました。(古くからある手法です)&lt;/p&gt;

&lt;h2 id=&#34;前処理&#34;&gt;前処理&lt;/h2&gt;

&lt;p&gt;前処理として、画像全体から牌が写っている全体領域を切り出します。&lt;br /&gt;
牌の全体領域と背景には明らかに差が見て取れるので、これは比較的容易に行うことができます。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/25/mahjong-camera-introduction//detection_4.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;具体的には、二値化画像に対し輪郭抽出を行い、頂点数や面積が一定以上のものの外接矩形を牌の全体領域としてみなす、という処理をしています。&lt;/p&gt;

&lt;p&gt;処理後の画像はこのような形となります。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/25/mahjong-camera-introduction//template_matching_1.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;h2 id=&#34;テンプレートマッチング&#34;&gt;テンプレートマッチング&lt;/h2&gt;

&lt;p&gt;前処理を行った画像に対し、テンプレートマッチングを用いて牌を判定していきます。&lt;/p&gt;

&lt;p&gt;テンプレートマッチングとは、テンプレート画像を少しずつ端から端までずらしていって、その類似度を計算していく、という手法です。&lt;/p&gt;

&lt;p&gt;例えばテンプレート画像に「五萬」を用いるとしましょう。&lt;br /&gt;
下記のように、少しずつずらして類似度を計算していき、その値を保持しておきます。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/25/mahjong-camera-introduction//template_matching_2.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;そして、一番類似度が高い領域を個別牌領域としてみなします。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/25/mahjong-camera-introduction//template_matching_3.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;実際には、テンプレート画像には、麻雀牌の種類 (34) x 上下左右の方向 (4) の合計136個を用い、一番類似度の高かったものを採用しています。&lt;/p&gt;

&lt;p&gt;そして、確定した領域を除いた残りの領域に対して更にテンプレートマッチングを繰り返していくことで牌の判定を行っていました。&lt;/p&gt;

&lt;h2 id=&#34;問題点&#34;&gt;問題点&lt;/h2&gt;

&lt;p&gt;テンプレートマッチングは、いわば総当たり作戦で「検出」と「認識」を一度に行ってしまおうというものです。&lt;br /&gt;
ご想像の通り、非常に効率が悪く、判定に時間がかかってしまいます。&lt;/p&gt;

&lt;p&gt;またテンプレート画像は上下左右の4方向しか用意していないため、例えば下記の中だと「六萬」の判定がうまくいきません。
&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/25/mahjong-camera-introduction//template_matching_4.png&#34;/&gt;
  
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;単純に考えると、テンプレートを増やせば良さそうですが、速度とのトレードオフなので現実的ではありません。&lt;/p&gt;

&lt;h2 id=&#34;次回予告&#34;&gt;次回予告&lt;/h2&gt;

&lt;p&gt;このように、テンプレートマッチングを用いる手法では、限られた条件下 (横並びに綺麗に牌が並んでいる) ではそれなりに精度は出るものの、条件から外れると精度が落ち、また判定に時間もかかってしまいました。&lt;/p&gt;

&lt;p&gt;次回以降、これらの問題点を解決すべく、いろいろな手法を試していきたいと思います。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>VPNサーバのパブリックIPをDNSに登録する 〜Raspberry Pi上で定期実行〜</title>
      <link>http://blog.algolab.jp/post/2016/08/24/vpn-dns/</link>
      <pubDate>Wed, 24 Aug 2016 15:48:58 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/08/24/vpn-dns/</guid>
      <description>

&lt;p&gt;弊社オフィスではVPN環境を構築していますが、固定IPを取得していないため、パブリックIPが変わるたびにVPNの接続先を変更しなければならず面倒です。&lt;/p&gt;

&lt;p&gt;そこで、定期的にIPアドレスを取得し、DNSに登録するようにしています。&lt;br /&gt;
ここでは、Raspberry Pi 上でAWS SDK for Python (Boto 3) を用いて定期実行させる手順をまとめます。&lt;/p&gt;

&lt;h2 id=&#34;aws-sdk-for-python-のインストールおよび設定&#34;&gt;AWS SDK for Python のインストールおよび設定&lt;/h2&gt;

&lt;p&gt;DNSはAWSのRoute53で管理しており、その操作のためにSDKを導入します。&lt;br /&gt;
なお、Raspberry PiにはPython (2.7系) がデフォルトでインストールされているため、そのまま用います。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ sudo pip install boto3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;続いて、Credentialを設定します。&lt;code&gt;YOUR_KEY&lt;/code&gt;および&lt;code&gt;YOUR_SECRET&lt;/code&gt;は環境に合わせて設定してください。&lt;br /&gt;
(Route53へのアクセス権限があれば問題ありません)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ vi ~/.aws/credentials
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[default]
aws_access_key_id = YOUR_KEY
aws_secret_access_key = YOUR_SECRET
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;dns更新&#34;&gt;DNS更新&lt;/h2&gt;

&lt;p&gt;DNS更新には&lt;code&gt;change_resource_record_sets&lt;/code&gt;メソッドを使用します。&lt;br /&gt;
詳細は、&lt;a href=&#34;http://boto3.readthedocs.io/en/latest/reference/services/route53.html#Route53.Client.change_resource_record_sets&#34;&gt;公式ドキュメント&lt;/a&gt; を参照してください。&lt;/p&gt;

&lt;h2 id=&#34;パブリックipの取得&#34;&gt;パブリックIPの取得&lt;/h2&gt;

&lt;p&gt;パブリックIPは、&lt;a href=&#34;http://httpbin.org&#34;&gt;httpbin.org&lt;/a&gt; にアクセスして取得します。&lt;br /&gt;
レスポンスとしては以下のようなものが返ってきます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ curl http://httpbin.org/ip
{
    &amp;quot;origin&amp;quot;: &amp;quot;xxx.xxx.xxx.xxx&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;スクリプト&#34;&gt;スクリプト&lt;/h2&gt;

&lt;p&gt;以上を踏まえて、スクリプトを書いていきます。&lt;br /&gt;
&lt;code&gt;DOMAIN&lt;/code&gt;および&lt;code&gt;HOST&lt;/code&gt;は環境に合わせて書き換えてください。下記では&lt;code&gt;hoge.example.com&lt;/code&gt;を登録する例としています。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# -*- coding: utf-8 -*-

import json
import urllib2

import boto3

DOMAIN = &#39;hoge&#39;
HOST = &#39;example.com&#39;
TTL = 300

# パブリックIPアドレスの取得
response = urllib2.urlopen(&#39;http://httpbin.org/ip&#39;)
ip_address = json.loads(response.read())[&#39;origin&#39;]

# AWS SDK Client
client = boto3.client(&#39;route53&#39;)

# hosted_zone_idの取得
hosted_zones = client.list_hosted_zones()[&#39;HostedZones&#39;]
hosted_zone_id = filter(lambda h: h[&#39;Name&#39;] == HOST + &#39;.&#39;, hosted_zones)[0][&#39;Id&#39;]

# 更新内容
change_batch = {
    &#39;Changes&#39;: [
        {
            &#39;Action&#39;: &#39;UPSERT&#39;,
            &#39;ResourceRecordSet&#39;: {
                &#39;Name&#39;: DOMAIN + &#39;.&#39; + HOST + &#39;.&#39;,
                &#39;Type&#39;: &#39;A&#39;,
                &#39;TTL&#39;: TTL,
                &#39;ResourceRecords&#39;: [
                    {&#39;Value&#39;: ip_address}
                ]
            }
        }
    ]
}

# 更新
client.change_resource_record_sets(
    HostedZoneId = hosted_zone_id,
    ChangeBatch = change_batch
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;あとは、上記のスクリプトを&lt;code&gt;cron&lt;/code&gt;に登録するなどして、定期実行させれば完了です。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ubuntu 16.04 LTSにXcfe (or LXDE) とTightVNC Serverでリモートデスクトップ環境を構築する</title>
      <link>http://blog.algolab.jp/post/2016/08/22/ubuntu-tightvnc-server/</link>
      <pubDate>Mon, 22 Aug 2016 17:53:57 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/08/22/ubuntu-tightvnc-server/</guid>
      <description>

&lt;p&gt;MacからUbuntu (16.04 LTS) へリモートデスクトップでアクセスする手順をまとめます。&lt;/p&gt;

&lt;h2 id=&#34;デスクトップ環境のインストール&#34;&gt;デスクトップ環境のインストール&lt;/h2&gt;

&lt;p&gt;デスクトップ環境には高速な軽量なXfceもしくはLXDEを最小限の構成でインストールします。&lt;br /&gt;
(お好みに合わせてください)&lt;/p&gt;

&lt;h3 id=&#34;xfceの場合&#34;&gt;Xfceの場合&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.xfce.org/&#34;&gt;https://www.xfce.org/&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ sudo apt-get install -y xfce4 xfce4-goodies
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;lxdeの場合&#34;&gt;LXDEの場合&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://lxde.org/&#34;&gt;http://lxde.org/&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ sudo apt-get install -y lxde-core
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;tightvnc-serverのセットアップ&#34;&gt;TightVNC Serverのセットアップ&lt;/h2&gt;

&lt;p&gt;リモートデスクトップを使用するため、TightVNCを用いてVNCサーバーを立てます。&lt;br /&gt;
&lt;a href=&#34;http://www.tightvnc.com/&#34;&gt;http://www.tightvnc.com/&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;インストール&#34;&gt;インストール&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ sudo apt-get install tightvncserver
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;起動設定&#34;&gt;起動設定&lt;/h3&gt;

&lt;p&gt;VNCサーバーからXcfeを起動するように設定を行います。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ vi ~/.vnc/xstartup
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;xfceの場合-1&#34;&gt;Xfceの場合&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
xrdb $HOME/.Xresources
startxfce4 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;lxdeの場合-1&#34;&gt;LXDEの場合&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
xrdb $HOME/.Xresources
lxsession -s LXDE &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;起動&#34;&gt;起動&lt;/h3&gt;

&lt;p&gt;初回起動時には、アクセスする際のパスワードが求められるので入力します。&lt;br /&gt;
view-only のパスワードは特に必要ないので、&lt;code&gt;n&lt;/code&gt;を選択しました。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ vncserver

You will require a password to access your desktops.

Password:
Verify:
Would you like to enter a view-only password (y/n)? n
...
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;アクセス&#34;&gt;アクセス&lt;/h3&gt;

&lt;p&gt;MacのFinderから&lt;code&gt;移動&lt;/code&gt; &amp;gt; &lt;code&gt;サーバーへ接続&lt;/code&gt;で、VNCクライアントを起動します。&lt;br /&gt;
アドレスバーには&lt;code&gt;vnc://[サーバーのIPアドレス]:5901&lt;/code&gt;を入力してください。
&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/22/ubuntu-tightvnc-server//vnc_client.png&#34;/&gt;
  
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;パスワードによる認証の後、リモートデスクトップにアクセスできます。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/22/ubuntu-tightvnc-server//vnc_server.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;h3 id=&#34;停止&#34;&gt;停止&lt;/h3&gt;

&lt;p&gt;以下のコマンドで停止します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ vncserver -kill :1
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;自動起動の設定&#34;&gt;自動起動の設定&lt;/h3&gt;

&lt;h4 id=&#34;起動ファイルの作成&#34;&gt;起動ファイルの作成&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ sudo vi /etc/systemd/system/vncserver@.service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;{{USERNAME}}&lt;/code&gt;は環境に合わせて設定してください。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[Unit]
Description=Start TightVNC server at startup
After=syslog.target network.target

[Service]
Type=forking
User={{USERNAME}}
PAMName=login
PIDFile=/home/{{USERNAME}}/.vnc/%H:%i.pid
ExecStartPre=-/usr/bin/vncserver -kill :%i &amp;gt; /dev/null 2&amp;gt;&amp;amp;1
ExecStart=/usr/bin/vncserver -depth 24 -geometry 1280x800 :%i
ExecStop=/usr/bin/vncserver -kill :%i

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;自動起動の設定-1&#34;&gt;自動起動の設定&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ sudo systemctl daemon-reload
$ sudo systemctl enable vncserver@1.service
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;起動-1&#34;&gt;起動&lt;/h4&gt;

&lt;p&gt;以下のコマンドで手動で起動できるようになります。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ sudo systemctl start vncserver@1
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;ステータス確認&#34;&gt;ステータス確認&lt;/h4&gt;

&lt;p&gt;ステータスは以下のコマンドで確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ sudo systemctl status vncserver@1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;xfce固有の設定&#34;&gt;Xfce固有の設定&lt;/h2&gt;

&lt;p&gt;デフォルトのままだとキーがうまく効かないので編集します。&lt;br /&gt;
&lt;code&gt;Applications&lt;/code&gt; &amp;gt; &lt;code&gt;Settings&lt;/code&gt; &amp;gt; &lt;code&gt;Window Manager&lt;/code&gt; &amp;gt; &lt;code&gt;Keyboard&lt;/code&gt;の設定を開きます。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Switch window for same application&lt;/code&gt;を選択して、&lt;code&gt;Clear&lt;/code&gt;することで、&lt;code&gt;Tab&lt;/code&gt;キーが正常に動作するようになります。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/22/ubuntu-tightvnc-server//tab.png&#34;/&gt;
  
&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>pyenv &#43; Anaconda (Ubuntu 16.04 LTS) で機械学習のPython開発環境をオールインワンで整える【随時更新】</title>
      <link>http://blog.algolab.jp/post/2016/08/21/pyenv-anaconda-ubuntu/</link>
      <pubDate>Sun, 21 Aug 2016 17:22:48 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/08/21/pyenv-anaconda-ubuntu/</guid>
      <description>

&lt;p&gt;筆者の機械学習系のPython開発環境は、&lt;a href=&#34;https://www.vagrantup.com/&#34;&gt;Vagrant&lt;/a&gt; を用いた &lt;a href=&#34;https://atlas.hashicorp.com/bento/boxes/ubuntu-16.04&#34;&gt;Ubuntu (16.04 LTS)&lt;/a&gt; 上に構築しています。&lt;br /&gt;
ここでは、画像認識、音声認識、自然言語処理などに必要な環境をオールインワンで構築する手順をまとめます。&lt;br /&gt;
(最終更新日: 2016/08/31)&lt;/p&gt;

&lt;h2 id=&#34;osバージョン&#34;&gt;OSバージョン&lt;/h2&gt;

&lt;p&gt;OSバージョンは下記の通りです。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh hljs bash&#34;&gt;$ cat /etc/lsb-release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=&lt;span class=&#34;hljs-number&#34;&gt;16.04&lt;/span&gt;
DISTRIB_CODENAME=xenial
DISTRIB_DESCRIPTION=&lt;span class=&#34;hljs-string&#34;&gt;&#34;Ubuntu 16.04.1 LTS&#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh hljs bash&#34;&gt;$ uname &lt;span class=&#34;hljs-operator&#34;&gt;-a&lt;/span&gt;
Linux vagrant &lt;span class=&#34;hljs-number&#34;&gt;4.4&lt;/span&gt;.&lt;span class=&#34;hljs-number&#34;&gt;0&lt;/span&gt;-&lt;span class=&#34;hljs-number&#34;&gt;31&lt;/span&gt;-generic &lt;span class=&#34;hljs-comment&#34;&gt;#50-Ubuntu SMP Wed Jul 13 00:07:12 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;


&lt;h2 id=&#34;pyenv-anaconda-の環境を構築&#34;&gt;pyenv + Anaconda の環境を構築&lt;/h2&gt;

&lt;p&gt;Python環境は、pyenv + Anacodaを用いて構築します。&lt;br /&gt;
pyenvやAnacondaの概要やメリットについては、下記の記事に詳しくまとまっています。&lt;br /&gt;
&lt;a href=&#34;http://qiita.com/y__sama/items/5b62d31cb7e6ed50f02c&#34;&gt;データサイエンティストを目指す人のpython環境構築 2016&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;上記の記事にあるように、ここでもpyenvはAnacondaのインストーラとしてのみ使用し、Python環境の切り替えはAnacondaで行うこととします。&lt;/p&gt;

&lt;h3 id=&#34;必要なパッケージのインストール&#34;&gt;必要なパッケージのインストール&lt;/h3&gt;

&lt;p&gt;まず、必要なパッケージをインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ sudo apt-get install -y make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev libncursesw5-dev
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;pyenvのインストール&#34;&gt;pyenvのインストール&lt;/h3&gt;

&lt;p&gt;pyenvおよびプラグインをインストールし、環境を整えます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ git clone git://github.com/yyuu/pyenv.git ~/.pyenv
$ git clone https://github.com/yyuu/pyenv-pip-rehash.git ~/.pyenv/plugins/pyenv-pip-rehash
$ echo &#39;export PYENV_ROOT=&amp;quot;$HOME/.pyenv&amp;quot;&#39; &amp;gt;&amp;gt; ~/.bashrc
$ echo &#39;export PATH=&amp;quot;$PYENV_ROOT/bin:$PATH&amp;quot;&#39; &amp;gt;&amp;gt; ~/.bashrc
$ echo &#39;eval &amp;quot;$(pyenv init -)&amp;quot;&#39; &amp;gt;&amp;gt; ~/.bashrc
$ source ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;anacondaのインストール&#34;&gt;Anacondaのインストール&lt;/h3&gt;

&lt;p&gt;まず、最新のAnaconda (Python 3系) のバージョンを確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ pyenv install -l | grep anaconda3
  anaconda3-2.0.0
  anaconda3-2.0.1
  anaconda3-2.1.0
  anaconda3-2.2.0
  anaconda3-2.3.0
  anaconda3-2.4.0
  anaconda3-2.4.1
  anaconda3-2.5.0
  anaconda3-4.0.0
  anaconda3-4.1.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最新のAnaconda (ここでは4.1.0) をインストールし、デフォルトとして設定します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ pyenv install anaconda3-4.1.0
$ pyenv global anaconda3-4.1.0
$ echo &#39;export PATH=&amp;quot;$PYENV_ROOT/versions/anaconda3-4.1.0/bin/:$PATH&amp;quot;&#39; &amp;gt;&amp;gt; ~/.bashrc
$ source ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pythonの環境を確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ python --version
Python 3.5.1 :: Anaconda 4.1.0 (64-bit)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;pythonライブラリのインストール&#34;&gt;Pythonライブラリのインストール&lt;/h2&gt;

&lt;p&gt;以下、用途に応じて必要なPythonライブラリ (+ 本体) をインストールしていきます。&lt;br /&gt;
&lt;code&gt;conda&lt;/code&gt;経由が便利なものは&lt;code&gt;conda&lt;/code&gt;で、それ以外は&lt;code&gt;pip&lt;/code&gt;で行います。&lt;/p&gt;

&lt;p&gt;諸々インストールする前に自身を更新しておきます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ conda update -y conda
$ pip install --upgrade pip
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;深層学習ライブラリ&#34;&gt;深層学習ライブラリ&lt;/h2&gt;

&lt;h3 id=&#34;tensorflow&#34;&gt;TensorFlow&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;https://www.tensorflow.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Googleの深層学習ライブラリ。&lt;code&gt;conda&lt;/code&gt;経由で最新バージョンを一発でインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ conda install -y -c jjhelmus tensorflow
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;chainer&#34;&gt;Chainer&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://chainer.org/&#34;&gt;http://chainer.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;PFNの深層学習ライブラリ。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ pip install chainer
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;keras&#34;&gt;Keras&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://keras.io/&#34;&gt;https://keras.io/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;TensorFlowおよびTheanoのラッパー。同時にTheanoも入ります。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ pip install keras
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;画像認識&#34;&gt;画像認識&lt;/h2&gt;

&lt;h3 id=&#34;imagemagick&#34;&gt;ImageMagick&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://imagemagick.org/script/index.php&#34;&gt;http://imagemagick.org/script/index.php&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;画像処理ライブラリ。&lt;code&gt;conda&lt;/code&gt;経由で本体もまとめてインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ conda install -y -c kalefranz imagemagick
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;opencv&#34;&gt;OpenCV&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://opencv.org/&#34;&gt;http://opencv.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;コンピュータビジョンライブラリ。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ conda install -y -c menpo opencv3
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;dlib&#34;&gt;Dlib&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://dlib.net/&#34;&gt;http://dlib.net/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;画像処理系が充実している機械学習ライブラリ。&lt;code&gt;cmake&lt;/code&gt;と&lt;code&gt;boost-python&lt;/code&gt;も同時にインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ sudo apt-get install libboost-python-dev cmake
$ conda install -y -c wordsforthewise dlib
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;selective-search&#34;&gt;Selective Search&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/AlpacaDB/selectivesearch&#34;&gt;https://github.com/AlpacaDB/selectivesearch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Alpacaが提供しているSelectiveSearchに特化したライブラリ。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ pip install selectivesearch
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;音声認識&#34;&gt;音声認識&lt;/h2&gt;

&lt;h3 id=&#34;kaldi&#34;&gt;Kaldi&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://kaldi-asr.org/&#34;&gt;http://kaldi-asr.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;深層学習を用いた音声認識ツールキット。下記の記事を参照してください。&lt;br /&gt;
&lt;a href=&#34;http://blog.algolab.jp/post/2016/08/31/kaldi/&#34;&gt;Kaldiで音声を学習させる 〜ディープラーニングを用いた音声認識ツールキット〜&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;ffmpeg&#34;&gt;FFmpeg&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://ffmpeg.org/&#34;&gt;https://ffmpeg.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;音声・動画処理ライブラリ。Ubuntu16.04から本体は&lt;code&gt;apt-get&lt;/code&gt;で入るようになりました。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ sudo apt install -y ffmpeg
$ pip install ffmpy
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;自然言語処理&#34;&gt;自然言語処理&lt;/h2&gt;

&lt;h3 id=&#34;mecab&#34;&gt;MeCab&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://taku910.github.io/mecab/&#34;&gt;http://taku910.github.io/mecab/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;形態素解析エンジン。本体は&lt;code&gt;apt-get&lt;/code&gt;でインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ sudo apt-get -y install libmecab-dev mecab mecab-ipadic mecab-ipadic-utf8
$ pip install mecab-python3
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;gensim&#34;&gt;gensim&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://radimrehurek.com/gensim/&#34;&gt;https://radimrehurek.com/gensim/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;トピックモデルのライブラリ。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ pip install gensim
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;強化学習&#34;&gt;強化学習&lt;/h2&gt;

&lt;h3 id=&#34;openai-gym&#34;&gt;OpenAI Gym&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://gym.openai.com/&#34;&gt;https://gym.openai.com/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;強化学習のトレーニング環境。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ pip install gym
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;その他便利ツール&#34;&gt;その他便利ツール&lt;/h2&gt;

&lt;h3 id=&#34;tightvnc-server&#34;&gt;TightVNC Server&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://www.tightvnc.com/&#34;&gt;http://www.tightvnc.com/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;リモートデスクトップ環境。下記の記事を参照してください。&lt;br /&gt;
&lt;a href=&#34;http://blog.algolab.jp/post/2016/08/22/ubuntu-tightvnc-server/&#34;&gt;Ubuntu 16.04 LTSにXcfeとTightVNC Serverでリモートデスクトップ環境を構築する&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>テレアポ模擬トレーニングBot 〜コミュニケーション教育ツールとしてのチャットボットの活用可能性〜</title>
      <link>http://blog.algolab.jp/post/2016/08/18/telephone-appointment-simulation-bot/</link>
      <pubDate>Thu, 18 Aug 2016 15:20:16 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/08/18/telephone-appointment-simulation-bot/</guid>
      <description>

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/18/telephone-appointment-simulation-bot//cover.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;LINEがBOT API、FacebookがMessenger Platformを発表してから数ヶ月たちました。&lt;br /&gt;
これまでに数々のチャットボットがリリースされていますが、まだ活用方法を模索している段階かと思います。&lt;/p&gt;

&lt;p&gt;その中で、コミュニケーション教育ツールとしての活用可能性があるのではないかと考え、実験的に「テレアポ模擬トレーニングBot」を &lt;a href=&#34;http://www.torix-corp.com/&#34;&gt;TORiX&lt;/a&gt; さんと共同開発させていただきました。&lt;/p&gt;

&lt;p&gt;今回は、このBotの概要と、開発に至った経緯について書きたいと思います。&lt;/p&gt;

&lt;h2 id=&#34;テレアポ模擬トレーニングbotとは&#34;&gt;テレアポ模擬トレーニングBotとは&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/18/telephone-appointment-simulation-bot//story.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;ストーリー形式で、テレアポを擬似体験することのできるコンテンツです。下記ボタンから体験できます。&lt;/p&gt;

&lt;script&gt;
window.fbAsyncInit = function() {
  FB.init({
    appId: &#34;131078513971096&#34;,
    xfbml: true,
    version: &#34;v2.6&#34;
  });
};

(function(d, s, id){
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) { return; }
  js = d.createElement(s); js.id = id;
  js.src = &#34;//connect.facebook.net/ja_JP/sdk.js&#34;;
  fjs.parentNode.insertBefore(js, fjs);
}(document, &#39;script&#39;, &#39;facebook-jssdk&#39;));
&lt;/script&gt;

&lt;div class=&#34;fb-messengermessageus&#34;
  messenger_app_id=&#34;131078513971096&#34;
  page_id=&#34;1771482106414711&#34;&gt;
&lt;/div&gt;


&lt;p&gt;ボタンが動作しない場合は、下記ページからメッセージを送ってみてください。&lt;br /&gt;
&lt;a href=&#34;https://www.facebook.com/1771482106414711/&#34;&gt;https://www.facebook.com/1771482106414711/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;開発の経緯&#34;&gt;開発の経緯&lt;/h2&gt;

&lt;p&gt;コミュニケーション領域の教育においては、擬似的にでも経験を積むことが一番スキルが身につく手段であると考えていますが、
書籍や動画等のコンテンツではそれを体験することが困難です。&lt;/p&gt;

&lt;p&gt;その中で、チャットボットを用いれば容易に実現できるのではないか、と考えたのが開発のきっかけです。&lt;/p&gt;

&lt;p&gt;実際に、テレアポ模擬トレーニングBotは1日もかからず実装することができました。&lt;br /&gt;
(ストーリー作成には別途1日かかりました。)&lt;/p&gt;

&lt;h2 id=&#34;コミュニケーション教育ツールとしての可能性&#34;&gt;コミュニケーション教育ツールとしての可能性&lt;/h2&gt;

&lt;p&gt;今回は題材としてテレアポを取り上げましたが、他にも応用可能性はあると考えています。&lt;/p&gt;

&lt;p&gt;例えば、&lt;a href=&#34;https://the-board.jp/&#34;&gt;board&lt;/a&gt; を展開する &lt;a href=&#34;http://www.velc.co.jp/about/&#34;&gt;VELC&lt;/a&gt; さんは、カスタマーサポートの教育にBotを用いていると語っています。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;そこで重要になってくるのが教育です。それに使っているのが、チャットツールである「Slack（スラック）」のbotです。&lt;br /&gt;
Slack上で「ok board」と打ち込むと、FAQからSlackに質問だけがランダムに飛んでくるようになっています。スタッフには空き時間にそれを使って回答文を作成する練習をしてもらい、後で僕がレビューをする。&lt;/p&gt;

&lt;p&gt;引用元: &lt;a href=&#34;https://seleck.cc/article/475&#34;&gt;「カスタマーサクセス」を意識したことはない。自然に神対応を実現した、CSの理想形&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;

&lt;p&gt;今回は、チャットボットを「チャットUIを用いたインタラクティブなアプリを簡単に作れるツール」として捉え、コミュニケーション教育ツールとしての活用可能性についてお届けしました。&lt;/p&gt;

&lt;p&gt;チャットボットはまだまだ可能性を秘めていると思いますので、今後も注目していきたいと思います。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>iOSで音声認識 〜Speech Frameworkを試す〜</title>
      <link>http://blog.algolab.jp/post/2016/08/16/speech-framework/</link>
      <pubDate>Tue, 16 Aug 2016 19:34:57 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/08/16/speech-framework/</guid>
      <description>

&lt;p&gt;2016/08/16現在、まだβ版という位置づけですが、Apple謹製の音声認識エンジン (Speech Framework) が公開されています。今回は、下記のサンプルコードを動作させてみます。
&lt;a href=&#34;https://developer.apple.com/library/prerelease/content/samplecode/SpeakToMe/Introduction/Intro.html&#34;&gt;https://developer.apple.com/library/prerelease/content/samplecode/SpeakToMe/Introduction/Intro.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;動作を確認するにはXcode 8.0以降、iOS 10.0 以降が必要なので、環境を整えるところから始めます。&lt;/p&gt;

&lt;h2 id=&#34;apple-developer-program-へ登録&#34;&gt;Apple Developer Program へ登録&lt;/h2&gt;

&lt;p&gt;諸々インストールするためには、Developer登録が必須なので、以下より登録を行います。
&lt;a href=&#34;https://developer.apple.com/programs/jp/&#34;&gt;https://developer.apple.com/programs/jp/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;xcode-8-betaをmacにインストール&#34;&gt;Xcode 8 betaをMacにインストール&lt;/h2&gt;

&lt;p&gt;Macから下記ページへアクセスし、Xcode 8 betaをダウンロード、インストールします。&lt;br /&gt;
&lt;a href=&#34;https://developer.apple.com/download/&#34;&gt;https://developer.apple.com/download/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;ios-10-betaをiphone-にインストール&#34;&gt;iOS 10 betaをiPhone にインストール&lt;/h2&gt;

&lt;p&gt;iPhoneか下記ページへアクセスし、iOS 10 betaをダウンロード、インストールします。&lt;br /&gt;
&lt;a href=&#34;https://developer.apple.com/download/&#34;&gt;https://developer.apple.com/download/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;サンプルコードをダウンロード&#34;&gt;サンプルコードをダウンロード&lt;/h2&gt;

&lt;p&gt;下記ページより、サンプルコードをダウンロードします。
&lt;a href=&#34;https://developer.apple.com/library/prerelease/content/samplecode/SpeakToMe/Introduction/Intro.html&#34;&gt;https://developer.apple.com/library/prerelease/content/samplecode/SpeakToMe/Introduction/Intro.html&lt;/a&gt;&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/16/speech-framework//speak_to_me.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;h2 id=&#34;署名の確認&#34;&gt;署名の確認&lt;/h2&gt;

&lt;p&gt;サンプルコードを開き、&lt;code&gt;TARGETS -&amp;gt; General -&amp;gt; Signing&lt;/code&gt;にDeveloper登録を行っているTeamが選択されているか確認します。
(ここが正しく設定されていないと実機でのビルドに失敗します)&lt;/p&gt;

&lt;h2 id=&#34;日本語対応&#34;&gt;日本語対応&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;ViewController.swift&lt;/code&gt;の15行目、言語指定のコードを&lt;code&gt;en-US&lt;/code&gt;から&lt;code&gt;ja-JP&lt;/code&gt;に書き換えます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: &amp;quot;ja-JP&amp;quot;))!
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;認識精度を確認&#34;&gt;認識精度を確認&lt;/h2&gt;

&lt;p&gt;実機で動作させ、認識精度を確認してみます。まず、「吾輩は猫である」を認識させてみます。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;吾輩は猫である。名前はまだ無い。どこで生れたかとんと見当がつかぬ。何でも薄暗いじめじめした所でニャーニャー泣いていた事だけは記憶している。吾輩はここで始めて人間というものを見た。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;認識結果がこちら。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;吾輩は猫である。名前はまだない。どこで生まれたかとんと見当がつかん。何でも薄暗いじめじめした所でニャンニャン泣いていた事だけは記憶している。吾輩はここで始めて人間というものを見た。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ほぼ正解と言っていい認識精度です。ここまで精度が高いとは正直驚きです。&lt;/p&gt;

&lt;h2 id=&#34;次回予告&#34;&gt;次回予告&lt;/h2&gt;

&lt;p&gt;次回はサンプルコードの中身を追ってみようと思います。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Googleカレンダー解析ツール作りました 〜時間の使い方チェッカー〜</title>
      <link>http://blog.algolab.jp/post/2016/08/16/calendar-report/</link>
      <pubDate>Tue, 16 Aug 2016 17:34:11 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/08/16/calendar-report/</guid>
      <description>

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/16/calendar-report//demo.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;タイトルの通りですが、時間の使い方チェッカーというツールを作りました (だいぶ前に作っていました)。&lt;br /&gt;
&lt;a href=&#34;http://tools.algolab.jp/calendar_reports&#34;&gt;http://tools.algolab.jp/calendar_reports&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ソースコードはGitHubに公開しています。&lt;br /&gt;
&lt;a href=&#34;https://github.com/algolab-inc/algo-tools/&#34;&gt;https://github.com/algolab-inc/algo-tools/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;時間の使い方チェッカーとは&#34;&gt;時間の使い方チェッカーとは？&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Googleカレンダーを解析して可視化するツールです&lt;/li&gt;
&lt;li&gt;タイトルを括弧で囲っているイベントのみを解析対象としています

&lt;ul&gt;
&lt;li&gt;(例) [仕事]&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;括弧を連続することで階層を表現することができます

&lt;ul&gt;
&lt;li&gt;(例) [仕事][営業]&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;データは一切保存しません (&amp;larr;大事)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;開発のきっかけ&#34;&gt;開発のきっかけ&lt;/h2&gt;

&lt;p&gt;弊社では、仕事を(準)委任契約でいただくことがあり、作業時間ログをGoogleカレンダー上で管理していました。&lt;br /&gt;
そして、月末に集計を手作業で行っていましたが、それを自動化したい、というのがきっかけです。&lt;/p&gt;

&lt;h2 id=&#34;実際に使ってみて&#34;&gt;実際に使ってみて&lt;/h2&gt;

&lt;p&gt;せっかくなので、全ての作業時間ログをGoogleカレンダー上で管理するということを数ヶ月間行ってみています。&lt;/p&gt;

&lt;p&gt;作業時間ログを都度つける行動は習慣化しているため (常にカレンダーを立ち上げています) 入力作業に負担はなく、後から時間の使い方を振り返ることができるのでなかなか良い感じです。&lt;/p&gt;

&lt;p&gt;具体的には、以下のような階層構造で管理を行っています。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;受託

&lt;ul&gt;
&lt;li&gt;プロジェクト1&lt;/li&gt;
&lt;li&gt;プロジェクト2&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;自社

&lt;ul&gt;
&lt;li&gt;サービスA&lt;/li&gt;
&lt;li&gt;サービスB&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;経営

&lt;ul&gt;
&lt;li&gt;経営計画&lt;/li&gt;
&lt;li&gt;財務&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;弊社は受託事業を行いながら自社サービスを開発しており、また筆者は代表を務めているので、時間の使い方としては、大きく「受託」「自社」「経営」の3つに分かれます。&lt;/p&gt;

&lt;p&gt;その下にも幾つか階層を設けていますが、基本的にはこの3つの時間比率を毎月振り返るようにしています。&lt;/p&gt;

&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;

&lt;p&gt;時間の使い方を可視化してみると色々な気づきがあります。是非試してみてください！&lt;br /&gt;
&lt;a href=&#34;http://tools.algolab.jp/calendar_reports&#34;&gt;http://tools.algolab.jp/calendar_reports&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Amazon Echoを6,000円で自作する 〜Raspberry Pi 3 &#43; Alexa Voice Services (AVS)〜</title>
      <link>http://blog.algolab.jp/post/2016/08/11/raspberry-pi-alexa/</link>
      <pubDate>Thu, 11 Aug 2016 19:08:44 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/08/11/raspberry-pi-alexa/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;http://blog.algolab.jp/post/2016/07/29/mary-meeker-2016/&#34;&gt;音声は新しいパラダイムシフトになる 〜2016年度版メアリー・ミーカー氏レポートまとめ〜&lt;/a&gt; でも触れたように、次世代デバイスとしてAmazon Echoは注目するべき存在です。&lt;/p&gt;

&lt;p&gt;しかしながら、日本では技適の関係で未だ使用できません。&lt;br /&gt;
ただ、Alexa Voice Services (AVS) というものが公開されており、Amazon Echoを様々なデバイスで動作させることが可能です。&lt;/p&gt;

&lt;p&gt;今回は、Raspberry Pi 3からAVSを利用できるようにしました。&lt;br /&gt;
セットアップについては下記にある通りですが、低予算での最低限の手順をまとめてみます。
&lt;a href=&#34;https://github.com/amzn/alexa-avs-raspberry-pi&#34;&gt;https://github.com/amzn/alexa-avs-raspberry-pi&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;完成したもの&#34;&gt;完成したもの&lt;/h2&gt;

&lt;p&gt;いきなり動画ですが、こんな感じで動きます。英語で話かけると、リクエストを解釈して実行してくれたり、音声で応答してくれて面白いです。

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/fWubPL5_YaU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34; allowfullscreen frameborder=&#34;0&#34;&gt;&lt;/iframe&gt;
 &lt;/div&gt;
&lt;/p&gt;

&lt;h2 id=&#34;用意したもの&#34;&gt;用意したもの&lt;/h2&gt;

&lt;p&gt;音声入力にUSBマイクロフォンが必要なので、Raspberry Pi 3と併せて購入。他はありあわせで用意しました。&lt;br /&gt;
Raspberry Pi用のディスプレイを用意してもよいですが、今回はVNC server (Linux版リモートデスクトップ) を使います。&lt;/p&gt;

&lt;h3 id=&#34;買ったもの&#34;&gt;買ったもの&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Raspberry Pi 3 (4,800円)

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.co.jp/gp/product/B01D1FR2WE/&#34;&gt;https://www.amazon.co.jp/gp/product/B01D1FR2WE/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;USBマイクロフォン (1,600円)

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.co.jp/gp/product/B0027WPY82&#34;&gt;https://www.amazon.co.jp/gp/product/B0027WPY82&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;ありあわせ&#34;&gt;ありあわせ&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Micro SDカード

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.co.jp/dp/B00CDJNOX6/&#34;&gt;https://www.amazon.co.jp/dp/B00CDJNOX6/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Micro-USB (A-MicroB) ケーブル&lt;/li&gt;
&lt;li&gt;スピーカー&lt;/li&gt;
&lt;li&gt;LANケーブル&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;raspberry-pi-を起動する&#34;&gt;Raspberry Pi を起動する&lt;/h2&gt;

&lt;h3 id=&#34;osイメージの準備&#34;&gt;OSイメージの準備&lt;/h3&gt;

&lt;p&gt;以下の記事を参考に進めました。&lt;br /&gt;
&lt;a href=&#34;http://qiita.com/onlyindreams/items/acc70807b69b43e176bf&#34;&gt;Raspberry Pi 3にRaspbianをインストール(Mac OS X を使用)&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Rasbian Jessie は &lt;code&gt;2016-05-27&lt;/code&gt; リリースのものを用いました&lt;/li&gt;
&lt;li&gt;ddコマンドのオプションで、ブロックサイズは大文字 (&lt;code&gt;bs=1M&lt;/code&gt;) で指定しました&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;起動手順&#34;&gt;起動手順&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;MicroSD、LAN、 USBマイクロフォン、スピーカーを接続しておきます。&lt;/li&gt;
&lt;li&gt;電源用としてUSBケーブルを挿すとBIOSが起動します。今回はOSであるRaspbian Jessieも自動で起動しました。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;必要なアカウント-ライブラリの準備&#34;&gt;必要なアカウント・ライブラリの準備&lt;/h2&gt;

&lt;p&gt;AVSを利用するために必要なものを諸々準備します。&lt;/p&gt;

&lt;h3 id=&#34;amazon-developer-アカウントの登録&#34;&gt;Amazon Developer アカウントの登録&lt;/h3&gt;

&lt;p&gt;下記よりアカウントを登録します。登録済みであれば不要です。&lt;br /&gt;
&lt;a href=&#34;https://developer.amazon.com/login.html&#34;&gt;https://developer.amazon.com/login.html&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;サンプルアプリのダウンロード&#34;&gt;サンプルアプリのダウンロード&lt;/h3&gt;

&lt;p&gt;公式のGithub上にある &lt;a href=&#34;https://github.com/amzn/alexa-avs-raspberry-pi/archive/master.zip&#34;&gt;Sample app&lt;/a&gt; をダウンロード&amp;amp;解凍して下記のようにデスクトップなどのパスに保存します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;/home/pi/Desktop/alexa-avs-raspberry-pi-master/
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;vnc-serverのインストール&#34;&gt;VNC Serverのインストール&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# install
$ sudo apt-get install tightvncserver
# run
$ tightvncserver
# auto run setup
$ vi /home/pi/.config/tightvnc.desktop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;tightvnc.desktop&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[Desktop Entry]
Type=Application
Name=TightVNC
Exec=vncserver :1
StartupNotify=false
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;vncでraspberry-piへアクセス&#34;&gt;VNCでRaspberry Piへアクセス&lt;/h3&gt;

&lt;p&gt;Macからアクセスする手順は &lt;a href=&#34;http://blog.algolab.jp/post/2016/08/22/ubuntu-tightvnc-server/#アクセス&#34;&gt;こちら&lt;/a&gt; をご参照ください。&lt;/p&gt;

&lt;h3 id=&#34;vlcのインストール&#34;&gt;VLCのインストール&lt;/h3&gt;

&lt;p&gt;VLC media playerをインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# install
$ sudo apt-get install vlc-nox vlc-data
# add env vars
$ echo &amp;quot;export LD_LIBRARY_PATH=/usr/lib/vlc:$LD_LIBRARY_PATH&amp;quot; &amp;gt;&amp;gt; ~/.bashrc
$ echo &amp;quot;export VLC_PLUGIN_PATH=/usr/lib/vlc/plugins&amp;quot; &amp;gt;&amp;gt; ~/.bashrc
$ soure ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;nodeとnpmのインストール&#34;&gt;NodeとNPMのインストール&lt;/h3&gt;

&lt;p&gt;後に出てくるサーバーの起動に必要なNodeとNPMをインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# apt-get update &amp;amp; upgrade. It takes about 15 min.
$ sudo apt-get update &amp;amp;&amp;amp; sudo apt-get upgrade
# install nodejs
$ curl -sL https://deb.nodesource.com/setup | sudo bash -
$ sudo apt-get install nodejs
$ cd /home/pi/Desktop/alexa-avs-raspberry-pi-master/samples/companionService
$ npm install
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;jdkとmavenのインストール&#34;&gt;JDKとMavenのインストール&lt;/h3&gt;

&lt;p&gt;公式DocはMavenの環境変数は &lt;code&gt;/etc/profile.d/maven.sh&lt;/code&gt; に追加する方法ですが、うまくいかなかったので手っ取り早く &lt;code&gt;bashrc&lt;/code&gt; に追加して進めました。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# java
$ cd /home/pi/Desktop/alexa-avs-raspberry-pi-master/samples/javaclient
$ ./install-java8.sh
# maven
$ wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz
$ sudo tar xvf apache-maven-3.3.9-bin.tar.gz  -C /opt
# add maven_vars
$ echo &amp;quot;export M2_HOME=/opt/apache-maven-3.3.9&amp;quot; &amp;gt;&amp;gt; ~/.bashrc
$ echo &amp;quot;export PATH=$PATH:$M2_HOME/bin&amp;quot; &amp;gt;&amp;gt; ~/.bashrc
$ source ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;証明書生成スクリプトを実行&#34;&gt;証明書生成スクリプトを実行&lt;/h3&gt;

&lt;p&gt;プロダクトID、シリアル番号、パスワードの3つを入力します。今回はパスワードは空のままで進めます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ /home/pi/Desktop/alexa-avs-raspberry-pi-master/samples/javaclient/generate.sh
&amp;gt; product ID: my_device
&amp;gt; Serial Number: 123456
&amp;gt; Password: [blank]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;クライアントidとclientsecretを発行&#34;&gt;クライアントIDとClientSecretを発行&lt;/h3&gt;

&lt;p&gt;ここは &lt;a href=&#34;https://github.com/amzn/alexa-avs-raspberry-pi#user-content-6---getting-started-with-alexa-voice-service&#34;&gt;公式Doc&lt;/a&gt; の画像のとおり進めればよいです。&lt;/p&gt;

&lt;h3 id=&#34;サーバとクライアントを起動&#34;&gt;サーバとクライアントを起動&lt;/h3&gt;

&lt;p&gt;下記のとおりサーバを起動します。 &lt;code&gt;config.js&lt;/code&gt; には先ほど発行したクライアントIDとClientSecretを入力しておきます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# setup clientId and ClientSecret
$ vi /home/pi/Desktop/alexa-avs-raspberry-pi-master/samples/companionService/config.js
$ cd /home/pi/Desktop/alexa-avs-raspberry-pi-master/samples/companionService
$ npm start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;続いてクライアントも起動します。起動するとGUIも一緒に立ち上がります。 &lt;code&gt;DISPLAY=:1.0&lt;/code&gt; はVNC経由の場合の指定です。外部ディスプレイを使う場合は &lt;code&gt;DISPLAY=:0.0&lt;/code&gt; です。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ cd /home/pi/Desktop/alexa-avs-raspberry-pi-master/samples/javaclient
$ mvn install
$ export DISPLAY=:1.0
$ mvn exec:exec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;GUIに出てくるURLにアクセスしてデバイスの登録になります。ここも &lt;a href=&#34;https://github.com/amzn/alexa-avs-raspberry-pi#user-content-10---obtain-authorsization-from-login-with-amazon&#34;&gt;公式Doc&lt;/a&gt; の画像のとおりです。以上が終わると、AVSを利用できます。&lt;/p&gt;

&lt;h2 id=&#34;次回予告&#34;&gt;次回予告&lt;/h2&gt;

&lt;p&gt;次回はAlexa Skillsを登録して使ってみようと思います。乞うご期待。Don&amp;rsquo;t miss out!&lt;br /&gt;
(2016/08/25 公開しました &amp;rarr; &lt;a href=&#34;http://blog.algolab.jp/post/2016/08/29/alexa-skills-kit/&#34;&gt;Alexa Skills KitをAWS Lamdaから使う&lt;/a&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ディープラーニング徹底入門 〜AIトレーニング第1回〜</title>
      <link>http://blog.algolab.jp/post/2016/08/07/deep-learning-introduction/</link>
      <pubDate>Sun, 07 Aug 2016 15:46:48 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/08/07/deep-learning-introduction/</guid>
      <description>

&lt;p&gt;弊社で実施している &lt;a href=&#34;http://blog.algolab.jp/post/2016/07/25/ai-training/&#34;&gt;AIトレーニング&lt;/a&gt; の実況中継シリーズです。&lt;br /&gt;
前回の内容は &lt;a href=&#34;http://blog.algolab.jp/post/2016/08/01/ai-training-kick-off/&#34;&gt;AIトレーニングキックオフ 〜ハムケツを認識したい〜&lt;/a&gt; をご覧ください。&lt;br /&gt;
今回は、ディープラーニングのイメージを掴んでもらうためにAさんにお話した内容をお届けします。&lt;/p&gt;

&lt;h2 id=&#34;画像認識とは&#34;&gt;画像認識とは？&lt;/h2&gt;

&lt;p&gt;さて、突然ですが、皆さんに問題です。&lt;br /&gt;
以下のようにグループが分かれているものとします。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/07/deep-learning-introduction//group_1.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;このとき、下の画像はどちらのグループになるでしょうか？&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/07/deep-learning-introduction//square_red.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;正解は「B」です。正解者の皆さんおめでとうございます！&lt;br /&gt;
ではなぜ「B」なのか。おそらくこう考えたはずです。&lt;/p&gt;

&lt;p&gt;「色がポイントだ。青ければグループA、赤ければグループB。だから、この画像はグループBだろう。」&lt;/p&gt;

&lt;p&gt;図式化すると、下記のような形かと思います。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/07/deep-learning-introduction//image_recognition_1.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;これが画像を見分けるメカニズムで、これを機械に代替させようというのが機械学習における画像認識の分野です。&lt;br /&gt;
そして、機械学習の文脈では、前半を「特徴抽出」、後半を「分類」と呼びます。&lt;/p&gt;

&lt;h2 id=&#34;画像認識の難しさ&#34;&gt;画像認識の難しさ&lt;/h2&gt;

&lt;p&gt;さて、冒頭に出した問題ですが、以下のようなグループ分けだったらどうでしょう。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/07/deep-learning-introduction//group_2.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;先ほどは「色」に注目しましたが、今度は「形」に注目して判定する必要がありそうです。&lt;br /&gt;
図を再掲すると以下のようになります。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/07/deep-learning-introduction//image_recognition_2.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;このように、注目するポイントは問題設定によって異なります。&lt;br /&gt;
そして、この注目する部分を決める「特徴抽出」の設計は、人間が行う必要があり、大変でした。&lt;/p&gt;

&lt;h2 id=&#34;ディープラーニングとは&#34;&gt;ディープラーニングとは？&lt;/h2&gt;

&lt;p&gt;これらの作業を機械が自動的に行ってくれるのがディープラーニングです。&lt;br /&gt;
具体的には、この画像はグループA、この画像はグループBといったように大量の画像を機械に覚えさせるだけで良い、というイメージです。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/07/deep-learning-introduction//deep_learning.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;つまり、これまでよりも簡単に画像認識ができるようになったということです。しかも、より高い精度が出るようになったということで、ディープラーニングは爆発的に広まりました。&lt;/p&gt;

&lt;h2 id=&#34;畳み込みニューラルネットワーク-cnn&#34;&gt;畳み込みニューラルネットワーク (CNN)&lt;/h2&gt;

&lt;p&gt;画像認識におけるディープラーニングでは、畳み込みニューラルネットワークが用いられるのが一般的です。&lt;br /&gt;
教科書などでは、よく下記のような図が用いられていると思います。
&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/07/deep-learning-introduction//dnn.png&#34;/&gt;
  
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;縦に並んだ丸の列が「層」を表しており、上記の図は4層のニューラルネットワークを表現しています。&lt;br /&gt;
大雑把にいうと、最後の層の前までで「特徴抽出」を行い、最後の層で「分類」を行っていると考えてください。&lt;br /&gt;
本当はそれぞれの層に意味がありますが、現段階では上記の理解で十分かと思います。&lt;/p&gt;

&lt;h2 id=&#34;次回予告&#34;&gt;次回予告&lt;/h2&gt;

&lt;p&gt;ディープラーニング（畳み込みニューラルネットワーク）のイメージを掴んでいただいた上で、Aさんに課題を設定しました。&lt;/p&gt;

&lt;p&gt;「学習済みモデルを用いて、画像の特徴抽出を行い、次元削減を行った上で可視化せよ」&lt;/p&gt;

&lt;p&gt;見慣れない用語が幾つか出てきましたが、調べれば理解出来る内容だと考え、上記課題としました。&lt;br /&gt;
次回はその内容についてお届けします。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TorchをAWSのGPUインスタンス (Ubuntu 14.04) で動かす</title>
      <link>http://blog.algolab.jp/post/2016/08/03/torch-aws-gpu-ubuntu/</link>
      <pubDate>Wed, 03 Aug 2016 17:12:34 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/08/03/torch-aws-gpu-ubuntu/</guid>
      <description>

&lt;p&gt;TorchをAWSのGPUインスタンス (Ubuntu 14.04) で動かす手順をまとめます。&lt;br /&gt;
環境は以下の通りです。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ubuntu Server 14.04 LTS&lt;/li&gt;
&lt;li&gt;CUDA7.5&lt;/li&gt;
&lt;li&gt;CuDNN v5&lt;/li&gt;
&lt;li&gt;Torch7&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;インスタンスを起動&#34;&gt;インスタンスを起動&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/03/torch-aws-gpu-ubuntu//ubuntu.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;Ubuntu Server 14.04 LTS (HVM), SSD Volume Type - ami-2d39803a をベースに構築します。&lt;br /&gt;
インスタンスタイプはg2.2xlargeを用いました。&lt;br /&gt;
ストレージ容量はデフォルトの8GBでは不足するので、16GBとします。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/03/torch-aws-gpu-ubuntu//storage.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;h2 id=&#34;パッケージ更新&#34;&gt;パッケージ更新&lt;/h2&gt;

&lt;p&gt;インスタンスが起動したら、SSHでログインのうえ、まずパッケージを更新します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ sudo apt-get update
$ sudo apt-get upgrade -y
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;cudaインストール&#34;&gt;CUDAインストール&lt;/h2&gt;

&lt;p&gt;CUDAのインストールはハマりどころが多いですが、先人の知恵にならって進めます。&lt;br /&gt;
&lt;a href=&#34;https://gist.github.com/erikbern/78ba519b97b440e10640&#34;&gt;https://gist.github.com/erikbern/78ba519b97b440e10640&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;既存のドライバ (Noveau) を無効にします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ echo -e &amp;quot;blacklist nouveau\nblacklist lbm-nouveau\noptions nouveau modeset=0\nalias nouveau off\nalias lbm-nouveau off\n&amp;quot; | sudo tee /etc/modprobe.d/blacklist-nouveau.conf
echo options nouveau modeset=0 | sudo tee -a /etc/modprobe.d/nouveau-kms.conf
sudo update-initramfs -u
sudo reboot
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;必要なカーネルモジュールをインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ sudo apt-get install -y linux-image-extra-virtual
$ sudo reboot
$ sudo apt-get install -y linux-source linux-headers-`uname -r`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;CUDA7.5をインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ wget http://developer.download.nvidia.com/compute/cuda/7.5/Prod/local_installers/cuda_7.5.18_linux.run
$ chmod +x cuda_7.5.18_linux.run
$ ./cuda_7.5.18_linux.run -extract=`pwd`/nvidia_installers
$ cd nvidia_installers
$ sudo ./NVIDIA-Linux-x86_64-352.39.run
$ sudo modprobe nvidia
$ sudo ./cuda-linux64-rel-7.5.18-19867135.run
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;途中でシンボリックリンクを作成するか聞かれますが、yesを選択します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;Would you like to create a symbolic link /usr/local/cuda pointing to /usr/local/cuda-7.5? ((y)es/(n)o/(a)bort) [ default is yes ]: y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;CUDAのパスを環境変数に追加します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ echo -e &amp;quot;export PATH=/usr/local/cuda/bin:\$PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:\$LD_LIBRARY_PATH&amp;quot; | tee -a ~/.bashrc
$ source ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;cudnnインストール&#34;&gt;CUDNNインストール&lt;/h2&gt;

&lt;p&gt;まず、下記のサイトからアカウントを登録します。&lt;br /&gt;
&lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;https://developer.nvidia.com/cudnn&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;アカウント登録後、ダウンロードページから、cuDNN v5 Library for Linuxをダウンロードします。
&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/03/torch-aws-gpu-ubuntu//cudnn.png&#34;/&gt;
  
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;ダウンロードしたファイルをサーバへ転送後、サーバ上で展開します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ tar -xzf cudnn-7.5-linux-x64-v5.0-ga.tgz
$ sudo cp cuda/lib64/libcudnn* /usr/local/cuda-7.5/lib64
$ sudo cp cuda/include/cudnn.h /usr/local/cuda/include
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;torchインストール&#34;&gt;Torchインストール&lt;/h2&gt;

&lt;p&gt;公式に従って、インストールします。&lt;br /&gt;
&lt;a href=&#34;http://torch.ch/docs/getting-started.html&#34;&gt;http://torch.ch/docs/getting-started.html&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ sudo apt-get install -y git
$ git clone https://github.com/torch/distro.git ~/torch --recursive
$ cd ~/torch; bash install-deps;
$ ./install.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;環境変数を.bashrcに書き込むか聞かれますが、yesを選択します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;Do you want to automatically prepend the Torch install location
to PATH and LD_LIBRARY_PATH in your /home/ubuntu/.bashrc? (yes/no)
[yes] &amp;gt;&amp;gt;&amp;gt; 
yes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;環境変数を反映します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ source ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最後に、CUDAおよびcuDNNを使うためのLuaライブラリをインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ luarocks install cutorch
$ luarocks install cunn
$ luarocks install cunnx
$ luarocks install https://raw.githubusercontent.com/soumith/cudnn.torch/master/cudnn-scm-1.rockspec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上で環境構築は完了です。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>超シンプルにTensorFlowでDQN (Deep Q Network) を実装してみる 〜導入編〜</title>
      <link>http://blog.algolab.jp/post/2016/08/01/tf-dqn-simple-1/</link>
      <pubDate>Mon, 01 Aug 2016 22:06:25 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/08/01/tf-dqn-simple-1/</guid>
      <description>

&lt;p&gt;みなさん、DQNしてますか？&lt;br /&gt;
DQNについては、下記の記事によくまとめられており、実装してみようとした方も多いのではないでしょうか。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5&#34;&gt;DQNの生い立ち　＋　Deep Q-NetworkをChainerで書いた&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://qiita.com/icoxfog417/items/242439ecd1a477ece312&#34;&gt;ゼロからDeepまで学ぶ強化学習&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;しかし、いざ自力で動作させてみようとすると、こんな問題にぶち当たると思います。&lt;/p&gt;

&lt;p&gt;「学習時間なげえ。。。」&lt;/p&gt;

&lt;p&gt;DQNに限らず、ディープラーニングのモデルを学習させようとすると、平気で数日以上かかります。&lt;br /&gt;
そして、学習させたモデルが期待通りの動作をしなかったとしたら、もう投げ出したくなってしまいます。&lt;br /&gt;
(よくある話です)&lt;/p&gt;

&lt;p&gt;なので、筆者が新しいモデルを一から実装する際には、なるべく単純なモデル、データから始めるようにしています。&lt;/p&gt;

&lt;p&gt;ここでは、超シンプルなDQNを実装し、動作させてみることにします。&lt;br /&gt;
早速いってみましょう。CPUで3分もあれば学習が終わります！&lt;/p&gt;

&lt;h2 id=&#34;まずは動かしてみよう&#34;&gt;まずは動かしてみよう&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/01/tf-dqn-simple-1//demo-catch_ball.gif&#34;/&gt;
  
&lt;/figure&gt;


&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;

&lt;p&gt;具体的には、上図のように上から落ちてくるボールをキャッチする、というタスクを学習させます。&lt;br /&gt;
TensorFlowで実装しており、ソースコードは下記に公開しています。&lt;br /&gt;
&lt;a href=&#34;https://github.com/algolab-inc/tf-dqn-simple&#34;&gt;https://github.com/algolab-inc/tf-dqn-simple&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;環境構築&#34;&gt;環境構築&lt;/h3&gt;

&lt;p&gt;はじめにソースコードをダウンロードします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ git clone https://github.com/algolab-inc/tf-dqn-simple.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;次に、動作のためにTensorFlowとMatplotlibが必要なので、インストールします。&lt;/p&gt;

&lt;p&gt;Tensorflowについては下記リンクを参照のうえインストールを行ってください。&lt;br /&gt;
&lt;a href=&#34;https://www.tensorflow.org/versions/master/get_started/os_setup.html&#34;&gt;https://www.tensorflow.org/versions/master/get_started/os_setup.html&lt;/a&gt;&lt;br /&gt;
(2016/08/01現在、Python3.5.2 + Tensorflow0.9.0での動作を確認しています)&lt;/p&gt;

&lt;p&gt;Matolotlibはpipでインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ pip install matplotlib
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;学習&#34;&gt;学習&lt;/h3&gt;

&lt;p&gt;環境が整ったら、ソースコードのディレクトリに移動して、train.pyを叩くと学習が始まります。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ cd tf-dqn-simple
$ python train.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下記のようなログが出ていれば、正しく学習が行われています。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;EPOCH: 000/999 | WIN: 001 | LOSS: 0.0068 | Q_MAX: 0.0008&lt;br /&gt;
EPOCH: 001/999 | WIN: 002 | LOSS: 0.0447 | Q_MAX: 0.0013&lt;br /&gt;
&amp;hellip;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;数分ほどで学習が終わったかと思います。&lt;br /&gt;
では学習したモデルでテストしてみましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ python test.py
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;WIN: 001/001 (100.0%)&lt;br /&gt;
WIN: 002/002 (100.0%)&lt;br /&gt;
&amp;hellip;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;キャッチボールのアニメーションとともに、上記のようなログが出れば成功です。&lt;br /&gt;
きちんと動作しましたでしょうか？&lt;br /&gt;
学習がうまくいっていれば、おそらく100%でキャッチできていると思います。&lt;/p&gt;

&lt;p&gt;次回は、実装編についてお届けします。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AIトレーニングキックオフ 〜ハムケツを認識したい〜</title>
      <link>http://blog.algolab.jp/post/2016/08/01/ai-training-kick-off/</link>
      <pubDate>Mon, 01 Aug 2016 12:11:45 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/08/01/ai-training-kick-off/</guid>
      <description>

&lt;p&gt;弊社で実施している &lt;a href=&#34;http://blog.algolab.jp/post/2016/07/25/ai-training/&#34;&gt;AIトレーニング&lt;/a&gt; の実況中継シリーズです。&lt;br /&gt;
今回は、Aさんのキックオフの内容についてお届けします。&lt;/p&gt;

&lt;h2 id=&#34;ハムケツ&#34;&gt;ハムケツ&lt;/h2&gt;

&lt;p&gt;「あの・・ですね。その・・・。ハムケツを認識したいんです。」&lt;/p&gt;

&lt;p&gt;トレーニングの最初にゴールを設定するのですが、その際、Aさんから出たひと言目がこれでした。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/01/ai-training-kick-off//hamuketu.jpg&#34;/&gt;
  
  &lt;figcaption&gt;&lt;p&gt;http://hamuketu.blog.jp/archives/51191945.html&lt;/p&gt;&lt;/figcaption&gt;
  
&lt;/figure&gt;


&lt;p&gt;ハムケツとは最近ネットや書籍で話題になったハムスターのおしりのことで、
独特の可愛さで評判となったので目にされた方もいらっしゃるのではないでしょうか。&lt;/p&gt;

&lt;p&gt;Aさんの話を詳しくうかがってみると、妻がハムケツの得も言われぬ可愛さに身もだえていた時に「ハムケツ画像が自動的に送られてくるアプリがあったらな」と思ったとのことです。&lt;/p&gt;

&lt;p&gt;素晴らしい！ではハムケツでいきましょう、と速攻でテーマが決まり、その後は大の男二人で「ハムケツの特徴」や「こっちがハムケツ、こっちはハムケツじゃない」などと熱い議論を交わしつつ成果物のイメージをより具体的に話し合いました。&lt;/p&gt;

&lt;p&gt;実際にアプリを作るとなると様々な工程がありますが、ハムケツ画像を集める段階で人工知能がハムケツを認識してくれれば、ハムケツであるかどうかの判定を人間が行わなくてよくなります。&lt;/p&gt;

&lt;p&gt;この認識して判定するフェーズを今回のトレーニングで取り扱い、出来上がったモデルを誰にでも触ってもらえる形にしようということで、&lt;/p&gt;

&lt;p&gt;「ハムケツ画像を入力すると認識結果のテキストを出力で得られるWebアプリを作る」&lt;/p&gt;

&lt;p&gt;をゴールとすることになりました。&lt;/p&gt;

&lt;h2 id=&#34;現状を把握する&#34;&gt;現状を把握する&lt;/h2&gt;

&lt;p&gt;ゴールが決まったところで、Aさんの現状のスキルをヒアリングしていきます。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Webサービスや業務システムの設計、開発、運用を10年&lt;/li&gt;
&lt;li&gt;開発言語はJava、Ruby、Python、PHP&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;受講者のトレーニング開始時ベンチマークとしてご参考にしていただければと思います。&lt;/p&gt;

&lt;h2 id=&#34;次回予告&#34;&gt;次回予告&lt;/h2&gt;

&lt;p&gt;Aさんはだいぶ基礎がありそうなので、ディープラーニングのイメージを掴んでいただいた上で、成果物完成までのステップを提示し、それをこなしてもらう、というスタイルでトレーニングを開始することにしました。&lt;br /&gt;
なお、仕組みを理解いただくため、少し遠回りなステップを提示させていただいています。&lt;/p&gt;

&lt;p&gt;次回は、ディープラーニングのイメージを掴んでもらうためにお話した内容についてお届けします。&lt;br /&gt;
(2016/08/07 公開しました &amp;rarr; &lt;a href=&#34;http://blog.algolab.jp/post/2016/08/07/deep-learning-introduction/&#34;&gt;ディープラーニング徹底入門 〜AIトレーニング第1回〜&lt;/a&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Seq2Seqモデルを用いたチャットボット作成 〜英会話のサンプルをTorchで動かす〜</title>
      <link>http://blog.algolab.jp/post/2016/07/30/seq2seq-chatbot/</link>
      <pubDate>Sat, 30 Jul 2016 15:50:23 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/07/30/seq2seq-chatbot/</guid>
      <description>

&lt;p&gt;最近、チャットボットが話題となっていますが、自然な会話を成り立たせること、は大きな課題の一つです。&lt;br /&gt;
ここでは、Deep Learningの一種である、Seq2Seqモデルを用いて、チャットボットを動作させてみます。&lt;br /&gt;
ゴールとして、英語を学習させ、実際に会話を行ってみることを目指します。&lt;/p&gt;

&lt;h2 id=&#34;seq2seq-sequence-to-sequence-モデルとは&#34;&gt;Seq2Seq (Sequence to Sequence) モデルとは&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/30/seq2seq-chatbot//seq2seq.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;平たく言うと、ある文字列から、次の文字列を予測するモデルのことです。&lt;br /&gt;
上記の図では、「ABC」を入力として、「WXYZ」を出力 (予測) しています。&lt;/p&gt;

&lt;p&gt;Seq2Seqモデルの対話タスクへの応用を試みたのがGoogleで、2015年に下記の論文を発表しています。&lt;/p&gt;

&lt;p&gt;A Neural Conversational Model&lt;br /&gt;
&lt;a href=&#34;http://arxiv.org/abs/1506.05869&#34;&gt;http://arxiv.org/abs/1506.05869&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;これまでの対話モデルは、ドメインを絞り (飛行機を予約するなど) 、手でルールを記載する必要があったが、Seq2Seqモデルを用いて対話データを学習させることで、自然な応答ができるようになった、と論文内で述べています。&lt;/p&gt;

&lt;h2 id=&#34;実装例&#34;&gt;実装例&lt;/h2&gt;

&lt;p&gt;Seq2Seqモデルを用いたチャットボットの実装は、色々な人が公開しています。&lt;br /&gt;
&lt;a href=&#34;https://github.com/nicolas-ivanov/seq2seq_chatbot_links&#34;&gt;https://github.com/nicolas-ivanov/seq2seq_chatbot_links&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;今回は、実装例の中で、最も良い結果が出たとされている、以下のリポジトリのコードを動作させてみます。&lt;br /&gt;
&lt;a href=&#34;https://github.com/macournoyer/neuralconvo&#34;&gt;https://github.com/macournoyer/neuralconvo&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;環境構築&#34;&gt;環境構築&lt;/h2&gt;

&lt;p&gt;基本的には下記の手順で進めます。&lt;br /&gt;
&lt;a href=&#34;https://github.com/macournoyer/neuralconvo#installing&#34;&gt;https://github.com/macournoyer/neuralconvo#installing&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;筆者は下記の環境をベースに、追加で必要なLuaモジュールをインストール (更新) しました。&lt;br /&gt;
&lt;a href=&#34;http://blog.algolab.jp/post/2016/08/03/torch-aws-gpu-ubuntu/&#34;&gt;TorchをAWSのGPUインスタンス (Ubuntu 14.04) で動かす&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ luarocks install nn
$ luarocks install rnn
$ luarocks install penlight
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;データセットの準備&#34;&gt;データセットの準備&lt;/h3&gt;

&lt;p&gt;データセットは、下記で公開されている映画の台詞コーパスを用います。&lt;br /&gt;
&lt;a href=&#34;http://www.mpi-sws.org/~cristian/Cornell_Movie-Dialogs_Corpus.html&#34;&gt;http://www.mpi-sws.org/~cristian/Cornell_Movie-Dialogs_Corpus.html&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ git clone https://github.com/macournoyer/neuralconvo.git
$ cd neuralconvo/data
$ wget http://www.mpi-sws.org/~cristian/data/cornell_movie_dialogs_corpus.zip
$ unzip cornell_movie_dialogs_corpus.zip
$ mv cornell\ movie-dialogs\ corpus cornell_movie_dialogs
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;学習&#34;&gt;学習&lt;/h2&gt;

&lt;p&gt;準備が整ったら学習をしてみます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ th train.lua --cuda --dataset 50000 --hiddenSize 1000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;学習時間は4日弱で、エラー率の推移は下記となりました。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/30/seq2seq-chatbot//error.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;h2 id=&#34;会話してみる&#34;&gt;会話してみる&lt;/h2&gt;

&lt;p&gt;学習したモデルを用いて実際に会話をしてみました。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ th eval.lua
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;you&amp;gt;&lt;/strong&gt; Hello?&lt;br /&gt;
&lt;strong&gt;neuralconvo&amp;gt;&lt;/strong&gt; Hello, darling.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;you&amp;gt;&lt;/strong&gt; How are you?&lt;br /&gt;
&lt;strong&gt;neuralconvo&amp;gt;&lt;/strong&gt; I&amp;rsquo;m fine.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;you&amp;gt;&lt;/strong&gt; Are you a machine?&lt;br /&gt;
&lt;strong&gt;neuralconvo&amp;gt;&lt;/strong&gt; No, i don&amp;rsquo;t.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;you&amp;gt;&lt;/strong&gt; Are you intelligent?&lt;br /&gt;
&lt;strong&gt;neuralconvo&amp;gt;&lt;/strong&gt; No.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;それっぽい会話は成り立つようです。哲学的な質問をしてみます。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;you&amp;gt;&lt;/strong&gt; What is the purpose of living?&lt;br /&gt;
&lt;strong&gt;neuralconvo&amp;gt;&lt;/strong&gt; I&amp;rsquo;ve been watching over the phone thing&amp;hellip;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;うーん。深い&amp;hellip;!?&lt;/p&gt;

&lt;h2 id=&#34;会話が成り立たないケースもある&#34;&gt;会話が成り立たないケースもある&lt;/h2&gt;

&lt;p&gt;上記のように会話として成立するものもあれば、全く成り立たないケースもありました。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;you&amp;gt;&lt;/strong&gt; What color is the sky?&lt;br /&gt;
&lt;strong&gt;neuralconvo&amp;gt;&lt;/strong&gt; The other plate is currently in new york, in some kind of a tree in a decent, don&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;

&lt;p&gt;実用的に使える精度か、という点では疑問符が残るものの、End-to-End で学習ができるというのは魅力的です。&lt;br /&gt;
いかに有用なデータセットを構築するか、が重要なポイントとなってきそうです。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>