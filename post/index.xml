<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on ALGO GEEKS</title>
    <link>http://blog.algolab.jp/post/</link>
    <description>Recent content in Posts on ALGO GEEKS</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Mon, 01 Aug 2016 12:11:45 +0900</lastBuildDate>
    <atom:link href="http://blog.algolab.jp/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>AIãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚­ãƒƒã‚¯ã‚ªãƒ• ã€œãƒãƒ ã‚±ãƒ„ã‚’èªè­˜ã—ãŸã„ã€œ</title>
      <link>http://blog.algolab.jp/post/2016/08/01/ai-training-kick-off/</link>
      <pubDate>Mon, 01 Aug 2016 12:11:45 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/08/01/ai-training-kick-off/</guid>
      <description>

&lt;p&gt;å¼Šç¤¾ã§å®Ÿæ–½ã—ã¦ã„ã‚‹ &lt;a href=&#34;http://blog.algolab.jp/post/2016/07/25/ai-training/&#34;&gt;AIãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°&lt;/a&gt; ã®å®Ÿæ³ä¸­ç¶™ã‚·ãƒªãƒ¼ã‚ºã¨ãªã‚Šã¾ã™ã€‚&lt;br /&gt;
ä»Šå›ã¯ã€Wã•ã‚“ã®ã‚­ãƒƒã‚¯ã‚ªãƒ•ã®å†…å®¹ã«ã¤ã„ã¦ãŠå±Šã‘ã—ã¾ã™ã€‚&lt;/p&gt;

&lt;h2 id=&#34;ãƒãƒ ã‚±ãƒ„&#34;&gt;ãƒãƒ ã‚±ãƒ„&lt;/h2&gt;

&lt;p&gt;ã€Œã‚ã®ãƒ»ãƒ»ã§ã™ã­ã€‚ãã®ãƒ»ãƒ»ãƒ»ã€‚ãƒãƒ ã‚±ãƒ„ã‚’èªè­˜ã—ãŸã„ã‚“ã§ã™ã€‚ã€&lt;/p&gt;

&lt;p&gt;ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®æœ€åˆã«ã‚´ãƒ¼ãƒ«ã‚’è¨­å®šã™ã‚‹ã®ã§ã™ãŒã€ãã®éš›ã€Wã•ã‚“ã‹ã‚‰å‡ºãŸã²ã¨è¨€ç›®ãŒã“ã‚Œã§ã—ãŸã€‚&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/01/ai-training-kick-off//hamuketu.jpg&#34;/&gt;
  
  &lt;figcaption&gt;&lt;p&gt;http://hamuketu.blog.jp/archives/51191945.html&lt;/p&gt;&lt;/figcaption&gt;
  
&lt;/figure&gt;


&lt;p&gt;ãƒãƒ ã‚±ãƒ„ã¨ã¯æœ€è¿‘ãƒãƒƒãƒˆã‚„æ›¸ç±ã§è©±é¡Œã«ãªã£ãŸãƒãƒ ã‚¹ã‚¿ãƒ¼ã®ãŠã—ã‚Šã®ã“ã¨ã§ã€
ç‹¬ç‰¹ã®å¯æ„›ã•ã§è©•åˆ¤ã¨ãªã£ãŸã®ã§ç›®ã«ã•ã‚ŒãŸæ–¹ã‚‚ã„ã‚‰ã£ã—ã‚ƒã‚‹ã®ã§ã¯ãªã„ã§ã—ã‚‡ã†ã‹ã€‚&lt;/p&gt;

&lt;p&gt;Wã•ã‚“ã®è©±ã‚’è©³ã—ãã†ã‹ãŒã£ã¦ã¿ã‚‹ã¨ã€å¦»ãŒãƒãƒ ã‚±ãƒ„ã®å¾—ã‚‚è¨€ã‚ã‚Œã¬å¯æ„›ã•ã«èº«ã‚‚ã ãˆã¦ã„ãŸæ™‚ã«ã€Œãƒãƒ ã‚±ãƒ„ç”»åƒãŒè‡ªå‹•çš„ã«é€ã‚‰ã‚Œã¦ãã‚‹ã‚¢ãƒ—ãƒªãŒã‚ã£ãŸã‚‰ãªã€ã¨æ€ã£ãŸã¨ã®ã“ã¨ã§ã™ã€‚&lt;/p&gt;

&lt;p&gt;ç´ æ™´ã‚‰ã—ã„ï¼ã§ã¯ãƒãƒ ã‚±ãƒ„ã§ã„ãã¾ã—ã‚‡ã†ã€ã¨é€Ÿæ”»ã§ãƒ†ãƒ¼ãƒãŒæ±ºã¾ã‚Šã€ãã®å¾Œã¯å¤§ã®ç”·äºŒäººã§ã€Œãƒãƒ ã‚±ãƒ„ã®ç‰¹å¾´ã€ã‚„ã€Œã“ã£ã¡ãŒãƒãƒ ã‚±ãƒ„ã€ã“ã£ã¡ã¯ãƒãƒ ã‚±ãƒ„ã˜ã‚ƒãªã„ã€ãªã©ã¨ç†±ã„è­°è«–ã‚’äº¤ã‚ã—ã¤ã¤æˆæœç‰©ã®ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’ã‚ˆã‚Šå…·ä½“çš„ã«è©±ã—åˆã„ã¾ã—ãŸã€‚&lt;/p&gt;

&lt;p&gt;å®Ÿéš›ã«ã‚¢ãƒ—ãƒªã‚’ä½œã‚‹ã¨ãªã‚‹ã¨æ§˜ã€…ãªå·¥ç¨‹ãŒã‚ã‚Šã¾ã™ãŒã€ãƒãƒ ã‚±ãƒ„ç”»åƒã‚’é›†ã‚ã‚‹æ®µéšã§äººå·¥çŸ¥èƒ½ãŒãƒãƒ ã‚±ãƒ„ã‚’èªè­˜ã—ã¦ãã‚Œã‚Œã°ã€ãƒãƒ ã‚±ãƒ„ã§ã‚ã‚‹ã‹ã©ã†ã‹ã®åˆ¤å®šã‚’äººé–“ãŒè¡Œã‚ãªãã¦ã‚ˆããªã‚Šã¾ã™ã€‚&lt;/p&gt;

&lt;p&gt;ã“ã®èªè­˜ã—ã¦åˆ¤å®šã™ã‚‹ãƒ•ã‚§ãƒ¼ã‚ºã‚’ä»Šå›ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§å–ã‚Šæ‰±ã„ã€å‡ºæ¥ä¸ŠãŒã£ãŸãƒ¢ãƒ‡ãƒ«ã‚’èª°ã«ã§ã‚‚è§¦ã£ã¦ã‚‚ã‚‰ãˆã‚‹å½¢ã«ã—ã‚ˆã†ã¨ã„ã†ã“ã¨ã§ã€&lt;/p&gt;

&lt;p&gt;ã€Œãƒãƒ ã‚±ãƒ„ç”»åƒã‚’å…¥åŠ›ã™ã‚‹ã¨èªè­˜çµæœã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡ºåŠ›ã§å¾—ã‚‰ã‚Œã‚‹Webã‚¢ãƒ—ãƒªã‚’ä½œã‚‹ã€&lt;/p&gt;

&lt;p&gt;ã‚’ã‚´ãƒ¼ãƒ«ã¨ã™ã‚‹ã“ã¨ã«ãªã‚Šã¾ã—ãŸã€‚&lt;/p&gt;

&lt;h2 id=&#34;ç¾çŠ¶ã‚’æŠŠæ¡ã™ã‚‹&#34;&gt;ç¾çŠ¶ã‚’æŠŠæ¡ã™ã‚‹&lt;/h2&gt;

&lt;p&gt;ã‚´ãƒ¼ãƒ«ãŒæ±ºã¾ã£ãŸã¨ã“ã‚ã§ã€Wã•ã‚“ã®ç¾çŠ¶ã®ã‚¹ã‚­ãƒ«ã‚’ãƒ’ã‚¢ãƒªãƒ³ã‚°ã—ã¦ã„ãã¾ã™ã€‚&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ã‚·ã‚¹ãƒ†ãƒ é–‹ç™º

&lt;ul&gt;
&lt;li&gt;webã‚µãƒ¼ãƒ“ã‚¹ã‚„æ¥­å‹™ã‚·ã‚¹ãƒ†ãƒ ã®è¨­è¨ˆã€é–‹ç™ºã€é‹ç”¨ã‚’10å¹´&lt;/li&gt;
&lt;li&gt;é–‹ç™ºè¨€èªã¯Javaã€Rubyã€Pythonã€PHP&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;æ©Ÿæ¢°å­¦ç¿’

&lt;ul&gt;
&lt;li&gt;AIã«ã‚ˆã‚Šè§£æ±ºã§ãã‚‹èª²é¡Œã¨ç†è«–ãŒæ¦‚å¿µãƒ¬ãƒ™ãƒ«ã§ã‚ã‹ã‚‹ã€‚åˆ†é¡ã‚„äºˆæ¸¬ãŒã§ãã‚‹ã¨ã‹ã€æ•™å¸«ã‚ã‚Šå­¦ç¿’ãƒ»æ•™å¸«ãªã—å­¦ç¿’ã®é•ã„ãŒåˆ†ã‹ã‚‹ã€‚&lt;/li&gt;
&lt;li&gt;çµ±è¨ˆã‚„è‡ªç„¶è¨€èªå‡¦ç†ã‚’ã‹ã˜ã£ãŸã“ã¨ãŒã‚ã‚‹&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;å—è¬›è€…ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹æ™‚ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨ã—ã¦ã”å‚è€ƒã«ã—ã¦ã„ãŸã ã‘ã‚Œã°ã¨æ€ã„ã¾ã™ã€‚&lt;/p&gt;

&lt;h2 id=&#34;æ¬¡å›äºˆå‘Š&#34;&gt;æ¬¡å›äºˆå‘Š&lt;/h2&gt;

&lt;p&gt;ã ã„ã¶åŸºç¤ãŒã‚ã‚Šãã†ãªã®ã§ã€ç•³ã¿ã“ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’æ´ã‚“ã§ã„ãŸã ã„ãŸä¸Šã§ã€æˆæœç‰©å®Œæˆã¾ã§ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’æç¤ºã—ã€ãã‚Œã‚’ã“ãªã—ã¦ã‚‚ã‚‰ã†ã€ã¨ã„ã†ã‚¹ã‚¿ã‚¤ãƒ«ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹ã“ã¨ã«ã—ã¾ã—ãŸã€‚&lt;/p&gt;

&lt;p&gt;æ¬¡å›ã¯ã€ç•³ã¿ã“ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’æ´ã‚“ã§ã‚‚ã‚‰ã†éš›ã«è¡Œã£ãŸè¬›ç¿’ã®å†…å®¹ã«ã¤ã„ã¦ãŠå±Šã‘ã—ã¾ã™ã€‚&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Seq2Seqãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆä½œæˆ ã€œè‹±ä¼šè©±ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å‹•ã‹ã™ã€œ</title>
      <link>http://blog.algolab.jp/post/2016/07/30/seq2seq-chatbot/</link>
      <pubDate>Sat, 30 Jul 2016 15:50:23 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/07/30/seq2seq-chatbot/</guid>
      <description>

&lt;p&gt;æœ€è¿‘ã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆãŒè©±é¡Œã¨ãªã£ã¦ã„ã¾ã™ãŒã€è‡ªç„¶ãªä¼šè©±ã‚’æˆã‚Šç«‹ãŸã›ã‚‹ã“ã¨ã€ã¯å¤§ããªèª²é¡Œã®ä¸€ã¤ã§ã™ã€‚&lt;br /&gt;
ã“ã“ã§ã¯ã€Deep Learningã®ä¸€ç¨®ã§ã‚ã‚‹ã€Seq2Seqãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦ã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚’å‹•ä½œã•ã›ã¦ã¿ã¾ã™ã€‚&lt;br /&gt;
ã‚´ãƒ¼ãƒ«ã¨ã—ã¦ã€è‹±èªã‚’å­¦ç¿’ã•ã›ã€å®Ÿéš›ã«ä¼šè©±ã‚’è¡Œã£ã¦ã¿ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¾ã™ã€‚&lt;/p&gt;

&lt;h2 id=&#34;seq2seq-sequence-to-sequence-ãƒ¢ãƒ‡ãƒ«ã¨ã¯&#34;&gt;Seq2Seq (Sequence to Sequence) ãƒ¢ãƒ‡ãƒ«ã¨ã¯&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/30/seq2seq-chatbot//seq2seq.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;å¹³ãŸãè¨€ã†ã¨ã€ã‚ã‚‹æ–‡å­—åˆ—ã‹ã‚‰ã€æ¬¡ã®æ–‡å­—åˆ—ã‚’äºˆæ¸¬ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ã“ã¨ã§ã™ã€‚&lt;br /&gt;
ä¸Šè¨˜ã®å›³ã§ã¯ã€ã€ŒABCã€ã‚’å…¥åŠ›ã¨ã—ã¦ã€ã€ŒWXYZã€ã‚’å‡ºåŠ› (äºˆæ¸¬) ã—ã¦ã„ã¾ã™ã€‚&lt;/p&gt;

&lt;p&gt;Seq2Seqãƒ¢ãƒ‡ãƒ«ã®å¯¾è©±ã‚¿ã‚¹ã‚¯ã¸ã®å¿œç”¨ã‚’è©¦ã¿ãŸã®ãŒGoogleã§ã€2015å¹´ã«ä¸‹è¨˜ã®è«–æ–‡ã‚’ç™ºè¡¨ã—ã¦ã„ã¾ã™ã€‚&lt;/p&gt;

&lt;p&gt;A Neural Conversational Model&lt;br /&gt;
&lt;a href=&#34;http://arxiv.org/abs/1506.05869&#34;&gt;http://arxiv.org/abs/1506.05869&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ã“ã‚Œã¾ã§ã®å¯¾è©±ãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’çµã‚Š (é£›è¡Œæ©Ÿã‚’äºˆç´„ã™ã‚‹ãªã©) ã€æ‰‹ã§ãƒ«ãƒ¼ãƒ«ã‚’è¨˜è¼‰ã™ã‚‹å¿…è¦ãŒã‚ã£ãŸãŒã€Seq2Seqãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã•ã›ã‚‹ã“ã¨ã§ã€è‡ªç„¶ãªå¿œç­”ãŒã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸã€ã¨è«–æ–‡å†…ã§è¿°ã¹ã¦ã„ã¾ã™ã€‚&lt;/p&gt;

&lt;h2 id=&#34;å®Ÿè£…ä¾‹&#34;&gt;å®Ÿè£…ä¾‹&lt;/h2&gt;

&lt;p&gt;Seq2Seqãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®å®Ÿè£…ã¯ã€è‰²ã€…ãªäººãŒå…¬é–‹ã—ã¦ã„ã¾ã™ã€‚&lt;br /&gt;
&lt;a href=&#34;https://github.com/nicolas-ivanov/seq2seq_chatbot_links&#34;&gt;https://github.com/nicolas-ivanov/seq2seq_chatbot_links&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ä»Šå›ã¯ã€å®Ÿè£…ä¾‹ã®ä¸­ã§ã€æœ€ã‚‚è‰¯ã„çµæœãŒå‡ºãŸã¨ã•ã‚Œã¦ã„ã‚‹ã€ä»¥ä¸‹ã®ãƒªãƒã‚¸ãƒˆãƒªã®ã‚³ãƒ¼ãƒ‰ã‚’å‹•ä½œã•ã›ã¦ã¿ã¾ã™ã€‚
&lt;a href=&#34;https://github.com/macournoyer/neuralconvo&#34;&gt;https://github.com/macournoyer/neuralconvo&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;ç’°å¢ƒæ§‹ç¯‰&#34;&gt;ç’°å¢ƒæ§‹ç¯‰&lt;/h2&gt;

&lt;p&gt;åŸºæœ¬çš„ã«ã¯ä¸‹è¨˜ã®æ‰‹é †ã§é€²ã‚ã¾ã™ã€‚&lt;br /&gt;
&lt;a href=&#34;https://github.com/macournoyer/neuralconvo#installing&#34;&gt;https://github.com/macournoyer/neuralconvo#installing&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰&#34;&gt;ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/macournoyer/neuralconvo.git
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;torchã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«&#34;&gt;Torchã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«&lt;/h3&gt;

&lt;p&gt;å…¬å¼ã«å¾“ã£ã¦ã€ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚&lt;br /&gt;
&lt;a href=&#34;http://torch.ch/docs/getting-started.html&#34;&gt;http://torch.ch/docs/getting-started.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ç’°å¢ƒã«ã‚ˆã£ã¦å©ãã‚³ãƒãƒ³ãƒ‰ãŒé•ã†ãŸã‚ã€è©³ç´°ã¯ä¸Šè¨˜ãƒªãƒ³ã‚¯ã‚’ã”å‚ç…§ãã ã•ã„ã€‚&lt;br /&gt;
ä¸‹è¨˜ã¯ã€Ubuntu + zshã§ã®ä¾‹ã§ã™ã€‚&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/torch/distro.git ~/torch --recursive
cd ~/torch; bash install-deps;
./install.sh
source ~/.zshrc
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;luaãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«&#34;&gt;Luaãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;luarocks install nn
luarocks install rnn
luarocks install penlight
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;GPUã‚’ç”¨ã„ã¦å­¦ç¿’ã‚’è¡Œã†ã®ã§ã€ä¸‹è¨˜ã‚‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;luarocks install cutorch
luarocks install cunn
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™&#34;&gt;ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™&lt;/h3&gt;

&lt;p&gt;ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€ä¸‹è¨˜ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹æ˜ ç”»ã®å°è©ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ç”¨ã„ã¾ã™ã€‚&lt;br /&gt;
&lt;a href=&#34;http://www.mpi-sws.org/~cristian/Cornell_Movie-Dialogs_Corpus.html&#34;&gt;http://www.mpi-sws.org/~cristian/Cornell_Movie-Dialogs_Corpus.html&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd neuralconvo/data
wget http://www.mpi-sws.org/~cristian/data/cornell_movie_dialogs_corpus.zip
unzip cornell_movie_dialogs_corpus.zip
mv cornell\ movie-dialogs\ corpus cornell_movie_dialogs
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;å­¦ç¿’&#34;&gt;å­¦ç¿’&lt;/h2&gt;

&lt;p&gt;æº–å‚™ãŒæ•´ã£ãŸã‚‰å­¦ç¿’ã‚’ã—ã¦ã¿ã¾ã™ã€‚&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;th train.lua --cuda --dataset 50000 --hiddenSize 1000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;å­¦ç¿’ã«ã¯ AWSã®g2.2xlargeã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”¨ã„ã€å­¦ç¿’æ™‚é–“ã¯4æ—¥å¼±ã§ã—ãŸã€‚&lt;br /&gt;
ãªãŠã€ã‚¨ãƒ©ãƒ¼ç‡ã®æ¨ç§»ã¯ä¸‹è¨˜ã¨ãªã‚Šã¾ã—ãŸã€‚&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/30/seq2seq-chatbot//error.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;h2 id=&#34;ãƒ†ã‚¹ãƒˆ&#34;&gt;ãƒ†ã‚¹ãƒˆ&lt;/h2&gt;

&lt;p&gt;å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦å®Ÿéš›ã«ä¼šè©±ã‚’ã—ã¦ã¿ã¾ã—ãŸã€‚&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;th eval.lua
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;you&amp;gt;&lt;/strong&gt; Hello?&lt;br /&gt;
&lt;strong&gt;neuralconvo&amp;gt;&lt;/strong&gt; Hello, darling.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;you&amp;gt;&lt;/strong&gt; How are you?&lt;br /&gt;
&lt;strong&gt;neuralconvo&amp;gt;&lt;/strong&gt; I&amp;rsquo;m fine.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;you&amp;gt;&lt;/strong&gt; Are you a machine?&lt;br /&gt;
&lt;strong&gt;neuralconvo&amp;gt;&lt;/strong&gt; No, i don&amp;rsquo;t.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;you&amp;gt;&lt;/strong&gt; Are you intelligent?&lt;br /&gt;
&lt;strong&gt;neuralconvo&amp;gt;&lt;/strong&gt; No.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ãã‚Œã£ã½ã„ä¼šè©±ã¯æˆã‚Šç«‹ã¤ã‚ˆã†ã§ã™ã€‚å“²å­¦çš„ãªè³ªå•ã‚’ã—ã¦ã¿ã¾ã™ã€‚&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;you&amp;gt;&lt;/strong&gt; What is the purpose of living?&lt;br /&gt;
&lt;strong&gt;neuralconvo&amp;gt;&lt;/strong&gt; I&amp;rsquo;ve been watching over the phone thing&amp;hellip;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ã†ãƒ¼ã‚“ã€‚æ·±ã„&amp;hellip;!?&lt;/p&gt;

&lt;h2 id=&#34;è©•ä¾¡&#34;&gt;è©•ä¾¡&lt;/h2&gt;

&lt;p&gt;ä¸Šè¨˜ã®ã‚ˆã†ã«ä¼šè©±ã¨ã—ã¦æˆç«‹ã™ã‚‹ã‚‚ã®ã‚‚ã‚ã‚Œã°ã€å…¨ãæˆã‚Šç«‹ãŸãªã„ã‚±ãƒ¼ã‚¹ã‚‚ã‚ã‚Šã¾ã—ãŸã€‚&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;you&amp;gt;&lt;/strong&gt; What color is the sky?&lt;br /&gt;
&lt;strong&gt;neuralconvo&amp;gt;&lt;/strong&gt; The other plate is currently in new york, in some kind of a tree in a decent, don&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;çŸ¥è­˜ãŒè¶³ã‚Šãªã„ã®ã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¸è¶³ã§ã€æ–‡ç« ã¨ã—ã¦æˆç«‹ã—ã¦ã„ãªã„ã®ã¯å­¦ç¿’ä¸è¶³ã€ã¨ã„ã£ãŸã¨ã“ã‚ã§ã—ã‚‡ã†ã‹ã€‚&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>éŸ³å£°ã¯æ–°ã—ã„ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚·ãƒ•ãƒˆã«ãªã‚‹ ã€œ2016å¹´åº¦ç‰ˆãƒ¡ã‚¢ãƒªãƒ¼ãƒ»ãƒŸãƒ¼ã‚«ãƒ¼æ°ãƒ¬ãƒãƒ¼ãƒˆã¾ã¨ã‚ã€œ</title>
      <link>http://blog.algolab.jp/post/2016/07/29/mary-meeker-2016/</link>
      <pubDate>Fri, 29 Jul 2016 11:47:41 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/07/29/mary-meeker-2016/</guid>
      <description>

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/D0N5V1PjTsIasR&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;p&gt;&lt;strong&gt;ã€ŒéŸ³å£°ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã¯æ–°ã—ã„ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚·ãƒ•ãƒˆã«ãªã‚‹ã€&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ä¼èª¬ã®ã‚¢ãƒŠãƒªã‚¹ãƒˆã€ãƒ¡ã‚¢ãƒªãƒ¼ãƒ»ãƒŸãƒ¼ã‚«ãƒ¼æ°ã¯ã€ &lt;a href=&#34;http://www.kpcb.com/internet-trends&#34;&gt;ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰&lt;/a&gt; 2016å¹´åº¦ç‰ˆã®ä¸­ã§è¿°ã¹ã¦ã„ã¾ã™ã€‚&lt;br /&gt;
ã“ã“ã§ã¯ã€ãƒ¬ãƒãƒ¼ãƒˆã®ä¸­ã‹ã‚‰ã€éŸ³å£°ã«é–¢ã™ã‚‹ã‚‚ã®ã‚’ã¾ã¨ã‚ã¦ã„ãã¾ã™ã€‚&lt;/p&gt;

&lt;h2 id=&#34;ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®æŠ€è¡“é©æ–°ã¯10å¹´æ¯ã«èµ·ãã‚‹&#34;&gt;ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®æŠ€è¡“é©æ–°ã¯10å¹´æ¯ã«èµ·ãã‚‹&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//114.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;ãƒ’ãƒ¥ãƒ¼ãƒãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®æ­´å²ã‚’æŒ¯ã‚Šè¿”ã£ã¦ã¿ã‚‹ã¨ã€ã“ã“åŠä¸–ç´€ã«ãŠã„ã¦ã¯10å¹´å˜ä½ã§æŠ€è¡“é©æ–°ãŒèµ·ãã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚&lt;br /&gt;
iPhoneã«ã‚ˆã‚‹ã€ã‚¿ãƒƒãƒ + ã‚«ãƒ¡ãƒ©ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ãŒç™»å ´ã—ãŸã®ãŒ 2007å¹´ã€‚&lt;br /&gt;
æ¬¡ã®10å¹´ã§ã¯ã€Siri ã‚„ Amazon Echo ã«ä»£è¡¨ã•ã‚Œã‚‹éŸ³å£°ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ãŒæŠ€è¡“é©æ–°ã‚’èµ·ã“ã™ã ã‚ã†ã€ã¨äºˆæ¸¬ã—ã¦ã„ã¾ã™ã€‚&lt;/p&gt;

&lt;h2 id=&#34;éŸ³å£°ã¯æœ€ã‚‚åŠ¹ç‡ã®è‰¯ã„å…¥åŠ›æ–¹æ³•ã§ã‚ã‚‹&#34;&gt;éŸ³å£°ã¯æœ€ã‚‚åŠ¹ç‡ã®è‰¯ã„å…¥åŠ›æ–¹æ³•ã§ã‚ã‚‹&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//116.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;ãªãœéŸ³å£°ã‹ã€ã¨ã„ã†å•ã„ã«å¯¾ã—ã¦ã€ãƒ¡ãƒªãƒƒãƒˆã¨ç‹¬è‡ªæ€§ã®è¦³ç‚¹ã‹ã‚‰ç†ç”±ã‚’è¿°ã¹ã¦ã„ã¾ã™ã€‚&lt;br /&gt;
ä½•ã‚ˆã‚Šã€ã€Œæ—©ã„ã€ã€Œç°¡å˜ã€ã¨ã„ã†ã®ãŒéŸ³å£°ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®ãƒ¡ãƒªãƒƒãƒˆã§ã—ã‚‡ã†ã€‚&lt;br /&gt;
ã¾ãŸã€ç…©é›‘ãªGUIã‚’å¿…è¦ã¨ã›ãšã€ä½ã‚³ã‚¹ãƒˆã§å ´æ‰€ã‚’ã¨ã‚‰ãªã„ã“ã¨ã‹ã‚‰ã€IoTã¨ã‚‚ç›¸æ€§ãŒè‰¯ã„ã€ã¨ã—ã¦ã„ã¾ã™ã€‚&lt;/p&gt;

&lt;h2 id=&#34;éŸ³å£°èªè­˜ã¯äººé–“ä¸¦ã¿ã«é€²æ­©&#34;&gt;éŸ³å£°èªè­˜ã¯äººé–“ä¸¦ã¿ã«é€²æ­©&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//118.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;Googleã®ç ”ç©¶æˆæœã«ã‚ˆã‚‹ã¨ã€èªå½™æ•°ã€èªè­˜ç²¾åº¦ã¨ã‚‚ã«å¹´ã€…å‘ä¸Šã—ã¦ã„ã¾ã™ã€‚&lt;/p&gt;

&lt;h2 id=&#34;ç‰¹ã«-èªè­˜ç²¾åº¦ã¯ã“ã“æ•°å¹´ã§æ€¥æ¿€ã«å‘ä¸Š&#34;&gt;ç‰¹ã«ã€èªè­˜ç²¾åº¦ã¯ã“ã“æ•°å¹´ã§æ€¥æ¿€ã«å‘ä¸Š&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//119.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;æ•°å¹´å‰ã¾ã§ã¯è‰¯ãã¦80%ç¨‹åº¦ã ã£ãŸã‚‚ã®ãŒã€æœ€è¿‘ã¯90%ã‚’å„ªã«è¶…ãˆã¦ãã¦ã„ã‚‹ã®ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚&lt;br /&gt;
äººå·¥çŸ¥èƒ½ç ”ç©¶ã®æ¨©å¨ã§ã‚ã‚‹ã€Baidu ã® Andrew Ng æ°ã¯ã€ç²¾åº¦ãŒ 99% ã‚’è¶…ãˆã‚‹ã¨ã‚²ãƒ¼ãƒ ãƒã‚§ãƒ³ã‚¸ãƒ£ãƒ¼ã«ãªã‚‹ (= ä¸–ç•ŒãŒå¤‰ã‚ã‚‹) ã¨è¿°ã¹ã¦ã„ã¾ã™ã€‚&lt;br /&gt;
æŠ€è¡“é€²æ­©ã®éµã¨ãªã‚‹ã®ã¯ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã€éŸ³å£°èªè­˜åˆ†é‡ã«ãŠã„ã¦ã¯ã€Baidu ãŒä¸€æ­©ãƒªãƒ¼ãƒ‰ã—ã¦ã„ã‚‹å°è±¡ã§ã™ã€‚&lt;br /&gt;
Baidu ã®è«–æ–‡ã«ã¤ã„ã¦ã¯ã€ä¸‹è¨˜ã®è¨˜äº‹å†…ã§ã‚‚å–ã‚Šä¸Šã’ã¦ã„ã¾ã™ã®ã§ã”å‚ç…§ãã ã•ã„ã€‚&lt;br /&gt;
&lt;a href=&#34;http://blog.algolab.jp/post/2016/07/21/icml-2016-reading/#deep-speech-2-end-to-end-speech-recognition-in-english-and-mandarin&#34;&gt;ICML2016èª­ã¿ä¼š ã¾ã¨ã‚&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;éŸ³å£°ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®åˆ©ç”¨ã¯æŠ€è¡“ã®é€²æ­©ãŒç‰½å¼•&#34;&gt;éŸ³å£°ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®åˆ©ç”¨ã¯æŠ€è¡“ã®é€²æ­©ãŒç‰½å¼•&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//121.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;å…¥åŠ›ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ãŒã‚­ãƒ¼ãƒœãƒ¼ãƒ‰ã‹ã‚‰éŸ³å£°ã«ç½®ãæ›ã‚ã‚‹ã®ã¯ã¾ã å°‘ã—æ—©ã„ã¨å‰ç½®ãã‚’ã—ãªãŒã‚‰ã€åˆ©ç”¨çŠ¶æ³ã«ã¤ã„ã¦ã¾ã¨ã‚ã¦ã„ã¾ã™ã€‚&lt;br /&gt;
éŸ³å£°ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®åˆ©ç”¨è€…ã¯2015å¹´æ™‚ç‚¹ã§65%ã§ã€ä½¿ã„å§‹ã‚ã‚‹ãã£ã‹ã‘ã¨ã—ã¦ã¯ã€ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢æŠ€è¡“ã®é€²æ­©ã®ç†ç”±ãŒä¸€ç•ªã¨ã®ã“ã¨ã§ã™ã€‚&lt;/p&gt;

&lt;h2 id=&#34;éŸ³å£°æ¤œç´¢ã®åˆ©ç”¨ã¯é–‹å§‹æ™‚ç‚¹ã®35å€ã«&#34;&gt;éŸ³å£°æ¤œç´¢ã®åˆ©ç”¨ã¯é–‹å§‹æ™‚ç‚¹ã®35å€ã«&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//122.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;iPhoneãŠã‚ˆã³GoogleãŒéŸ³å£°æ¤œç´¢ã‚’é–‹å§‹ã—ãŸã®ãŒ2008å¹´ã§ã™ãŒã€ãã®æ™‚ã«æ¯”ã¹ã€åˆ©ç”¨å›æ•°ã¯å³è‚©ã‚ãŒã‚Šã«ä¼¸ã³ã¦ã„ã¾ã™ã€‚&lt;/p&gt;

&lt;h2 id=&#34;ã‚¿ã‚¤ãƒ”ãƒ³ã‚°ãŒé›£ã—ã„ä¸­å›½èªã§ã¯ã•ã‚‰ã«ä¼¸é•·&#34;&gt;ã‚¿ã‚¤ãƒ”ãƒ³ã‚°ãŒé›£ã—ã„ä¸­å›½èªã§ã¯ã•ã‚‰ã«ä¼¸é•·&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//123.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;Baiduã®åˆ©ç”¨çŠ¶æ³ã‚’è¦‹ã‚‹ã¨ã€éŸ³å£°å…¥åŠ›ã€éŸ³å£°èª­ã¿ä¸Šã’ã¨ã‚‚ã«ä¼¸ã³ã¦ã„ã¾ã™ã€‚&lt;br /&gt;
ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã«ãŠã‘ã‚‹è¨€èªã®ã‚¿ã‚¤ãƒ”ãƒ³ã‚°ã®ã—ã‚„ã™ã•ã€ã‚‚éŸ³å£°å…¥åŠ›ã¸ã®åˆ©ç”¨ã¸å½±éŸ¿ã‚’ä¸ãˆãã†ã§ã™ã€‚&lt;/p&gt;

&lt;h2 id=&#34;1æ—¥ã«6-8å›éŸ³å£°æ¤œç´¢ã™ã‚‹&#34;&gt;1æ—¥ã«6-8å›éŸ³å£°æ¤œç´¢ã™ã‚‹&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//124.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;&lt;a href=&#34;http://www.soundhound.com/hound&#34;&gt;Hound&lt;/a&gt; (éŸ³å£°ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã‚¢ãƒ—ãƒª) ã®ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹ã¨ã€1æ—¥ã§6-8å›éŸ³å£°æ¤œç´¢ã‚’è¡Œã†ã‚ˆã†ã§ã™ã€‚&lt;br /&gt;
ã‚«ãƒ†ã‚´ãƒªã¨ã—ã¦ã¯ã€ã€Œä¸€èˆ¬æƒ…å ±ã€ã€Œã‚¨ãƒ³ã‚¿ãƒ¼ãƒ†ã‚¤ãƒ¡ãƒ³ãƒˆã€ã€Œåœ°åŸŸæƒ…å ±ã€ã€Œã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã€ã®4ã¤ã«ã¾ãŸãŒã‚‹ã€ã¨ã®ã“ã¨ã§ã™ã€‚&lt;/p&gt;

&lt;h2 id=&#34;2020å¹´ã«ã¯éŸ³å£°æ¤œç´¢ãŒ50-ã‚’è¶…ãˆã‚‹&#34;&gt;2020å¹´ã«ã¯éŸ³å£°æ¤œç´¢ãŒ50%ã‚’è¶…ãˆã‚‹&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//125.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;éŸ³å£°æ¤œç´¢ã®åˆ©ç”¨ã«ã¤ã„ã¦ã€éå»ã€ç¾åœ¨ã€æœªæ¥ã‚’ã¾ã¨ã‚ã¦ã„ã¾ã™ã€‚&lt;br /&gt;
Adrew Ngæ°ã¯ã€2020å¹´ã«ã¯æ¤œç´¢ã®åŠåˆ†ä»¥ä¸ŠãŒéŸ³å£°ã‹ç”»åƒã«ãªã‚‹ã€ã¨äºˆæ¸¬ã—ã¦ã„ã¾ã™ã€‚&lt;/p&gt;

&lt;h2 id=&#34;ãƒãƒ³ã‚ºãƒ•ãƒªãƒ¼-ç”»é¢ãƒ•ãƒªãƒ¼&#34;&gt;ãƒãƒ³ã‚ºãƒ•ãƒªãƒ¼ &amp;amp; ç”»é¢ãƒ•ãƒªãƒ¼&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//127.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;éŸ³å£°ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’ä½¿ã†ç†ç”±ã®ãƒˆãƒƒãƒ—ãŒã€ã€Œæ‰‹ (ã‚‚ã—ãã¯ç”»é¢) ãŒãµã•ãŒã£ã¦ã„ã‚‹æ™‚ã«ä¾¿åˆ©ã ã‹ã‚‰ã€ã§ã€&lt;br /&gt;
åˆ©ç”¨ã‚·ãƒãƒ¥ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã—ã¦ã¯ã€ã€Œå®¶ã€ã€Œè»Šã€ã€Œç§»å‹•ä¸­ã€ãŒå¤§éƒ¨åˆ†ã‚’å ã‚ã¦ã„ã¾ã™ã€‚&lt;/p&gt;

&lt;h2 id=&#34;ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã¯æ§‹ç¯‰ã•ã‚Œ-ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ã®å‹•ãã‚‚é€Ÿã„&#34;&gt;ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã¯æ§‹ç¯‰ã•ã‚Œã€ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ã®å‹•ãã‚‚é€Ÿã„&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//129.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;å‰ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã§è¿°ã¹ãŸã€Œå®¶ã€ã€Œè»Šã€ã€Œç§»å‹•ä¸­ã€ã«ãŠã„ã¦ã€Amazon Alexaã¯æ§˜ã€…ãªOEMã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚&lt;br /&gt;
ã¾ãŸã€Alexaã‚’æ‹¡å¼µã§ãã‚‹Alexa Skills Kitã®é–‹ç™ºã‚‚ç››ã‚“ã«ãªã£ã¦ã„ã¾ã™ã€‚&lt;/p&gt;

&lt;h2 id=&#34;ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ã‚‚è¿…é€Ÿã«&#34;&gt;ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ã‚‚è¿…é€Ÿã«&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//130.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;Amazonã¯ã€ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ã‚’ãƒ¢ãƒã‚¤ãƒ«ã‚¢ãƒ—ãƒªã‹ã‚‰éŸ³å£°å…¥åŠ›ã¸ç½®ãæ›ãˆã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚&lt;/p&gt;

&lt;h2 id=&#34;amazon-echoã®æ‰€æœ‰ç‡ã¯5&#34;&gt;Amazon Echoã®æ‰€æœ‰ç‡ã¯5%&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//131.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;CIRPã«ã‚ˆã‚‹ã¨ã€AmazonEchoã®æ‰€æœ‰è€…ã¯5%ã§ã€èªçŸ¥åº¦ã¯61%ã¨ã®ã“ã¨ã§ã™ã€‚&lt;/p&gt;

&lt;h2 id=&#34;2016å¹´ã¯ç”£æ¥­ã®å¤‰ã‚ã‚Šç›®ã¨ãªã‚‹&#34;&gt;2016å¹´ã¯ç”£æ¥­ã®å¤‰ã‚ã‚Šç›®ã¨ãªã‚‹&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//133.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ç”£æ¥­ã®å¤‰ã‚ã‚Šç›®ã¯å¾Œã‹ã‚‰æŒ¯ã‚Šè¿”ã‚‹ã¨æ˜ç¢ºãªã‚‚ã®ã§ã‚ã‚‹ã¨å‰ç½®ãã—ãŸä¸Šã§ã€iPhoneã®å£²ä¸ŠãŒ2015å¹´ã«ãƒ”ãƒ¼ã‚¯ã‚’è¿ãˆãŸã“ã¨ã‚’åˆ†å²ç‚¹ã¨æ‰ãˆã€ä»Šå¾Œã¯Amazon Echoã®å£²ä¸ŠãŒæ€¥æ¿€ã«ä¼¸ã³ã‚‹ã®ã§ã¯ãªã„ã‹ã€ã¨ç· ã‚ããã£ã¦ã„ã¾ã™ã€‚&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AIãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã˜ã‚ã¾ã—ãŸ</title>
      <link>http://blog.algolab.jp/post/2016/07/25/ai-training/</link>
      <pubDate>Mon, 25 Jul 2016 16:35:26 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/07/25/ai-training/</guid>
      <description>

&lt;p&gt;å¼Šç¤¾ã§ã¯ã€ITã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å‘ã‘ã«æ©Ÿæ¢°å­¦ç¿’ã‚¹ã‚­ãƒ«ã®ç¿’å¾—ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹è©¦ã¿ (AIãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°) ã‚’ã¯ã˜ã‚ã¾ã—ãŸã€‚&lt;br /&gt;
ã“ã“ã§ã¯ã€1äººç›®ã®å—è¬›è€…ã§ã‚ã‚‹Wã•ã‚“ã®è¨±å¯ã‚’é ‚ãã€å®Ÿæ³ä¸­ç¶™ã¨ã„ã†å½¢ã§ã€ãã®å†…å®¹ã‚’éšæ™‚å…¬é–‹ã—ã¦ã„ããŸã„ã¨æ€ã„ã¾ã™ã€‚&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/25/ai-training//training.jpg&#34;/&gt;
  
  &lt;figcaption&gt;&lt;p&gt;http://www.ashinari.com/2009/07/18-024460.php&lt;/p&gt;&lt;/figcaption&gt;
  
&lt;/figure&gt;


&lt;h2 id=&#34;ãªãœã‚„ã‚‹ã®ã‹&#34;&gt;ãªãœã‚„ã‚‹ã®ã‹&lt;/h2&gt;

&lt;p&gt;ä¸€è¨€ã§è¨€ãˆã°ã€æŠ€è¡“ç¿’å¾—ã¾ã§ã®æœ€çŸ­çµŒè·¯ãŒäººã«ã‚ˆã£ã¦ç•°ãªã‚‹ãŸã‚ã€ã§ã™ã€‚&lt;/p&gt;

&lt;p&gt;æ©Ÿæ¢°å­¦ç¿’ãŒã‚«ãƒãƒ¼ã™ã‚‹é ˜åŸŸã¯ã¨ã¦ã‚‚åºƒç¯„å›²ã«æ¸¡ã‚‹ãŸã‚ã€ã©ã®ã‚ˆã†ãªã“ã¨ã‚’èº«ã«ã¤ã‘ãŸã„ã‹ (ã‚´ãƒ¼ãƒ«)ã€ãŠã‚ˆã³ã€ç¾çŠ¶ã®ã‚¹ã‚­ãƒ«ã«ã‚ˆã£ã¦ã€ç¿’å¾—çµŒè·¯ãŒä¸€æœ¬é“ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚&lt;/p&gt;

&lt;p&gt;ä¸€æ–¹ã§ã€æ›¸ç±ãªã©ã®æ•™æã¯ã€ä½“ç³»çš„ãƒ»ç¶²ç¾…çš„ã«è¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ã‚‚ã®ã®ã€å€‹ã€…ã«æœ€é©åŒ–ã•ã‚Œã¦ã¯ã„ã¾ã›ã‚“ã€‚&lt;/p&gt;

&lt;p&gt;ãã®ãŸã‚ã€æ©Ÿæ¢°å­¦ç¿’ã‚¹ã‚­ãƒ«ã‚’èº«ã«ã¤ã‘ã‚ˆã†ã¨ã—ãŸéš›ã€ä½•ã‹ã‚‰æ‰‹ã‚’ã¤ã‘ã¦è‰¯ã„ã‹ã‚ã‹ã‚‰ãªã„ã€å…¥é–€æ›¸ã‹ã‚‰å§‹ã‚ã¦ã¿ãŸãŒã‚´ãƒ¼ãƒ«ã«ã©ã†ç¹‹ãŒã‚‹ã‹è¦‹ãˆãšæŒ«æŠ˜ã—ã¦ã—ã¾ã£ãŸã€ã¨ã„ã†ã‚ˆã†ãªã‚±ãƒ¼ã‚¹ãŒã‚ˆãè¦‹ã‚‰ã‚Œã¾ã™ã€‚&lt;/p&gt;

&lt;p&gt;ãã“ã§ã€å€‹ã€…ã«æœ€é©åŒ–ã—ãŸé“ç­‹ã‚’æç¤ºã™ã‚‹ã“ã¨ã§ä¸Šè¨˜ã®å•é¡ŒãŒè§£æ±ºã§ãã‚‹ã®ã§ã¯ãªã„ã‹ã€ã¨è€ƒãˆã€AIãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹ã“ã¨ã«ã—ã¾ã—ãŸã€‚&lt;/p&gt;

&lt;h2 id=&#34;ã©ã†ã‚„ã£ã¦ã‚„ã‚‹ã®ã‹&#34;&gt;ã©ã†ã‚„ã£ã¦ã‚„ã‚‹ã®ã‹&lt;/h2&gt;

&lt;p&gt;åŸºæœ¬çš„ã«ã¯ã€å¼Šç¤¾ã¯é“ç­‹ã‚’ç¤ºã™å½¹å‰²ã«å¾¹ã—ã€ç‹¬ç¿’ã¨ã„ã†å½¢ã§é€²ã‚ã•ã›ã¦ã„ãŸã ã„ã¦ã„ã¾ã™ã€‚&lt;/p&gt;

&lt;p&gt;å…·ä½“çš„ã«ã¯ã€é€±1å›ç¨‹åº¦ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§ãŠæ‰“ã¡åˆã‚ã›ã‚’ã•ã›ã¦ã„ãŸã ããªãŒã‚‰ã€ç¾æ™‚ç‚¹ã§ã®ç¿’å¾—çŠ¶æ³ã‚’è¸ã¾ãˆã€é“ç­‹ã‚’æŸ”è»Ÿã«æç¤ºã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚&lt;/p&gt;

&lt;p&gt;ã¾ãŸã€ç†è§£ã®åŠ©ã‘ã«ãªã‚‹ã¨åˆ¤æ–­ã—ãŸå ´åˆã«ã¯ã€ãã®å ´ã§è¬›ç¿’ã¨ã„ã†å½¢ã‚’å–ã‚‹å ´åˆã‚‚ã‚ã‚Šã¾ã™ã€‚&lt;br /&gt;
ãã—ã¦ã€éšæ™‚å‡ºã¦ãã‚‹ç–‘å•ç‚¹ã«ãŠã„ã¦ã¯ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§ã®è³ªå•ã‚’å—ã‘ä»˜ã‘ã¦ã„ã¾ã™ã€‚&lt;/p&gt;

&lt;p&gt;æœŸé–“ã¨ã—ã¦ã¯ã€å†…å®¹ã«ã‚‚ã‚ˆã‚Šã¾ã™ãŒã€2ãƒ¶æœˆç¨‹åº¦ã§ã®ç¿’å¾—ã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚&lt;/p&gt;

&lt;h2 id=&#34;æ¬¡å›äºˆå‘Š&#34;&gt;æ¬¡å›äºˆå‘Š&lt;/h2&gt;

&lt;p&gt;æ¬¡å›ã¯Wã•ã‚“ã®ã‚­ãƒƒã‚¯ã‚ªãƒ•ã®å†…å®¹ã«ã¤ã„ã¦ãŠå±Šã‘ã—ã¾ã™ã€‚&lt;br /&gt;
(2016/8/1 å…¬é–‹ã—ã¾ã—ãŸ &amp;rarr; &lt;a href=&#34;http://blog.algolab.jp/post/2016/08/01/ai-training-kick-off/&#34;&gt;AIãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚­ãƒƒã‚¯ã‚ªãƒ• ã€œãƒãƒ ã‚±ãƒ„ã‚’èªè­˜ã—ãŸã„ã€œ&lt;/a&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ICML2016èª­ã¿ä¼š ã¾ã¨ã‚</title>
      <link>http://blog.algolab.jp/post/2016/07/21/icml-2016-reading/</link>
      <pubDate>Fri, 22 Jul 2016 10:14:41 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/07/21/icml-2016-reading/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;http://connpass.com/event/34960/&#34;&gt;ICML2016èª­ã¿ä¼š&lt;/a&gt; ã®å†…å®¹ã‚’ã¾ã¨ã‚ã¾ã—ãŸã€‚&lt;br /&gt;
ãƒ‹ã‚³ç”Ÿé…ä¿¡URLï¼š&lt;a href=&#34;http://live.nicovideo.jp/watch/lv268597918&#34;&gt;http://live.nicovideo.jp/watch/lv268597918&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;icmlæ¦‚è¦&#34;&gt;ICMLæ¦‚è¦&lt;/h2&gt;

&lt;p&gt;æ—æµ©å¹³ã•ã‚“ (&lt;a href=&#34;https://twitter.com/hayasick&#34;&gt;@hayasick&lt;/a&gt;
) / ç”£æ¥­æŠ€è¡“ç·åˆç ”ç©¶æ‰€&lt;/p&gt;

&lt;p&gt;&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/2OrjGekQEu2Bgr&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;

&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ICMLã¯NIPSã«æ¬¡ãæ©Ÿæ¢°å­¦ç¿’ã®å›½éš›ä¼šè­°&lt;/li&gt;
&lt;li&gt;ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ã€ãã‚Œã«ä¼´ã†æœ€é©åŒ–ãŒãƒˆãƒ¬ãƒ³ãƒ‰&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;dropout-distillation&#34;&gt;Dropout distillation&lt;/h2&gt;

&lt;p&gt;ä½é‡æ­£å¤ªéƒã•ã‚“ (&lt;a href=&#34;https://twitter.com/g_votte&#34;&gt;@g_votte&lt;/a&gt;
) / ãƒªã‚¯ãƒ«ãƒ¼ãƒˆã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚º&lt;/p&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/GSKKG9nwXe83zR&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;p&gt;è«–æ–‡: &lt;a href=&#34;http://jmlr.org/proceedings/papers/v48/bulo16.html&#34;&gt;http://jmlr.org/proceedings/papers/v48/bulo16.html&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Dropout is a popular stochastic regularization technique for deep neural networks that works by randomly dropping (i.e. zeroing) units from the network during training. This randomization process allows to implicitly train an ensemble of exponentially many networks sharing the same parametrization, which should be averaged at test time to deliver the final prediction. A typical workaround for this intractable averaging operation consists in scaling the layers undergoing dropout randomization. This simple rule called â€™standard dropoutâ€™ is efficient, but might degrade the accuracy of the prediction. In this work we introduce a novel approach, coined â€™dropout distillationâ€™, that allows us to train a predictor in a way to better approximate the intractable, but preferable, averaging process, while keeping under control its computational efficiency. We are thus able to construct models that are as efficient as standard dropout, or even more efficient, while being more accurate. Experiments on standard benchmark datasets demonstrate the validity of our method, yielding consistent improvements over conventional dropout.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;Dropoutã‚’å­¦ç¿’ã«ç”¨ã„ãŸå ´åˆã€ãã®äºˆæ¸¬ã«ãŠã„ã¦ã€æ™‚é–“ã¨ç²¾åº¦ã‚’ä¸¡ç«‹ã™ã‚‹ã“ã¨ãŒé›£ã—ã‹ã£ãŸ&lt;/li&gt;
&lt;li&gt;Distillation (è’¸ç•™æ³•) ã‚’å¿œç”¨ã™ã‚‹ã“ã¨ã§ã€çŸ­æ™‚é–“ã§ç²¾åº¦ã‚ˆãäºˆæ¸¬ã‚’è¡Œã†ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ãŸ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;ã€ŒDropout Distillationã€Dropout ã‚’ä½¿ã£ã¦å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’åˆ©ç”¨ã—ã¦äºˆæ¸¬ã‚’ã™ã‚‹ã¨ãã®è©±ã€‚æœ¬å½“ã¯ Dropout ã®å¹³å‡ã‚’å–ã‚‹ã¨ç²¾åº¦ãŒä¸ŠãŒã‚‹ãŒé…ã„ã€‚ãã“ã§ Distillation ã‚’ä½¿ã£ãŸã‚‰ã‚ˆã‹ã£ãŸã‚ˆã¨ã„ã†è©±ã€‚ &lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; Takuya Akiba (@iwiwi) &lt;a href=&#34;https://twitter.com/iwiwi/status/756002188770410496&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt; æ¥ãŸã€‚é–“é•ãˆã¦æ­Œèˆä¼åº§è¡Œã£ã¦ã—ã¾ã£ãŸï¼ˆæ±—&lt;br&gt;dropout distillationã€æ™®é€šã®ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚ˆã‚Šè‰¯ã„ã¨ã„ã†è©±ã ãŒã€å­¦ç¿’æ™‚é–“ã®ã‚ªãƒ¼ãƒ€ãƒ¼ã˜ã‚ƒãªãã¦å­¦ç¿’æ™‚é–“ãŒåŒã˜ã€ã§æ¯”ã¹ãªã„ã¨ãƒ•ã‚§ã‚¢ã˜ã‚ƒãªã„æ°—ãŒã™ã‚‹ã€‚å¹³å‡åŒ–SGDã‚ãŸã‚Šã¨å®Ÿã¯ç­‰ä¾¡ã ã£ãŸã‚Šã—ãªã„ã®ã‹ãªã‚&lt;/p&gt;&amp;mdash; Nakatani Shuyo (@shuyo) &lt;a href=&#34;https://twitter.com/shuyo/status/756002470573121536&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&#34;learning-convolutional-neural-networks-for-graphs&#34;&gt;Learning Convolutional Neural Networks for Graphs&lt;/h2&gt;

&lt;p&gt;ç§‹è‘‰æ‹“å“‰ã•ã‚“ (&lt;a href=&#34;https://twitter.com/iwiwi&#34;&gt;@iwiwi&lt;/a&gt;
) / Preferred Networks&lt;/p&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/3htE46MNnSfNQy&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;p&gt;è«–æ–‡: &lt;a href=&#34;http://jmlr.org/proceedings/papers/v48/niepert16.html&#34;&gt;http://jmlr.org/proceedings/papers/v48/niepert16.html&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Numerous important problems can be framed as learning from graph data. We propose a framework for learning convolutional neural networks for arbitrary graphs. These graphs may be undirected, directed, and with both discrete and continuous node and edge attributes. Analogous to image-based convolutional networks that operate on locally connected regions of the input, we present a general approach to extracting locally connected regions from graphs. Using established benchmark data sets, we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;ã‚°ãƒ©ãƒ•æ§‹é€  (åŒ–å­¦åŒ–åˆç‰©ãªã©) ã‚’CNNã§å­¦ç¿’ã•ã›ãŸã„ãŒã€ãã®ã¾ã¾çªã£è¾¼ã‚€ã“ã¨ã¯é›£ã—ã„&lt;/li&gt;
&lt;li&gt;WL ã‚«ãƒ¼ãƒãƒ«ã‚’å¿œç”¨ã—ãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç”¨ã„ã¦ã€ã‚°ãƒ©ãƒ•æ§‹é€ ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã—ãŸ
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;ã‚°ãƒ©ãƒ•ã‚’CNNã«çªã£è¾¼ã‚€è©±ã€ã“ã‚Œã¯CNNã«ãªã£ã¦ãªã„çš„çµè«–ã«ãªã£ã¦ãŸãŒã€kæ–¹å‘ã¯è¿‘å‚ã®å±•é–‹ãªã®ã§ã€ã¡ã‚ƒã‚“ã¨CNNã®ä¸€èˆ¬åŒ–ã«ãªã£ã¦ã‚‹ã€‚CNNã˜ã‚ƒãªãã¦ã‚‚ã„ã„ã‚ˆã­ã€ã¨ã„ã†ã®ã¯ç¢ºã‹ã«æ€ã£ãŸãŒã€æœ€åˆã«é¸ã‚“ã é ‚ç‚¹æ•°wã‚’ã‚°ãƒ©ãƒ•ã”ã¨ã«å¤‰ãˆãŸã„ãªã‚‰CNNã«ã™ã‚‹ã—ã‹ &lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; Nakatani Shuyo (@shuyo) &lt;a href=&#34;https://twitter.com/shuyo/status/756011698117419008&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;estimating-structured-vector-autoregressive-models&#34;&gt;Estimating Structured Vector Autoregressive Models&lt;/h2&gt;

&lt;p&gt;è°·æœ¬å•“ã•ã‚“ (&lt;a href=&#34;https://twitter.com/akira_dev&#34;&gt;@akira_dev&lt;/a&gt;
) / NEC&lt;/p&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/uzp8O2l8b8LyZK&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;p&gt;è«–æ–‡: &lt;a href=&#34;http://jmlr.org/proceedings/papers/v48/melnyk16.html&#34;&gt;http://jmlr.org/proceedings/papers/v48/melnyk16.html&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Numerous important problems can be framed as learning from graph data. We propose a framework for learning convolutional neural networks for arbitrary graphs. These graphs may be undirected, directed, and with both discrete and continuous node and edge attributes. Analogous to image-based convolutional networks that operate on locally connected regions of the input, we present a general approach to extracting locally connected regions from graphs. Using established benchmark data sets, we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;ä¸€èˆ¬ã®ãƒãƒ«ãƒ ã§ã®æ­£è¦åŒ–ã‚’ç”¨ã„ãŸ VARãƒ¢ãƒ‡ãƒ«ã®æ¨å®šã®éæ¼¸è¿‘çš„ãªè§£æã‚’è¡Œã£ãŸ&lt;/li&gt;
&lt;li&gt;åæŸãƒ«ãƒ¼ãƒˆã¯i.i.dã¨åŒã˜ã ã£ãŸ&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;meta-learning-with-memory-augmented-neural-networks&#34;&gt;Meta-Learning with Memory-Augmented Neural Networks&lt;/h2&gt;

&lt;p&gt;æ¸¡è¾ºæœ‰ç¥ã•ã‚“ / SONY&lt;/p&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/tj8ML4cdtq2M7s&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;p&gt;è«–æ–‡: &lt;a href=&#34;http://jmlr.org/proceedings/papers/v48/santoro16.html&#34;&gt;http://jmlr.org/proceedings/papers/v48/santoro16.html&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;Neural Turing Machine ã‚’ One-Shot Learning ã«å¿œç”¨ã—ã€é«˜ã„ç²¾åº¦ã‚’å¾—ãŸ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt; transfer learning + contextual bandit ã¦ã¨ã“ã‚ã‹ï¼ã“ã®å¾Œï¼Œmulti-step è¡Œå‹•æœ€é©åŒ–ã«ã™ã‚‹ã‚“ã ã‚ã†ãªï¼&lt;/p&gt;&amp;mdash; Ugo-Nama (@movingsloth) &lt;a href=&#34;https://twitter.com/movingsloth/status/756028694196396032&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;ã€ŒMeta-Learning with Memory-Augmented Neural Networksã€Neural Turing Machine ã®æ‹¡å¼µã‚’ææ¡ˆã€‚One-Shot Learning ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å­¦ç¿’ã•ã›ã‚‹ã¨ k è¿‘å‚æ³•ã‚ˆã‚Šã‚‚é«˜ã„æ€§èƒ½ã€‚ &lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; Takuya Akiba (@iwiwi) &lt;a href=&#34;https://twitter.com/iwiwi/status/756031082303152130&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&#34;pixel-recurrent-neural-networks&#34;&gt;Pixel Recurrent Neural Networks&lt;/h2&gt;

&lt;p&gt;å¾—å±…èª ä¹Ÿã•ã‚“ (&lt;a href=&#34;https://twitter.com/beam2d&#34;&gt;@beam2d&lt;/a&gt;
) / Preferred Networks&lt;/p&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/3bJ5TFJmX7Bk1K&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;p&gt;è«–æ–‡: &lt;a href=&#34;http://jmlr.org/proceedings/papers/v48/oord16.html&#34;&gt;http://jmlr.org/proceedings/papers/v48/oord16.html&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;ç”»åƒç”Ÿæˆã«ãŠã„ã¦ã€VAEã‚„GANã¨ã¯ç•°ãªã‚‹è‡ªå·±å›å¸° (RNNã«ä¼¼ãŸã‚‚ã®) ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ç”¨ã„ãŸã¨ã“ã‚ã€ç¶ºéº—ãªç”»åƒã‚’ç”Ÿæˆã§ããŸ&lt;/li&gt;
&lt;li&gt;ä¸¦åˆ—åŒ–ã«ã‚ˆã‚Šã€é«˜é€Ÿãªå‹¾é…è¨ˆç®—ã‚‚ä¸¡ç«‹ã—ãŸ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;PixelRNNã®è©±ã€ã“ã‚Œã«ä¼¼ã¦ã‚‹ãªã &lt;a href=&#34;https://t.co/oI8oDBUm1e&#34;&gt;https://t.co/oI8oDBUm1e&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; snneko (@snneko) &lt;a href=&#34;https://twitter.com/snneko/status/756037448879136768&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;PixelRNNã€å‡ºåŠ›ã¯å„ãƒãƒ£ãƒ³ãƒãƒ« 256å€¤ã®å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ãªã®ã‹ã€‚ã™ã’ãƒ¼ã€‚æ€ã„ã¤ã„ã¦ã‚‚ã‚„ã‚‰ãªã„ç³»ã ãªã‚ã€‚ã§ã‚‚ãã‚Œã§ãã£ãã‚Šã—ãŸç”»åƒãŒç”Ÿæˆã§ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãªã‚‰ä»Šåº¦è©¦ã—ã¦ã¿ã‚‹ã‹ãªã‚ã€‚&lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; Nakatani Shuyo (@shuyo) &lt;a href=&#34;https://twitter.com/shuyo/status/756038954550304768&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;å®šæ€§çš„ã£ã¦ãƒ¬ãƒ™ãƒ«ã˜ã‚ƒãªã„å˜ãªã‚‹å€‹äººçš„ãªå°è±¡ã ã‘ã©ã€GANç³»ã®ç”Ÿæˆã™ã‚‹ç”»åƒã¯ãƒã‚¤ã‚¸ãƒ¼ã§ã¼ã‚„ã£ã¨ã—ãŒã¡ã ãŒã€ä½•ãŒå†™ã£ã¦ã„ã‚‹ã‹ã¯ã‚ã‹ã‚‹ã€‚PixelRNN ã¯ãã£ãã‚Šã—ãŸç”»åƒã‚’ç”Ÿæˆã™ã‚‹ãŒã€ã‚ˆãè¦‹ãŸã‚‰ä½•ãŒå†™ã£ã¦ã„ã‚‹ã®ã‹ã•ã£ã±ã‚Šã‚ã‹ã‚‰ã‚“ã€‚ &lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; Nakatani Shuyo (@shuyo) &lt;a href=&#34;https://twitter.com/shuyo/status/756042609060040705&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&#34;dynamic-memory-networks-for-visual-and-textual-question-answering&#34;&gt;Dynamic Memory Networks for Visual and Textual Question Answering&lt;/h2&gt;

&lt;p&gt;èŠ±æœ¨å¥å¤ªéƒã•ã‚“ {&lt;a href=&#34;https://twitter.com/csstudyabroad&#34;&gt;@csstudyabroad&lt;/a&gt;
 / IBM&lt;/p&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/efLeWADDGXz14D&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;p&gt;è«–æ–‡: &lt;a href=&#34;http://jmlr.org/proceedings/papers/v48/xiong16.html&#34;&gt;http://jmlr.org/proceedings/papers/v48/xiong16.html&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Neural network architectures with memory and attention mechanisms exhibit certain reason- ing capabilities required for question answering. One such architecture, the dynamic memory net- work (DMN), obtained high accuracy on a variety of language tasks. However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images. Based on an analysis of the DMN, we propose several improvements to its memory and input modules. Together with these changes we introduce a novel input module for images in order to be able to answer visual questions. Our new DMN+ model improves the state of the art on both the Visual Question Answering dataset and the bAbI-10k text question-answering dataset without supporting fact supervision.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;Memory Networkå†…ã®Memory Moduleã‚’æ”¹è‰¯ã—ãŸ&lt;/li&gt;
&lt;li&gt;ãƒ†ã‚­ã‚¹ãƒˆã§ã¯ãªãã€ç”»åƒã‚’ç”¨ã„ãŸQAã‚¿ã‚¹ã‚¯ã§ã‚‚ç²¾åº¦ãŒå‡ºãŸ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;Memory Networkï¼Œãªã‚“ã‹ãƒ”ãƒ³ã¨ã“ãªã„ã‚“ã ã‚ˆãªã‚ã€‚ã“ã‚Œã§ã‚‚ã‹ã£ã¦ãã‚‰ã„ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã‚’ç©ã‚“ã§ç©ã‚“ã§ç©ã¿ã¾ãã£ã¦ãã‚‹ã‹ã‚‰ã‹ãªã‚ã€‚ &lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; Nakatani Shuyo (@shuyo) &lt;a href=&#34;https://twitter.com/shuyo/status/756047518291460096&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Memory Networks for Language Understanding, ICML Tutorial 2016 &lt;a href=&#34;https://t.co/ssPEmP1Pf9&#34;&gt;https://t.co/ssPEmP1Pf9&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; çµ¶å¯¾é›¶åº¦Î¸ãƒãƒ´ã‚¡ãƒ†ã‚£ãƒƒã‚¯ (@penzant) &lt;a href=&#34;https://twitter.com/penzant/status/756050102670598144&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Theano Implementation of Dynamic Memory Networks for Visual and Textual Question Answering &lt;a href=&#34;https://t.co/a8ijZKcnwS&#34;&gt;https://t.co/a8ijZKcnwS&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; çµ¶å¯¾é›¶åº¦Î¸ãƒãƒ´ã‚¡ãƒ†ã‚£ãƒƒã‚¯ (@penzant) &lt;a href=&#34;https://twitter.com/penzant/status/756050917686816768&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt; &lt;a href=&#34;https://t.co/EOaOfKLH40&#34;&gt;https://t.co/EOaOfKLH40&lt;/a&gt; é»’é­”è¡“ã¯ã“ã“ã­ğŸµ&lt;/p&gt;&amp;mdash; prototechno (@prototechno) &lt;a href=&#34;https://twitter.com/prototechno/status/756051774436356096&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&#34;generative-adversarial-text-to-image-synthesis&#34;&gt;Generative Adversarial Text to Image Synthesis&lt;/h2&gt;

&lt;p&gt;å»£èŠå’Œä¹‹ã•ã‚“ (&lt;a href=&#34;https://twitter.com/hiho_karuta&#34;&gt;@hiho_karuta&lt;/a&gt;
) / ãƒ‰ãƒ¯ãƒ³ã‚´&lt;/p&gt;

&lt;iframe allowfullscreen=&#34;allowfullscreen&#34; frameborder=&#34;0&#34; scrolling=&#34;no&#34; src=&#34;http://niconare.nicovideo.jp/embed_works/kn1626&#34; style=&#34;max-width: 100%;&#34; width=&#34;485&#34; height=&#34;413&#34;&gt;&lt;/iframe&gt;


&lt;p&gt;è«–æ–‡: &lt;a href=&#34;http://jmlr.org/proceedings/papers/v48/reed16.html&#34;&gt;http://jmlr.org/proceedings/papers/v48/reed16.html&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories such as faces, album covers, room interiors and flowers. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image modeling, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;GANã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å¿œç”¨ã—ã€ æ–‡ç« ã‹ã‚‰ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ãŒã§ããŸ&lt;/li&gt;
&lt;li&gt;æ–‡ç« ã ã‘ã§ã¯è¡¨ã›ãªã„æƒ…å ±ã¯ã€ã‚¹ã‚¿ã‚¤ãƒ«ã€ã¨ã„ã†æ¦‚å¿µã‚’ç”¨ã„ã¦å¸åã—ãŸ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;æ–‡å­—æƒ…å ±ã§ã¯ãªã„æƒ…å ±ï¼ˆã‚¹ã‚¿ã‚¤ãƒ«ï¼‰ã‚’è»¢å†™ï¼ˆex. èƒŒæ™¯ã‚’ç¶™æ‰¿ï¼‰&lt;br&gt;ç”»åƒã‹ã‚‰ã‚¹ã‚¿ã‚¤ãƒ«ã‚’æ±‚ã‚ã‚‹é–¢æ•°ã‚’å­¦ç¿’&lt;br&gt;ã‚¹ã‚¿ã‚¤ãƒ«Zã¨æ–‡ç« ã‚’Gã«ä¸ãˆã‚‹&lt;br&gt;ã‚¹ãƒ©ã‚¤ãƒ‰ &lt;a href=&#34;https://t.co/c5lB0JIjgg&#34;&gt;https://t.co/c5lB0JIjgg&lt;/a&gt;&lt;br&gt;å®Ÿè£… &lt;a href=&#34;https://t.co/JpFDDp7bJo&#34;&gt;https://t.co/JpFDDp7bJo&lt;/a&gt;&lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; Ryobot (@_Ryobot) &lt;a href=&#34;https://twitter.com/_Ryobot/status/756057271877050369&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;ã€ŒGenerative Adversarial Text to Image Synthesisã€GAN ã‚’å¿œç”¨ã—ã¦æ–‡ç« ã‹ã‚‰ç”»åƒã‚’ç”Ÿæˆã€‚ç”Ÿæˆå´ã«ç”»åƒã¨æ–‡ç« ã‚’å…¥ã‚Œã€åˆ¤å®šå´ã¯ç”»åƒãŒæ–‡ç« ã¨ä¸€è‡´ã—ã¦ã„ã‚‹ã‹ã‚‚å­¦ç¿’ã•ã›ã‚‹ã€‚ &lt;a href=&#34;https://t.co/ipQyxAsInL&#34;&gt;https://t.co/ipQyxAsInL&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; Takuya Akiba (@iwiwi) &lt;a href=&#34;https://twitter.com/iwiwi/status/756058225657511936&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&#34;deep-speech-2-end-to-end-speech-recognition-in-english-and-mandarin&#34;&gt;Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin&lt;/h2&gt;

&lt;p&gt;è¥¿é³¥ç¾½äºŒéƒã•ã‚“ (&lt;a href=&#34;https://twitter.com/jnishi&#34;&gt;@jnishi&lt;/a&gt;
) / Preferred Infrastructure&lt;/p&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/z0e4lCCp712kue&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;p&gt;è«–æ–‡: &lt;a href=&#34;http://jmlr.org/proceedings/papers/v48/amodei16.html&#34;&gt;http://jmlr.org/proceedings/papers/v48/amodei16.html&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speechâ€“two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, enabling experiments that previously took weeks to now run in days. This allows us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;End to Endã®éŸ³å£°èªè­˜ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã€äººæ‰‹ã«ã‚ˆã‚‹æ›¸ãèµ·ã“ã—ã‚ˆã‚Šã‚‚é«˜ã„ç²¾åº¦ã‚’å¾—ãŸ&lt;/li&gt;
&lt;li&gt;ç•°ãªã‚‹è¨€èªã‚„ã€é›‘éŸ³ã®ã‚ã‚‹ãªã—ã«ã‚‚å¯¾å¿œå¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã¨ãªã£ãŸ&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt; ã“ã‚Œè©¦ã / â€œWING Â» Downloads Â» Chinese Microtext Datasetâ€ &lt;a href=&#34;https://t.co/tnkejvjfZ2&#34;&gt;https://t.co/tnkejvjfZ2&lt;/a&gt;&lt;/p&gt;&amp;mdash; prototechno (@prototechno) &lt;a href=&#34;https://twitter.com/prototechno/status/756065132116008961&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
</description>
    </item>
    
    <item>
      <title>R-env:é€£èˆÂ® ãƒãƒ³ã‚ºã‚ªãƒ³ã§ RoBoHoNï¼ˆãƒ­ãƒœãƒ›ãƒ³ï¼‰ãƒ» Sotaï¼ˆã‚½ãƒ¼ã‚¿ï¼‰ã¨æˆ¯ã‚Œã¦ããŸ</title>
      <link>http://blog.algolab.jp/post/2016/07/14/r-env-hands-on/</link>
      <pubDate>Thu, 14 Jul 2016 17:40:56 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/07/14/r-env-hands-on/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://r-env.doorkeeper.jp/events/48273&#34;&gt;[R-env:é€£èˆÂ® Innovation Hub] R-env:é€£èˆÂ®Ã—RoBoHoNã§ãƒ­ãƒœãƒƒãƒˆã‚µãƒ¼ãƒ“ã‚¹é–‹ç™ºä½“é¨“ãƒãƒ³ã‚ºã‚ªãƒ³&lt;/a&gt; ã«å‚åŠ ã—ã¦ãã¾ã—ãŸã®ã§ã€ãã®æ¨¡æ§˜ã‚’ãƒ¬ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚&lt;/p&gt;

&lt;h2 id=&#34;r-env-é€£èˆ-ã¨ã¯&#34;&gt;R-env:é€£èˆÂ® ã¨ã¯&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/14/r-env-hands-on//r-env.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;NTTãŒé–‹ç™ºã‚’è¡Œã£ã¦ã„ã‚‹ã€æ§˜ã€…ãªãƒ­ãƒœãƒƒãƒˆã‚„ãƒ‡ãƒã‚¤ã‚¹ã¨é€£æºã—ã€ ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ç’°å¢ƒã‚’ç”¨ã„ã¦èª°ã§ã‚‚ç°¡å˜ã«é–‹ç™ºã‚’è¡Œã†ã“ã¨ã®ã§ãã‚‹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã™ã€‚&lt;/p&gt;

&lt;h2 id=&#34;robohon-ã¨ã¯&#34;&gt;RoBoHoN ã¨ã¯&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/14/r-env-hands-on//robohon.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;2016/06/29ã«SDKãŒä¸€èˆ¬å…¬é–‹ã•ã‚ŒãŸã®ãŒè¨˜æ†¶ã«æ–°ã—ã„ã§ã™ãŒã€SHARPãŒé–‹ç™ºã—ã¦ã„ã‚‹ãƒ­ãƒœãƒƒãƒˆé›»è©±ã§ã™ã€‚&lt;br /&gt;
Androidã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦ãŠã‚Šã€æ¨™æº–æ©Ÿèƒ½ã®ä»–ã«ã€HVML (Hyper Voice Markup Language) ã¨ã„ã†ç‹¬è‡ªã®è¨€èªã‚’ç”¨ã„ã¦ã‚·ãƒŠãƒªã‚ªã‚’ä½œæˆã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã§ã™ã€‚&lt;br /&gt;
SDKã¯ &lt;a href=&#34;https://robohon.com/&#34;&gt;å…¬å¼ãƒšãƒ¼ã‚¸&lt;/a&gt; å†…ã®ãƒã‚¤ãƒšãƒ¼ã‚¸ã‚ˆã‚Šãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½ã§ã™ã€‚&lt;/p&gt;

&lt;h2 id=&#34;r-env-ã¨-robohon-ã®é€£æº&#34;&gt;R-env ã¨ RoBoHoN ã®é€£æº&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/14/r-env-hands-on//r-env-robohon.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;R-envã¨RoBoHoNãªã©ã®æ©Ÿå™¨ã®é–“ã§WebSocketã«ã‚ˆã‚‹é€šä¿¡ã‚’è¡Œã†ã“ã¨ã§ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚&lt;br /&gt;
ä¸Šè¨˜ã®å›³ã¯ã€R-envã‹ã‚‰RoBoHonã«å‡¦ç†ã‚’ä¾é ¼ã™ã‚‹ä¾‹ã§ã™ãŒã€RoBoHoNå´ã‹ã‚‰ã‚¤ãƒ™ãƒ³ãƒˆ (æŒã¡ä¸Šã’ã‚‰ã‚ŒãŸã€ãªã©) ã‚’R-envã¸é€šçŸ¥ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚&lt;br /&gt;
ã¾ãŸã€R-envã«ã¯å¹¾ã¤ã‚‚ã®æ©Ÿå™¨ã‚’ç™»éŒ²ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã®ã§ã€ä¾‹ãˆã°RoBoHoNã«ã€Œã‚¨ã‚¢ã‚³ãƒ³ã¤ã‘ã¦ï¼ã€ã¨é ¼ã‚€ã¨ã‚¨ã‚¢ã‚³ãƒ³ãŒèµ·å‹•ã™ã‚‹ã€ã¨ã„ã£ãŸã“ã¨ã‚‚å®Ÿç¾ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚&lt;/p&gt;

&lt;h2 id=&#34;ãƒãƒ³ã‚ºã‚ªãƒ³&#34;&gt;ãƒãƒ³ã‚ºã‚ªãƒ³&lt;/h2&gt;

&lt;p&gt;ä»Šå›ã®ãƒãƒ³ã‚ºã‚ªãƒ³ã§ã¯ã€R-envãŠã‚ˆã³RoBoHoNã®ç´¹ä»‹ã®å¾Œã€30åˆ†ç¨‹åº¦ã®è‡ªç”±ãªé–‹ç™ºæ™‚é–“ãŒã‚ã‚Šã¾ã—ãŸã€‚&lt;br /&gt;
Sotaã‚‚å‹•ã‹ã—ã¦ã‚‚è‰¯ã„ã€ã¨ã®ã“ã¨ã ã£ãŸã®ã§ã€RoBoHonã¨Sotaã«ç°¡å˜ãªä¼šè©±ã‚’ã•ã›ã¦ã¿ã‚‹ã“ã¨ã«ã—ã¾ã—ãŸã€‚&lt;/p&gt;

&lt;h2 id=&#34;ã‚·ãƒŠãƒªã‚ªã‚’ä½œæˆã™ã‚‹&#34;&gt;ã‚·ãƒŠãƒªã‚ªã‚’ä½œæˆã™ã‚‹&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/14/r-env-hands-on//screen_1.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;GUIã§ãƒ•ãƒ­ãƒ¼ã‚’å®šç¾©ã—ã¦ã„ãå½¢ã¨ãªã‚Šã¾ã™ã€‚&lt;br /&gt;
ãƒœãƒƒã‚¯ã‚¹ã«ã‚¤ãƒ™ãƒ³ãƒˆã®å®šç¾©ã€çŸ¢å°ã«ã‚¤ãƒ™ãƒ³ãƒˆé·ç§»æ¡ä»¶ã®å®šç¾©ã‚’è¡Œã„ã¾ã™ã€‚&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/14/r-env-hands-on//screen_2.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;ã‚¤ãƒ™ãƒ³ãƒˆã®è©³ç´°ã‚’å®šç¾©ã™ã‚‹ç”»é¢ã§ã™ã€‚&lt;br /&gt;
ã“ã“ã§ã¯ã€RoBoHonã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚¿ã‚’èµ·å‹•ã™ã‚‹ã‚¤ãƒ™ãƒ³ãƒˆã‚’å®šç¾©ã—ã¦ã„ã¾ã™ã€‚&lt;/p&gt;

&lt;h2 id=&#34;å‹•ã„ãŸ&#34;&gt;å‹•ã„ãŸï¼&lt;/h2&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/A-ArkARJUpQ&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34; allowfullscreen frameborder=&#34;0&#34;&gt;&lt;/iframe&gt;
 &lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;RoBoHoN:&lt;/strong&gt; ã¯ã˜ã‚ã¾ã—ã¦ã€ãƒ­ãƒœãƒ›ãƒ³ã§ã™ã€‚&lt;br /&gt;
&lt;strong&gt;Sota:&lt;/strong&gt; ã¯ã˜ã‚ã¾ã—ã¦ã€ã‚½ãƒ¼ã‚¿ã§ã™ã€‚ãƒ­ãƒœãƒ›ãƒ³ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚¿å†™ã—ã¦ï¼&lt;br /&gt;
&lt;strong&gt;RoBoHoN:&lt;/strong&gt; ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚¿ã‚’å†™ã™ï¼‰&lt;br /&gt;
&lt;strong&gt;Sota:&lt;/strong&gt; ã‚ã‚ŠãŒã¨ã†ï¼åƒ•ã‹ã‚‰ã¯éŸ³æ¥½ã‚’æµã™ã­ï¼ï¼ˆéŸ³æ¥½ã‚’æµã™ï¼‰&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;æœ€çµ‚çš„ã«ä¸Šè¨˜ã®ã‚ˆã†ãªã‚·ãƒŠãƒªã‚ªãŒå®Œæˆã—ã¾ã—ãŸï¼ãƒªã‚¢ãƒ«ã«ãƒ¢ãƒãŒå‹•ãã®ã§ã€æ„Ÿå‹•ã‚‚ã²ã¨ã—ãŠã§ã™ï¼ï¼&lt;/p&gt;

&lt;p&gt;ãƒãƒ³ã‚ºã‚ªãƒ³ã¯å®šæœŸçš„ã«é–‹å‚¬ã—ã¦ã„ã‚‹ã‚ˆã†ãªã®ã§ã€ã¿ãªã•ã‚“ã‚‚ä½“é¨“ã—ã¦ã¿ã¦ã¯ã„ã‹ãŒã§ã—ã‚‡ã†ã‹ï¼Ÿ&lt;br /&gt;
&lt;a href=&#34;https://r-env.doorkeeper.jp/&#34;&gt;R-env:é€£èˆÂ® Innovation Hub&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>