<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on ALGO GEEKS</title>
    <link>http://blog.algolab.jp/post/</link>
    <description>Recent content in Posts on ALGO GEEKS</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Sun, 21 Aug 2016 17:22:48 +0900</lastBuildDate>
    <atom:link href="http://blog.algolab.jp/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>【随時更新】pyenv &#43; Anaconda (Ubuntu 16.04 LTS) で機械学習のPython開発環境をオールインワンで整える</title>
      <link>http://blog.algolab.jp/post/2016/08/21/pyenv-anaconda-ubuntu/</link>
      <pubDate>Sun, 21 Aug 2016 17:22:48 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/08/21/pyenv-anaconda-ubuntu/</guid>
      <description>

&lt;p&gt;機械学習系のPython開発環境は、&lt;a href=&#34;https://www.vagrantup.com/&#34;&gt;Vagrant&lt;/a&gt; を用いた &lt;a href=&#34;https://atlas.hashicorp.com/bento/boxes/ubuntu-16.04&#34;&gt;Ubuntu (16.04 LTS)&lt;/a&gt; の仮想環境上に構築しています。&lt;br /&gt;
ここでは、画像認識、音声認識、自然言語処理などに必要な環境をオールインワンで構築する手順をまとめます。&lt;br /&gt;
(2016/08/21 初版公開)&lt;/p&gt;

&lt;h2 id=&#34;osバージョン&#34;&gt;OSバージョン&lt;/h2&gt;

&lt;p&gt;OSバージョンは下記の通りです。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ cat /etc/lsb-release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=16.04
DISTRIB_CODENAME=xenial
DISTRIB_DESCRIPTION=&amp;quot;Ubuntu 16.04.1 LTS&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ uname -a
Linux vagrant 4.4.0-31-generic #50-Ubuntu SMP Wed Jul 13 00:07:12 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;パッケージのインストール&#34;&gt;パッケージのインストール&lt;/h2&gt;

&lt;p&gt;まず、汎用的に使うパッケージをインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo apt-get install -y git swig
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;pyenv-anaconda-の環境を構築&#34;&gt;pyenv + Anaconda の環境を構築&lt;/h2&gt;

&lt;p&gt;Python環境は、pyenv + Anacodaを用いて構築します。&lt;br /&gt;
pyenvやAnacondaの概要やメリットについては、下記の記事に詳しくまとまっています。&lt;br /&gt;
&lt;a href=&#34;http://qiita.com/y__sama/items/5b62d31cb7e6ed50f02c&#34;&gt;データサイエンティストを目指す人のpython環境構築 2016&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;必要なパッケージのインストール&#34;&gt;必要なパッケージのインストール&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ sudo apt-get install -y make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev libncursesw5-dev
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;pyenvのインストール&#34;&gt;pyenvのインストール&lt;/h3&gt;

&lt;p&gt;pyenvおよびプラグインをインストールし、環境を整えます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ git clone git://github.com/yyuu/pyenv.git ~/.pyenv
$ git clone https://github.com/yyuu/pyenv-pip-rehash.git ~/.pyenv/plugins/pyenv-pip-rehash
$ echo &#39;export PYENV_ROOT=&amp;quot;$HOME/.pyenv&amp;quot;&#39; &amp;gt;&amp;gt; ~/.bashrc
$ echo &#39;export PATH=&amp;quot;$PYENV_ROOT/bin:$PATH&amp;quot;&#39; &amp;gt;&amp;gt; ~/.bashrc
$ echo &#39;eval &amp;quot;$(pyenv init -)&amp;quot;&#39; &amp;gt;&amp;gt; ~/.bashrc
$ source ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;anacondaのインストール&#34;&gt;Anacondaのインストール&lt;/h3&gt;

&lt;p&gt;まず、最新のAnaconda (Python 3系) のバージョンを確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ pyenv install -l | grep anaconda3
  anaconda3-2.0.0
  anaconda3-2.0.1
  anaconda3-2.1.0
  anaconda3-2.2.0
  anaconda3-2.3.0
  anaconda3-2.4.0
  anaconda3-2.4.1
  anaconda3-2.5.0
  anaconda3-4.0.0
  anaconda3-4.1.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最新のAnaconda (ここでは4.1.0) をインストールし、デフォルトとして設定します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ pyenv install anaconda3-4.1.0
$ pyenv global anaconda3-4.1.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pythonの環境を確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ python --version
Python 3.5.1 :: Anaconda 4.1.0 (64-bit)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;pythonライブラリのインストール&#34;&gt;Pythonライブラリのインストール&lt;/h2&gt;

&lt;p&gt;以下、用途に応じて必要なPythonライブラリをインストールしていきます。&lt;br /&gt;
&lt;code&gt;conda&lt;/code&gt;が便利なものは&lt;code&gt;conda&lt;/code&gt;で、それ以外は&lt;code&gt;pip&lt;/code&gt;で行います。&lt;/p&gt;

&lt;p&gt;諸々インストールする前に自身を更新しておきます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;conda update -y conda
pip install --upgrade pip
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;深層学習ライブラリ&#34;&gt;深層学習ライブラリ&lt;/h3&gt;

&lt;h4 id=&#34;tensorflow&#34;&gt;TensorFlow&lt;/h4&gt;

&lt;p&gt;Googleの深層学習ライブラリ。&lt;code&gt;conda&lt;/code&gt;経由で最新バージョンをインストールします。&lt;br /&gt;
&lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;https://www.tensorflow.org/&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ conda install -y -c jjhelmus tensorflow
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;chainer&#34;&gt;Chainer&lt;/h4&gt;

&lt;p&gt;PFNの深層学習ライブラリ。&lt;br /&gt;
&lt;a href=&#34;http://chainer.org/&#34;&gt;http://chainer.org/&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ pip install chainer
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;theano&#34;&gt;Theano&lt;/h3&gt;

&lt;p&gt;2008年から開発されている古参のライブラリ。&lt;br /&gt;
&lt;a href=&#34;http://deeplearning.net/software/theano/&#34;&gt;http://deeplearning.net/software/theano/&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ pip install theano
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;keras&#34;&gt;Keras&lt;/h4&gt;

&lt;p&gt;TensorFlowおよびTheanoのラッパー。&lt;br /&gt;
&lt;a href=&#34;https://keras.io/&#34;&gt;https://keras.io/&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ pip install keras
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;画像認識&#34;&gt;画像認識&lt;/h3&gt;

&lt;h4 id=&#34;imagemagick&#34;&gt;ImageMagick&lt;/h4&gt;

&lt;p&gt;画像処理ライブラリ。&lt;br /&gt;
&lt;a href=&#34;http://imagemagick.org/script/index.php&#34;&gt;http://imagemagick.org/script/index.php&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ conda install -y -c kalefranz imagemagick
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;opencv&#34;&gt;OpenCV&lt;/h4&gt;

&lt;p&gt;コンピュータビジョンライブラリ。&lt;br /&gt;
&lt;a href=&#34;http://opencv.org/&#34;&gt;http://opencv.org/&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ conda install -y -c menpo opencv3
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;dlib&#34;&gt;Dlib&lt;/h4&gt;

&lt;p&gt;画像認識が充実している機械学習ライブラリ。&lt;br /&gt;
&lt;a href=&#34;http://dlib.net/&#34;&gt;http://dlib.net/&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ conda install -y -c menpo dlib
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;音声認識&#34;&gt;音声認識&lt;/h3&gt;

&lt;p&gt;音声・動画処理ライブラリ。Ubuntu16.04から本体は&lt;code&gt;apt&lt;/code&gt;で入るようになりました。&lt;br /&gt;
&lt;a href=&#34;https://ffmpeg.org/&#34;&gt;https://ffmpeg.org/&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ sudo apt install -y ffmpeg
$ pip install ffmpy
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;自然言語処理&#34;&gt;自然言語処理&lt;/h3&gt;

&lt;h4 id=&#34;mecab&#34;&gt;MeCab&lt;/h4&gt;

&lt;p&gt;形態素解析エンジン。本体は&lt;code&gt;apt-get&lt;/code&gt;でインストールします。&lt;br /&gt;
&lt;a href=&#34;http://taku910.github.io/mecab/&#34;&gt;http://taku910.github.io/mecab/&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ sudo apt-get -y install libmecab-dev mecab mecab-ipadic mecab-ipadic-utf8
$ pip install mecab-python3
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;gensim&#34;&gt;gensim&lt;/h4&gt;

&lt;p&gt;トピックモデルのライブラリ。&lt;br /&gt;
&lt;a href=&#34;https://radimrehurek.com/gensim/&#34;&gt;https://radimrehurek.com/gensim/&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ pip install gensim
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;強化学習&#34;&gt;強化学習&lt;/h3&gt;

&lt;h4 id=&#34;openai-gym&#34;&gt;OpenAI Gym&lt;/h4&gt;

&lt;p&gt;強化学習のトレーニング環境。&lt;br /&gt;
&lt;a href=&#34;https://gym.openai.com/&#34;&gt;https://gym.openai.com/&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ pip install gym
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>テレアポ模擬トレーニングBot 〜コミュニケーション教育ツールとしてのチャットボットの活用可能性〜</title>
      <link>http://blog.algolab.jp/post/2016/08/18/telephone-appointment-simulation-bot/</link>
      <pubDate>Thu, 18 Aug 2016 15:20:16 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/08/18/telephone-appointment-simulation-bot/</guid>
      <description>

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/18/telephone-appointment-simulation-bot//cover.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;LINEがBOT API、FacebookがMessenger Platformを発表してから数ヶ月たちました。&lt;br /&gt;
これまでに数々のチャットボットがリリースされていますが、まだ活用方法を模索している段階かと思います。&lt;/p&gt;

&lt;p&gt;その中で、コミュニケーション教育ツールとしての活用可能性があるのではないかと考え、実験的に「テレアポ模擬トレーニングBot」を &lt;a href=&#34;http://www.torix-corp.com/&#34;&gt;TORiX&lt;/a&gt; さんと共同開発させていただきました。&lt;/p&gt;

&lt;p&gt;今回は、このBotの概要と、開発に至った経緯について書きたいと思います。&lt;/p&gt;

&lt;h2 id=&#34;テレアポ模擬トレーニングbotとは&#34;&gt;テレアポ模擬トレーニングBotとは&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/18/telephone-appointment-simulation-bot//story.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;ストーリー形式で、テレアポを擬似体験することのできるコンテンツです。下記ボタンから体験できます。&lt;/p&gt;

&lt;script&gt;
window.fbAsyncInit = function() {
  FB.init({
    appId: &#34;131078513971096&#34;,
    xfbml: true,
    version: &#34;v2.6&#34;
  });
};

(function(d, s, id){
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) { return; }
  js = d.createElement(s); js.id = id;
  js.src = &#34;//connect.facebook.net/ja_JP/sdk.js&#34;;
  fjs.parentNode.insertBefore(js, fjs);
}(document, &#39;script&#39;, &#39;facebook-jssdk&#39;));
&lt;/script&gt;

&lt;div class=&#34;fb-messengermessageus&#34;
  messenger_app_id=&#34;131078513971096&#34;
  page_id=&#34;1771482106414711&#34;&gt;
&lt;/div&gt;


&lt;p&gt;ボタンが動作しない場合は、下記ページからメッセージを送ってみてください。&lt;br /&gt;
&lt;a href=&#34;https://www.facebook.com/1771482106414711/&#34;&gt;https://www.facebook.com/1771482106414711/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;開発の経緯&#34;&gt;開発の経緯&lt;/h2&gt;

&lt;p&gt;コミュニケーション領域の教育においては、疑似的にでも経験を積むことが一番スキルが身につく手段であると考えていますが、
書籍や動画等のコンテンツではそれを体験することが困難です。&lt;/p&gt;

&lt;p&gt;その中で、チャットボットを用いれば容易に実現できるのではないか、と考えたのが開発のきっかけです。&lt;/p&gt;

&lt;p&gt;実際に、テレアポ模擬トレーニングBotは1日もかからず実装することができました。&lt;br /&gt;
(ストーリーを考える時間は別途必要です)&lt;/p&gt;

&lt;h2 id=&#34;コミュニケーション教育ツールとしての可能性&#34;&gt;コミュニケーション教育ツールとしての可能性&lt;/h2&gt;

&lt;p&gt;今回は題材としてテレアポを取り上げましたが、他にも応用可能性はあると考えています。&lt;/p&gt;

&lt;p&gt;例えば、&lt;a href=&#34;https://the-board.jp/&#34;&gt;board&lt;/a&gt; を展開する &lt;a href=&#34;http://www.velc.co.jp/about/&#34;&gt;VELC&lt;/a&gt; さんは、カスタマーサポートの教育にBotを用いていると語っています。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;そこで重要になってくるのが教育です。それに使っているのが、チャットツールである「Slack（スラック）」のbotです。&lt;br /&gt;
Slack上で「ok board」と打ち込むと、FAQからSlackに質問だけがランダムに飛んでくるようになっています。スタッフには空き時間にそれを使って回答文を作成する練習をしてもらい、後で僕がレビューをする。&lt;/p&gt;

&lt;p&gt;引用元: &lt;a href=&#34;https://seleck.cc/article/475&#34;&gt;「カスタマーサクセス」を意識したことはない。自然に神対応を実現した、CSの理想形&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;

&lt;p&gt;今回は、チャットボットを「チャットUIを用いたインタラクティブなアプリを簡単に作れるツール」として捉え、コミュニケーション教育ツールとしての活用可能性についてお届けしました。&lt;/p&gt;

&lt;p&gt;まだまだ可能性を秘めていると思いますので、今後も注目していきたいと思います。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>iOSで音声認識 〜Speech Frameworkを試す〜</title>
      <link>http://blog.algolab.jp/post/2016/08/16/speech-framework/</link>
      <pubDate>Tue, 16 Aug 2016 19:34:57 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/08/16/speech-framework/</guid>
      <description>

&lt;p&gt;2016/08/16現在、まだβ版という位置づけですが、Apple謹製の音声認識エンジン (Speech Framework) が公開されています。今回は、下記のサンプルコードを動作させてみます。
&lt;a href=&#34;https://developer.apple.com/library/prerelease/content/samplecode/SpeakToMe/Introduction/Intro.html&#34;&gt;https://developer.apple.com/library/prerelease/content/samplecode/SpeakToMe/Introduction/Intro.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;動作を確認するにはXcode 8.0以降、iOS 10.0 以降が必要なので、環境を整えるところから始めます。&lt;/p&gt;

&lt;h2 id=&#34;apple-developer-program-へ登録&#34;&gt;Apple Developer Program へ登録&lt;/h2&gt;

&lt;p&gt;諸々インストールするためには、Developer登録が必須なので、以下より登録を行います。
&lt;a href=&#34;https://developer.apple.com/programs/jp/&#34;&gt;https://developer.apple.com/programs/jp/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;xcode-8-betaをmacにインストール&#34;&gt;Xcode 8 betaをMacにインストール&lt;/h2&gt;

&lt;p&gt;Macから下記ページへアクセスし、Xcode 8 betaをダウンロード、インストールします。&lt;br /&gt;
&lt;a href=&#34;https://developer.apple.com/download/&#34;&gt;https://developer.apple.com/download/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;ios-10-betaをiphone-にインストール&#34;&gt;iOS 10 betaをiPhone にインストール&lt;/h2&gt;

&lt;p&gt;iPhoneか下記ページへアクセスし、iOS 10 betaをダウンロード、インストールします。&lt;br /&gt;
&lt;a href=&#34;https://developer.apple.com/download/&#34;&gt;https://developer.apple.com/download/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;サンプルコードをダウンロード&#34;&gt;サンプルコードをダウンロード&lt;/h2&gt;

&lt;p&gt;下記ページより、サンプルコードをダウンロードします。
&lt;a href=&#34;https://developer.apple.com/library/prerelease/content/samplecode/SpeakToMe/Introduction/Intro.html&#34;&gt;https://developer.apple.com/library/prerelease/content/samplecode/SpeakToMe/Introduction/Intro.html&lt;/a&gt;&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/16/speech-framework//speak_to_me.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;h2 id=&#34;署名の確認&#34;&gt;署名の確認&lt;/h2&gt;

&lt;p&gt;サンプルコードを開き、&lt;code&gt;TARGETS -&amp;gt; General -&amp;gt; Signing&lt;/code&gt;にDeveloper登録を行っているTeamが選択されているか確認します。
(ここが正しく設定されていないと実機でのビルドに失敗します)&lt;/p&gt;

&lt;h2 id=&#34;日本語対応&#34;&gt;日本語対応&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;ViewController.swift&lt;/code&gt;の15行目、言語指定のコードを&lt;code&gt;en-US&lt;/code&gt;から&lt;code&gt;ja-JP&lt;/code&gt;に書き換えます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: &amp;quot;ja-JP&amp;quot;))!
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;認識精度を確認&#34;&gt;認識精度を確認&lt;/h2&gt;

&lt;p&gt;実機で動作させ、認識精度を確認してみます。まず、「吾輩は猫である」を認識させてみます。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;吾輩は猫である。名前はまだ無い。どこで生れたかとんと見当がつかぬ。何でも薄暗いじめじめした所でニャーニャー泣いていた事だけは記憶している。吾輩はここで始めて人間というものを見た。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;認識結果がこちら。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;吾輩は猫である。名前はまだない。どこで生まれたかとんと見当がつかん。何でも薄暗いじめじめした所でニャンニャン泣いていた事だけは記憶している。吾輩はここで始めて人間というものを見た。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ほぼ正解と言っていい認識精度です。ここまで精度が高いとは正直驚きです。&lt;/p&gt;

&lt;h2 id=&#34;次回予告&#34;&gt;次回予告&lt;/h2&gt;

&lt;p&gt;次回はサンプルコードの中身を追ってみようと思います。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Googleカレンダー解析ツール作りました 〜時間の使い方チェッカー〜</title>
      <link>http://blog.algolab.jp/post/2016/08/16/calendar-report/</link>
      <pubDate>Tue, 16 Aug 2016 17:34:11 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/08/16/calendar-report/</guid>
      <description>

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/16/calendar-report//demo.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;タイトルの通りですが、時間の使い方チェッカーというツールを作りました (だいぶ前に作っていました)。&lt;br /&gt;
&lt;a href=&#34;http://tools.algolab.jp/calendar_reports&#34;&gt;http://tools.algolab.jp/calendar_reports&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ソースコードはGitHubに公開しています。&lt;br /&gt;
&lt;a href=&#34;https://github.com/algolab-inc/algo-tools/&#34;&gt;https://github.com/algolab-inc/algo-tools/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;時間の使い方チェッカーとは&#34;&gt;時間の使い方チェッカーとは？&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Googleカレンダーを解析して可視化するツールです&lt;/li&gt;
&lt;li&gt;タイトルを括弧で囲っているイベントのみを解析対象としています

&lt;ul&gt;
&lt;li&gt;(例) [仕事]&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;括弧を連続することで階層を表現することができます

&lt;ul&gt;
&lt;li&gt;(例) [仕事][営業]&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;データは一切保存しません (&amp;larr;大事)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;開発のきっかけ&#34;&gt;開発のきっかけ&lt;/h2&gt;

&lt;p&gt;弊社では、仕事を(準)委任契約でいただくことがあり、作業時間ログをGoogleカレンダー上で管理していました。&lt;br /&gt;
そして、月末に集計を手作業で行っていましたが、それを自動化したい、というのがきっかけです。&lt;/p&gt;

&lt;h2 id=&#34;実際に使ってみて&#34;&gt;実際に使ってみて&lt;/h2&gt;

&lt;p&gt;せっかくなので、全ての作業時間ログをGoogleカレンダー上で管理するということを数ヶ月間行ってみています。&lt;/p&gt;

&lt;p&gt;作業時間ログを都度つける行動は習慣化しているため (常にカレンダーを立ち上げています) 入力作業に負担はなく、後から時間の使い方を振り返ることができるのでなかなか良い感じです。&lt;/p&gt;

&lt;p&gt;具体的には、以下のような階層構造で管理を行っています。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;受託

&lt;ul&gt;
&lt;li&gt;プロジェクト1&lt;/li&gt;
&lt;li&gt;プロジェクト2&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;自社

&lt;ul&gt;
&lt;li&gt;サービスA&lt;/li&gt;
&lt;li&gt;サービスB&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;経営

&lt;ul&gt;
&lt;li&gt;経営計画&lt;/li&gt;
&lt;li&gt;財務&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;弊社は受託事業を行いながら自社サービスを開発しており、また筆者は代表を務めているので、時間の使い方としては、大きく「受託」「自社」「経営」の3つに分かれます。&lt;/p&gt;

&lt;p&gt;その下にも幾つか階層を設けていますが、基本的にはこの3つの時間比率を毎月振り返るようにしています。&lt;/p&gt;

&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;

&lt;p&gt;時間の使い方を可視化してみると色々な気づきがあります。是非試してみてください！&lt;br /&gt;
&lt;a href=&#34;http://tools.algolab.jp/calendar_reports&#34;&gt;http://tools.algolab.jp/calendar_reports&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Amazon Echoを6,000円で自作する 〜Raspberry Pi 3 &#43; Alexa Voice Services (AVS)〜</title>
      <link>http://blog.algolab.jp/post/2016/08/11/raspberry-pi-alexa/</link>
      <pubDate>Thu, 11 Aug 2016 19:08:44 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/08/11/raspberry-pi-alexa/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;http://blog.algolab.jp/post/2016/07/29/mary-meeker-2016/&#34;&gt;音声は新しいパラダイムシフトになる 〜2016年度版メアリー・ミーカー氏レポートまとめ〜&lt;/a&gt; でも触れたように、次世代デバイスとしてAmazon Echoは注目するべき存在です。&lt;/p&gt;

&lt;p&gt;しかしながら、日本では技適の関係で未だ使用できません。&lt;br /&gt;
ただ、Alexa Voice Services (AVS) というものが公開されており、Amazon Echoを様々なデバイスで動作させることが可能です。&lt;/p&gt;

&lt;p&gt;今回は、Raspberry Pi 3からAVSを利用できるようにしました。&lt;br /&gt;
セットアップについては下記にある通りですが、低予算での最低限の手順をまとめてみます。
&lt;a href=&#34;https://github.com/amzn/alexa-avs-raspberry-pi&#34;&gt;https://github.com/amzn/alexa-avs-raspberry-pi&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;完成したもの&#34;&gt;完成したもの&lt;/h2&gt;

&lt;p&gt;いきなり動画ですが、こんな感じで動きます。英語で話かけると、リクエストを解釈して実行してくれたり、音声で応答してくれて面白いです。

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/fWubPL5_YaU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34; allowfullscreen frameborder=&#34;0&#34;&gt;&lt;/iframe&gt;
 &lt;/div&gt;
&lt;/p&gt;

&lt;h2 id=&#34;用意したもの&#34;&gt;用意したもの&lt;/h2&gt;

&lt;p&gt;音声入力にUSBマイクロフォンが必要なので、Raspberry Pi 3と併せて購入。他はありあわせで用意しました。&lt;br /&gt;
Raspberry Pi用のディスプレイを用意してもよいですが、今回はVNC server (Linux版リモートデスクトップ) を使います。&lt;/p&gt;

&lt;h3 id=&#34;買ったもの&#34;&gt;買ったもの&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Raspberry Pi 3 (4,800円)

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.co.jp/gp/product/B01D1FR2WE/&#34;&gt;https://www.amazon.co.jp/gp/product/B01D1FR2WE/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;USBマイクロフォン (1,600円)

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.co.jp/gp/product/B0027WPY82&#34;&gt;https://www.amazon.co.jp/gp/product/B0027WPY82&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;ありあわせ&#34;&gt;ありあわせ&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Micro SDカード

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.co.jp/dp/B00CDJNOX6/&#34;&gt;https://www.amazon.co.jp/dp/B00CDJNOX6/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Micro-USB (A-MicroB) ケーブル&lt;/li&gt;
&lt;li&gt;スピーカー&lt;/li&gt;
&lt;li&gt;LANケーブル&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;raspberry-pi-を起動する&#34;&gt;Raspberry Pi を起動する&lt;/h2&gt;

&lt;h3 id=&#34;osイメージの準備&#34;&gt;OSイメージの準備&lt;/h3&gt;

&lt;p&gt;以下の記事を参考に進めました。&lt;br /&gt;
&lt;a href=&#34;http://qiita.com/onlyindreams/items/acc70807b69b43e176bf&#34;&gt;Raspberry Pi 3にRaspbianをインストール(Mac OS X を使用)&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Rasbian Jessie は &lt;code&gt;2016-05-27&lt;/code&gt; リリースのものを用いました。&lt;/li&gt;
&lt;li&gt;ddコマンドのオプションで、ブロックサイズを大文字 (&lt;code&gt;bs=1M&lt;/code&gt;) で指定&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;起動手順&#34;&gt;起動手順&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;MicroSD、LAN、 USBマイクロフォン、スピーカーを接続しておきます。&lt;/li&gt;
&lt;li&gt;電源用としてUSBケーブルを挿すとBIOSが起動します。今回はOSであるRaspbian Jessieも自動で起動しました。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;必要なアカウント-ライブラリの準備&#34;&gt;必要なアカウント・ライブラリの準備&lt;/h2&gt;

&lt;p&gt;AVSを利用するために必要なものを諸々準備します。&lt;/p&gt;

&lt;h3 id=&#34;amazon-developer-アカウントの登録&#34;&gt;Amazon Developer アカウントの登録&lt;/h3&gt;

&lt;p&gt;下記よりアカウントを登録します。登録済みであれば不要です。&lt;br /&gt;
&lt;a href=&#34;https://developer.amazon.com/login.html&#34;&gt;https://developer.amazon.com/login.html&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;サンプルアプリのダウンロード&#34;&gt;サンプルアプリのダウンロード&lt;/h3&gt;

&lt;p&gt;公式のGithub上にある &lt;a href=&#34;https://github.com/amzn/alexa-avs-raspberry-pi/archive/master.zip&#34;&gt;Sample app&lt;/a&gt; をダウンロード&amp;amp;解凍して下記のようにデスクトップなどのパスに保存します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;/home/pi/Desktop/alexa-avs-raspberry-pi-master/
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;vnc-serverのインストール&#34;&gt;VNC Serverのインストール&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# install
sudo apt-get install tightvncserver
# run
tightvncserver
# auto run setup
vi /home/pi/.config/tightvnc.desktop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;tightvnc.desktop&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[Desktop Entry]
Type=Application
Name=TightVNC
Exec=vncserver :1
StartupNotify=false
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;vlcのインストール&#34;&gt;VLCのインストール&lt;/h3&gt;

&lt;p&gt;VLC media playerをインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# install
sudo apt-get install vlc-nox vlc-data
# add env vars
echo &amp;quot;export LD_LIBRARY_PATH=/usr/lib/vlc&amp;quot; &amp;gt;&amp;gt; ~/.bashrc
echo &amp;quot;export VLC_PLUGIN_PATH=/usr/lib/vlc/plugins&amp;quot; &amp;gt;&amp;gt; ~/.bashrc
soure ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;nodeとnpmのインストール&#34;&gt;NodeとNPMのインストール&lt;/h3&gt;

&lt;p&gt;後に出てくるサーバーの起動に必要なNodeとNPMをインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# apt-get update &amp;amp; upgrade. It takes about 15 min.
sudo apt-get update &amp;amp;&amp;amp; sudo apt-get upgrade
# install nodejs
curl -sL https://deb.nodesource.com/setup | sudo bash -
sudo apt-get install nodejs
cd /home/pi/Desktop/alexa-avs-raspberry-pi-master/samples/companionService
npm install
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;jdkとmavenのインストール&#34;&gt;JDKとMavenのインストール&lt;/h3&gt;

&lt;p&gt;公式DocはMavenの環境変数は &lt;code&gt;/etc/profile.d/maven.sh&lt;/code&gt; に追加する方法ですが、うまくいかなかったので手っ取り早く &lt;code&gt;bashrc&lt;/code&gt; に追加して進めました。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# java
cd /home/pi/Desktop/alexa-avs-raspberry-pi-master/samples/javaclient
./install-java8.sh
# maven
wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz
sudo tar xvf apache-maven-3.3.9-bin.tar.gz  -C /opt
# add maven_vars
echo &amp;quot;export M2_HOME=/opt/apache-maven-3.3.9&amp;quot; &amp;gt;&amp;gt; ~/.bashrc
echo &amp;quot;export PATH=$PATH:$M2_HOME/bin&amp;quot; &amp;gt;&amp;gt; ~/.bashrc
source ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;証明書生成スクリプトを実行&#34;&gt;証明書生成スクリプトを実行&lt;/h3&gt;

&lt;p&gt;プロダクトID、シリアル番号、パスワードの3つを入力します。今回はパスワードは空のままで進めます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;/home/pi/Desktop/alexa-avs-raspberry-pi-master/samples/javaclient/generate.sh
&amp;gt; product ID: my_device
&amp;gt; Serial Number: 123456
&amp;gt; Password: [blank]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;クライアントidとclientsecretを発行&#34;&gt;クライアントIDとClientSecretを発行&lt;/h3&gt;

&lt;p&gt;ここは &lt;a href=&#34;https://github.com/amzn/alexa-avs-raspberry-pi#user-content-6---getting-started-with-alexa-voice-service&#34;&gt;公式Doc&lt;/a&gt; の画像のとおり進めればよいです。&lt;/p&gt;

&lt;h3 id=&#34;サーバとクライアントを起動&#34;&gt;サーバとクライアントを起動&lt;/h3&gt;

&lt;p&gt;下記のとおりサーバを起動します。 &lt;code&gt;config.js&lt;/code&gt; には先ほど発行したクライアントIDとClientSecretを入力しておきます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# setup clientId and ClientSecret
vi /home/pi/Desktop/alexa-avs-raspberry-pi-master/samples/companionService/config.js
cd /home/pi/Desktop/alexa-avs-raspberry-pi-master/samples/companionService
npm start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;続いてクライアントも起動します。起動するとGUIも一緒に立ち上がります。 &lt;code&gt;DISPLAY=:1.0&lt;/code&gt; はVNC経由の場合の指定です。外部ディスプレイを使う場合は &lt;code&gt;DISPLAY=:0.0&lt;/code&gt; です。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd /home/pi/Desktop/alexa-avs-raspberry-pi-master/samples/javaclient
mvn install
export DISPLAY=:1.0
mvn exec:exec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;GUIに出てくるURLにアクセスしてデバイスの登録になります。ここも &lt;a href=&#34;https://github.com/amzn/alexa-avs-raspberry-pi#user-content-10---obtain-authorization-from-login-with-amazon&#34;&gt;公式Doc&lt;/a&gt; の画像のとおりです。以上が終わると、AVSを利用できます。&lt;/p&gt;

&lt;h2 id=&#34;次回予告&#34;&gt;次回予告&lt;/h2&gt;

&lt;p&gt;次回はAlexa Skillsを登録して使ってみようと思います。乞うご期待。Don&amp;rsquo;t miss out!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ディープラーニング徹底入門 〜AIトレーニング第1回〜</title>
      <link>http://blog.algolab.jp/post/2016/08/07/deep-learning-introduction/</link>
      <pubDate>Sun, 07 Aug 2016 15:46:48 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/08/07/deep-learning-introduction/</guid>
      <description>

&lt;p&gt;弊社で実施している &lt;a href=&#34;http://blog.algolab.jp/post/2016/07/25/ai-training/&#34;&gt;AIトレーニング&lt;/a&gt; の実況中継シリーズです。&lt;br /&gt;
前回の内容は &lt;a href=&#34;http://blog.algolab.jp/post/2016/08/01/ai-training-kick-off/&#34;&gt;AIトレーニングキックオフ 〜ハムケツを認識したい〜&lt;/a&gt; をご覧ください。&lt;br /&gt;
今回は、ディープラーニングのイメージを掴んでもらうためにAさんにお話した内容をお届けします。&lt;/p&gt;

&lt;h2 id=&#34;画像認識とは&#34;&gt;画像認識とは？&lt;/h2&gt;

&lt;p&gt;さて、突然ですが、皆さんに問題です。&lt;br /&gt;
以下のようにグループが分かれているものとします。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/07/deep-learning-introduction//group_1.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;このとき、下の画像はどちらのグループになるでしょうか？&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/07/deep-learning-introduction//square_red.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;正解は「B」です。正解者の皆さんおめでとうございます！&lt;br /&gt;
ではなぜ「B」なのか。おそらくこう考えたはずです。&lt;/p&gt;

&lt;p&gt;「色がポイントだ。青ければグループA、赤ければグループB。だから、この画像はグループBだろう。」&lt;/p&gt;

&lt;p&gt;図式化すると、下記のような形かと思います。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/07/deep-learning-introduction//image_recognition_1.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;これが画像を見分けるメカニズムで、これを機械に代替させようというのが機械学習における画像認識の分野です。&lt;br /&gt;
そして、機械学習の文脈では、前半を「特徴抽出」、後半を「分類」と呼びます。&lt;/p&gt;

&lt;h2 id=&#34;画像認識の難しさ&#34;&gt;画像認識の難しさ&lt;/h2&gt;

&lt;p&gt;さて、冒頭に出した問題ですが、以下のようなグループ分けだったらどうでしょう。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/07/deep-learning-introduction//group_2.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;先ほどは「色」に注目しましたが、今度は「形」に注目して判定する必要がありそうです。&lt;br /&gt;
図を再掲すると以下のようになります。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/07/deep-learning-introduction//image_recognition_2.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;このように、注目するポイントは問題設定によって異なります。&lt;br /&gt;
そして、この注目する部分を決める「特徴抽出」の設計は、人間が行う必要があり、大変でした。&lt;/p&gt;

&lt;h2 id=&#34;ディープラーニングとは&#34;&gt;ディープラーニングとは？&lt;/h2&gt;

&lt;p&gt;これらの作業を機械が自動的に行ってくれるのがディープラーニングです。&lt;br /&gt;
具体的には、この画像はグループA、この画像はグループBといったように大量の画像を機械に覚えさせるだけで良い、というイメージです。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/07/deep-learning-introduction//deep_learning.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;つまり、これまでよりも簡単に画像認識ができるようになったということです。しかも、より高い精度が出るようになったということで、ディープラーニングは爆発的に広まりました。&lt;/p&gt;

&lt;h2 id=&#34;畳み込みニューラルネットワーク-cnn&#34;&gt;畳み込みニューラルネットワーク (CNN)&lt;/h2&gt;

&lt;p&gt;画像認識におけるディープラーニングでは、畳み込みニューラルネットワークが用いられるのが一般的です。&lt;br /&gt;
教科書などでは、よく下記のような図が用いられていると思います。
&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/07/deep-learning-introduction//dnn.png&#34;/&gt;
  
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;縦に並んだ丸の列が「層」を表しており、上記の図は4層のニューラルネットワークを表現しています。&lt;br /&gt;
大雑把にいうと、最後の層の前までで「特徴抽出」を行い、最後の層で「分類」を行っていると考えてください。&lt;br /&gt;
本当はそれぞれの層に意味がありますが、現段階では上記の理解で十分かと思います。&lt;/p&gt;

&lt;h2 id=&#34;次回予告&#34;&gt;次回予告&lt;/h2&gt;

&lt;p&gt;ディープラーニング（畳み込みニューラルネットワーク）のイメージを掴んでいただいた上で、Aさんに課題を設定しました。&lt;/p&gt;

&lt;p&gt;「学習済みモデルを用いて、画像の特徴抽出を行い、次元削減を行った上で可視化せよ」&lt;/p&gt;

&lt;p&gt;見慣れない用語が幾つか出てきましたが、調べれば理解出来る内容だと考え、上記課題としました。&lt;br /&gt;
次回はその内容についてお届けします。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TorchをAWSのGPUインスタンス (Ubuntu 14.04) で動かす</title>
      <link>http://blog.algolab.jp/post/2016/08/03/torch-aws-gpu-ubuntu/</link>
      <pubDate>Wed, 03 Aug 2016 17:12:34 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/08/03/torch-aws-gpu-ubuntu/</guid>
      <description>

&lt;p&gt;TorchをAWSのGPUインスタンス (Ubuntu 14.04) で動かす手順をまとめます。&lt;br /&gt;
環境は以下の通りです。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ubuntu Server 14.04 LTS&lt;/li&gt;
&lt;li&gt;CUDA7.5&lt;/li&gt;
&lt;li&gt;CuDNN v5&lt;/li&gt;
&lt;li&gt;Torch7&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;インスタンスを起動&#34;&gt;インスタンスを起動&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/03/torch-aws-gpu-ubuntu//ubuntu.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;Ubuntu Server 14.04 LTS (HVM), SSD Volume Type - ami-2d39803a をベースに構築します。&lt;br /&gt;
インスタンスタイプはg2.2xlargeを用いました。&lt;br /&gt;
ストレージ容量はデフォルトの8GBでは不足するので、16GBとします。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/03/torch-aws-gpu-ubuntu//storage.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;h2 id=&#34;パッケージ更新&#34;&gt;パッケージ更新&lt;/h2&gt;

&lt;p&gt;インスタンスが起動したら、SSHでログインのうえ、まずパッケージを更新します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ sudo apt-get update
$ sudo apt-get upgrade -y
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;cudaインストール&#34;&gt;CUDAインストール&lt;/h2&gt;

&lt;p&gt;CUDAのインストールはハマりどころが多いですが、先人の知恵にならって進めます。&lt;br /&gt;
&lt;a href=&#34;https://gist.github.com/erikbern/78ba519b97b440e10640&#34;&gt;https://gist.github.com/erikbern/78ba519b97b440e10640&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;既存のドライバ (Noveau) を無効にします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ echo -e &amp;quot;blacklist nouveau\nblacklist lbm-nouveau\noptions nouveau modeset=0\nalias nouveau off\nalias lbm-nouveau off\n&amp;quot; | sudo tee /etc/modprobe.d/blacklist-nouveau.conf
echo options nouveau modeset=0 | sudo tee -a /etc/modprobe.d/nouveau-kms.conf
sudo update-initramfs -u
sudo reboot
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;必要なカーネルモジュールをインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ sudo apt-get install -y linux-image-extra-virtual
$ sudo reboot
$ sudo apt-get install -y linux-source linux-headers-`uname -r`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;CUDA7.5をインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ wget http://developer.download.nvidia.com/compute/cuda/7.5/Prod/local_installers/cuda_7.5.18_linux.run
$ chmod +x cuda_7.5.18_linux.run
$ ./cuda_7.5.18_linux.run -extract=`pwd`/nvidia_installers
$ cd nvidia_installers
$ sudo ./NVIDIA-Linux-x86_64-352.39.run
$ sudo modprobe nvidia
$ sudo ./cuda-linux64-rel-7.5.18-19867135.run
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;途中でシンボリックリンクを作成するか聞かれますが、yesを選択します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;Would you like to create a symbolic link /usr/local/cuda pointing to /usr/local/cuda-7.5? ((y)es/(n)o/(a)bort) [ default is yes ]: y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;CUDAのパスを環境変数に追加します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ echo -e &amp;quot;export PATH=/usr/local/cuda/bin:\$PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:\$LD_LIBRARY_PATH&amp;quot; | tee -a ~/.bashrc
$ source ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;cudnnインストール&#34;&gt;CUDNNインストール&lt;/h2&gt;

&lt;p&gt;まず、下記のサイトからアカウントを登録します。&lt;br /&gt;
&lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;https://developer.nvidia.com/cudnn&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;アカウント登録後、ダウンロードページから、cuDNN v5 Library for Linuxをダウンロードします。
&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/03/torch-aws-gpu-ubuntu//cudnn.png&#34;/&gt;
  
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;ダウンロードしたファイルをサーバへ転送後、サーバ上で展開します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ tar -xzf cudnn-7.5-linux-x64-v5.0-ga.tgz
$ sudo cp cuda/lib64/libcudnn* /usr/local/cuda-7.5/lib64
$ sudo cp cuda/include/cudnn.h /usr/local/cuda/include
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;torchインストール&#34;&gt;Torchインストール&lt;/h2&gt;

&lt;p&gt;公式に従って、インストールします。&lt;br /&gt;
&lt;a href=&#34;http://torch.ch/docs/getting-started.html&#34;&gt;http://torch.ch/docs/getting-started.html&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ sudo apt-get install -y git
$ git clone https://github.com/torch/distro.git ~/torch --recursive
$ cd ~/torch; bash install-deps;
$ ./install.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;環境変数を.bashrcに書き込むか聞かれますが、yesを選択します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;Do you want to automatically prepend the Torch install location
to PATH and LD_LIBRARY_PATH in your /home/ubuntu/.bashrc? (yes/no)
[yes] &amp;gt;&amp;gt;&amp;gt; 
yes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;環境変数を反映します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ source ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最後に、CUDAおよびcuDNNを使うためのLuaライブラリをインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ luarocks install cutorch
$ luarocks install cunn
$ luarocks install cunnx
$ luarocks install https://raw.githubusercontent.com/soumith/cudnn.torch/master/cudnn-scm-1.rockspec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上で環境構築は完了です。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>超シンプルにTensorFlowでDQN (Deep Q Network) を実装してみる 〜導入編〜</title>
      <link>http://blog.algolab.jp/post/2016/08/01/tf-dqn-simple-1/</link>
      <pubDate>Mon, 01 Aug 2016 22:06:25 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/08/01/tf-dqn-simple-1/</guid>
      <description>

&lt;p&gt;みなさん、DQNしてますか？&lt;br /&gt;
DQNについては、下記の記事によくまとめられており、実装してみようとした方も多いのではないでしょうか。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5&#34;&gt;DQNの生い立ち　＋　Deep Q-NetworkをChainerで書いた&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://qiita.com/icoxfog417/items/242439ecd1a477ece312&#34;&gt;ゼロからDeepまで学ぶ強化学習&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;しかし、いざ自力で動作させてみようとすると、こんな問題にぶち当たると思います。&lt;/p&gt;

&lt;p&gt;「学習時間なげえ。。。」&lt;/p&gt;

&lt;p&gt;DQNに限らず、ディープラーニングのモデルを学習させようとすると、平気で数日以上かかります。&lt;br /&gt;
そして、学習させたモデルが期待通りの動作をしなかったとしたら、もう投げ出したくなってしまいます。&lt;br /&gt;
(よくある話です)&lt;/p&gt;

&lt;p&gt;なので、筆者が新しいモデルを一から実装する際には、なるべく単純なモデル、データから始めるようにしています。&lt;/p&gt;

&lt;p&gt;ここでは、超シンプルなDQNを実装し、動作させてみることにします。&lt;br /&gt;
早速いってみましょう。CPUで3分もあれば学習が終わります！&lt;/p&gt;

&lt;h2 id=&#34;まずは動かしてみよう&#34;&gt;まずは動かしてみよう&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/01/tf-dqn-simple-1//demo-catch_ball.gif&#34;/&gt;
  
&lt;/figure&gt;


&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;

&lt;p&gt;具体的には、上図のように上から落ちてくるボールをキャッチする、というタスクを学習させます。&lt;br /&gt;
TensorFlowで実装しており、ソースコードは下記に公開しています。&lt;br /&gt;
&lt;a href=&#34;https://github.com/algolab-inc/tf-dqn-simple&#34;&gt;https://github.com/algolab-inc/tf-dqn-simple&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;環境構築&#34;&gt;環境構築&lt;/h3&gt;

&lt;p&gt;はじめにソースコードをダウンロードします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ git clone https://github.com/algolab-inc/tf-dqn-simple.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;次に、動作のためにTensorFlowとMatplotlibが必要なので、インストールします。&lt;/p&gt;

&lt;p&gt;Tensorflowについては下記リンクを参照のうえインストールを行ってください。&lt;br /&gt;
&lt;a href=&#34;https://www.tensorflow.org/versions/master/get_started/os_setup.html&#34;&gt;https://www.tensorflow.org/versions/master/get_started/os_setup.html&lt;/a&gt;&lt;br /&gt;
(2016/08/01現在、Python3.5.2 + Tensorflow0.9.0での動作を確認しています)&lt;/p&gt;

&lt;p&gt;Matolotlibはpipでインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ pip install matplotlib
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;学習&#34;&gt;学習&lt;/h3&gt;

&lt;p&gt;環境が整ったら、ソースコードのディレクトリに移動して、train.pyを叩くと学習が始まります。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ cd tf-dqn-simple
$ python train.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下記のようなログが出ていれば、正しく学習が行われています。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;EPOCH: 000/999 | WIN: 001 | LOSS: 0.0068 | Q_MAX: 0.0008&lt;br /&gt;
EPOCH: 001/999 | WIN: 002 | LOSS: 0.0447 | Q_MAX: 0.0013&lt;br /&gt;
&amp;hellip;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;数分ほどで学習が終わったかと思います。&lt;br /&gt;
では学習したモデルでテストしてみましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ python test.py
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;WIN: 001/001 (100.0%)&lt;br /&gt;
WIN: 002/002 (100.0%)&lt;br /&gt;
&amp;hellip;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;キャッチボールのアニメーションとともに、上記のようなログが出れば成功です。&lt;br /&gt;
きちんと動作しましたでしょうか？&lt;br /&gt;
学習がうまくいっていれば、おそらく100%でキャッチできていると思います。&lt;/p&gt;

&lt;p&gt;次回は、実装編についてお届けします。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AIトレーニングキックオフ 〜ハムケツを認識したい〜</title>
      <link>http://blog.algolab.jp/post/2016/08/01/ai-training-kick-off/</link>
      <pubDate>Mon, 01 Aug 2016 12:11:45 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/08/01/ai-training-kick-off/</guid>
      <description>

&lt;p&gt;弊社で実施している &lt;a href=&#34;http://blog.algolab.jp/post/2016/07/25/ai-training/&#34;&gt;AIトレーニング&lt;/a&gt; の実況中継シリーズです。&lt;br /&gt;
今回は、Aさんのキックオフの内容についてお届けします。&lt;/p&gt;

&lt;h2 id=&#34;ハムケツ&#34;&gt;ハムケツ&lt;/h2&gt;

&lt;p&gt;「あの・・ですね。その・・・。ハムケツを認識したいんです。」&lt;/p&gt;

&lt;p&gt;トレーニングの最初にゴールを設定するのですが、その際、Aさんから出たひと言目がこれでした。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/01/ai-training-kick-off//hamuketu.jpg&#34;/&gt;
  
  &lt;figcaption&gt;&lt;p&gt;http://hamuketu.blog.jp/archives/51191945.html&lt;/p&gt;&lt;/figcaption&gt;
  
&lt;/figure&gt;


&lt;p&gt;ハムケツとは最近ネットや書籍で話題になったハムスターのおしりのことで、
独特の可愛さで評判となったので目にされた方もいらっしゃるのではないでしょうか。&lt;/p&gt;

&lt;p&gt;Aさんの話を詳しくうかがってみると、妻がハムケツの得も言われぬ可愛さに身もだえていた時に「ハムケツ画像が自動的に送られてくるアプリがあったらな」と思ったとのことです。&lt;/p&gt;

&lt;p&gt;素晴らしい！ではハムケツでいきましょう、と速攻でテーマが決まり、その後は大の男二人で「ハムケツの特徴」や「こっちがハムケツ、こっちはハムケツじゃない」などと熱い議論を交わしつつ成果物のイメージをより具体的に話し合いました。&lt;/p&gt;

&lt;p&gt;実際にアプリを作るとなると様々な工程がありますが、ハムケツ画像を集める段階で人工知能がハムケツを認識してくれれば、ハムケツであるかどうかの判定を人間が行わなくてよくなります。&lt;/p&gt;

&lt;p&gt;この認識して判定するフェーズを今回のトレーニングで取り扱い、出来上がったモデルを誰にでも触ってもらえる形にしようということで、&lt;/p&gt;

&lt;p&gt;「ハムケツ画像を入力すると認識結果のテキストを出力で得られるWebアプリを作る」&lt;/p&gt;

&lt;p&gt;をゴールとすることになりました。&lt;/p&gt;

&lt;h2 id=&#34;現状を把握する&#34;&gt;現状を把握する&lt;/h2&gt;

&lt;p&gt;ゴールが決まったところで、Aさんの現状のスキルをヒアリングしていきます。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Webサービスや業務システムの設計、開発、運用を10年&lt;/li&gt;
&lt;li&gt;開発言語はJava、Ruby、Python、PHP&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;受講者のトレーニング開始時ベンチマークとしてご参考にしていただければと思います。&lt;/p&gt;

&lt;h2 id=&#34;次回予告&#34;&gt;次回予告&lt;/h2&gt;

&lt;p&gt;Aさんはだいぶ基礎がありそうなので、ディープラーニングのイメージを掴んでいただいた上で、成果物完成までのステップを提示し、それをこなしてもらう、というスタイルでトレーニングを開始することにしました。&lt;br /&gt;
なお、仕組みを理解いただくため、少し遠回りなステップを提示させていただいています。&lt;/p&gt;

&lt;p&gt;次回は、ディープラーニングのイメージを掴んでもらうためにお話した内容についてお届けします。&lt;br /&gt;
(2016/08/07 公開しました &amp;rarr; &lt;a href=&#34;http://blog.algolab.jp/post/2016/08/07/deep-learning-introduction/&#34;&gt;ディープラーニング徹底入門 〜AIトレーニング第1回〜&lt;/a&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Seq2Seqモデルを用いたチャットボット作成 〜英会話のサンプルをTorchで動かす〜</title>
      <link>http://blog.algolab.jp/post/2016/07/30/seq2seq-chatbot/</link>
      <pubDate>Sat, 30 Jul 2016 15:50:23 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/07/30/seq2seq-chatbot/</guid>
      <description>

&lt;p&gt;最近、チャットボットが話題となっていますが、自然な会話を成り立たせること、は大きな課題の一つです。&lt;br /&gt;
ここでは、Deep Learningの一種である、Seq2Seqモデルを用いて、チャットボットを動作させてみます。&lt;br /&gt;
ゴールとして、英語を学習させ、実際に会話を行ってみることを目指します。&lt;/p&gt;

&lt;h2 id=&#34;seq2seq-sequence-to-sequence-モデルとは&#34;&gt;Seq2Seq (Sequence to Sequence) モデルとは&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/30/seq2seq-chatbot//seq2seq.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;平たく言うと、ある文字列から、次の文字列を予測するモデルのことです。&lt;br /&gt;
上記の図では、「ABC」を入力として、「WXYZ」を出力 (予測) しています。&lt;/p&gt;

&lt;p&gt;Seq2Seqモデルの対話タスクへの応用を試みたのがGoogleで、2015年に下記の論文を発表しています。&lt;/p&gt;

&lt;p&gt;A Neural Conversational Model&lt;br /&gt;
&lt;a href=&#34;http://arxiv.org/abs/1506.05869&#34;&gt;http://arxiv.org/abs/1506.05869&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;これまでの対話モデルは、ドメインを絞り (飛行機を予約するなど) 、手でルールを記載する必要があったが、Seq2Seqモデルを用いて対話データを学習させることで、自然な応答ができるようになった、と論文内で述べています。&lt;/p&gt;

&lt;h2 id=&#34;実装例&#34;&gt;実装例&lt;/h2&gt;

&lt;p&gt;Seq2Seqモデルを用いたチャットボットの実装は、色々な人が公開しています。&lt;br /&gt;
&lt;a href=&#34;https://github.com/nicolas-ivanov/seq2seq_chatbot_links&#34;&gt;https://github.com/nicolas-ivanov/seq2seq_chatbot_links&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;今回は、実装例の中で、最も良い結果が出たとされている、以下のリポジトリのコードを動作させてみます。&lt;br /&gt;
&lt;a href=&#34;https://github.com/macournoyer/neuralconvo&#34;&gt;https://github.com/macournoyer/neuralconvo&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;環境構築&#34;&gt;環境構築&lt;/h2&gt;

&lt;p&gt;基本的には下記の手順で進めます。&lt;br /&gt;
&lt;a href=&#34;https://github.com/macournoyer/neuralconvo#installing&#34;&gt;https://github.com/macournoyer/neuralconvo#installing&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;筆者は下記の環境をベースに、追加で必要なLuaモジュールをインストール (更新) しました。&lt;br /&gt;
&lt;a href=&#34;http://blog.algolab.jp/post/2016/08/03/torch-aws-gpu-ubuntu/&#34;&gt;TorchをAWSのGPUインスタンス (Ubuntu 14.04) で動かす&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ luarocks install nn
$ luarocks install rnn
$ luarocks install penlight
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;データセットの準備&#34;&gt;データセットの準備&lt;/h3&gt;

&lt;p&gt;データセットは、下記で公開されている映画の台詞コーパスを用います。&lt;br /&gt;
&lt;a href=&#34;http://www.mpi-sws.org/~cristian/Cornell_Movie-Dialogs_Corpus.html&#34;&gt;http://www.mpi-sws.org/~cristian/Cornell_Movie-Dialogs_Corpus.html&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ git clone https://github.com/macournoyer/neuralconvo.git
$ cd neuralconvo/data
$ wget http://www.mpi-sws.org/~cristian/data/cornell_movie_dialogs_corpus.zip
$ unzip cornell_movie_dialogs_corpus.zip
$ mv cornell\ movie-dialogs\ corpus cornell_movie_dialogs
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;学習&#34;&gt;学習&lt;/h2&gt;

&lt;p&gt;準備が整ったら学習をしてみます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ th train.lua --cuda --dataset 50000 --hiddenSize 1000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;学習時間は4日弱で、エラー率の推移は下記となりました。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/30/seq2seq-chatbot//error.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;h2 id=&#34;会話してみる&#34;&gt;会話してみる&lt;/h2&gt;

&lt;p&gt;学習したモデルを用いて実際に会話をしてみました。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ th eval.lua
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;you&amp;gt;&lt;/strong&gt; Hello?&lt;br /&gt;
&lt;strong&gt;neuralconvo&amp;gt;&lt;/strong&gt; Hello, darling.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;you&amp;gt;&lt;/strong&gt; How are you?&lt;br /&gt;
&lt;strong&gt;neuralconvo&amp;gt;&lt;/strong&gt; I&amp;rsquo;m fine.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;you&amp;gt;&lt;/strong&gt; Are you a machine?&lt;br /&gt;
&lt;strong&gt;neuralconvo&amp;gt;&lt;/strong&gt; No, i don&amp;rsquo;t.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;you&amp;gt;&lt;/strong&gt; Are you intelligent?&lt;br /&gt;
&lt;strong&gt;neuralconvo&amp;gt;&lt;/strong&gt; No.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;それっぽい会話は成り立つようです。哲学的な質問をしてみます。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;you&amp;gt;&lt;/strong&gt; What is the purpose of living?&lt;br /&gt;
&lt;strong&gt;neuralconvo&amp;gt;&lt;/strong&gt; I&amp;rsquo;ve been watching over the phone thing&amp;hellip;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;うーん。深い&amp;hellip;!?&lt;/p&gt;

&lt;h2 id=&#34;会話が成り立たないケースもある&#34;&gt;会話が成り立たないケースもある&lt;/h2&gt;

&lt;p&gt;上記のように会話として成立するものもあれば、全く成り立たないケースもありました。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;you&amp;gt;&lt;/strong&gt; What color is the sky?&lt;br /&gt;
&lt;strong&gt;neuralconvo&amp;gt;&lt;/strong&gt; The other plate is currently in new york, in some kind of a tree in a decent, don&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;

&lt;p&gt;実用的に使える精度か、という点では疑問符が残るものの、End-to-End で学習ができるというのは魅力的です。&lt;br /&gt;
いかに有用なデータセットを構築するか、が重要なポイントとなってきそうです。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>音声は新しいパラダイムシフトになる 〜2016年度版メアリー・ミーカー氏レポートまとめ〜</title>
      <link>http://blog.algolab.jp/post/2016/07/29/mary-meeker-2016/</link>
      <pubDate>Fri, 29 Jul 2016 11:47:41 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/07/29/mary-meeker-2016/</guid>
      <description>

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/D0N5V1PjTsIasR&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;p&gt;&lt;strong&gt;「音声インターフェースは新しいパラダイムシフトになる」&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;伝説のアナリスト、メアリー・ミーカー氏は、 &lt;a href=&#34;http://www.kpcb.com/internet-trends&#34;&gt;インターネット・トレンド&lt;/a&gt; 2016年度版の中で述べています。&lt;br /&gt;
ここでは、レポートの中から、音声に関するものをまとめていきます。&lt;/p&gt;

&lt;h2 id=&#34;インターフェースの技術革新は10年毎に起きる&#34;&gt;インターフェースの技術革新は10年毎に起きる&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//114.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;ヒューマンインターフェースの歴史を振り返ってみると、ここ半世紀においては10年単位で技術革新が起きていることが分かります。&lt;br /&gt;
iPhoneによる、タッチ + カメラインターフェースが登場したのが 2007年。&lt;br /&gt;
次の10年では、SiriやAmazon Echoに代表される音声インターフェースが技術革新を起こすだろう、と予測しています。&lt;/p&gt;

&lt;h2 id=&#34;音声は最も効率の良い入力方法である&#34;&gt;音声は最も効率の良い入力方法である&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//116.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;なぜ音声か、という問いに対して、メリットと独自性の観点から理由を述べています。&lt;br /&gt;
何より、「早い」「簡単」というのが音声インターフェースのメリットでしょう。&lt;br /&gt;
また、煩雑なGUIを必要とせず、低コストで場所をとらないことから、IoTとも相性が良い、としています。&lt;/p&gt;

&lt;h2 id=&#34;音声認識は人間並みに進歩&#34;&gt;音声認識は人間並みに進歩&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//118.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;Googleの研究成果によると、語彙数、認識精度ともに年々向上しています。&lt;/p&gt;

&lt;h2 id=&#34;特に-認識精度はここ数年で急激に向上&#34;&gt;特に、認識精度はここ数年で急激に向上&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//119.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;数年前までは良くて80%程度だったものが、最近は90%を優に超えてきているのが分かります。&lt;br /&gt;
人工知能研究の権威である、BaiduのAndrew Ng氏は、精度が 99% を超えるとゲームチェンジャーになる (= 世界が変わる) と述べています。&lt;br /&gt;
技術進歩の鍵となるのはディープラーニングで、音声認識分野においては、Baiduが一歩リードしている印象です。&lt;br /&gt;
Baidu の論文については、下記の記事内でも取り上げていますのでご参照ください。&lt;br /&gt;
&lt;a href=&#34;http://blog.algolab.jp/post/2016/07/21/icml-2016-reading/#deep-speech-2-end-to-end-speech-recognition-in-english-and-mandarin&#34;&gt;ICML2016読み会 まとめ&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;音声アシスタントの利用は技術の進歩が牽引&#34;&gt;音声アシスタントの利用は技術の進歩が牽引&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//121.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;入力インターフェースがキーボードから音声に置き換わるのはまだ少し早いと前置きをしながら、利用状況についてまとめています。&lt;br /&gt;
音声アシスタントの利用者は2015年時点で65%で、使い始めるきっかけとしては、ソフトウェア技術の進歩の理由が一番とのことです。&lt;/p&gt;

&lt;h2 id=&#34;音声検索の利用は開始時点の35倍に&#34;&gt;音声検索の利用は開始時点の35倍に&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//122.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;iPhoneおよびGoogleが音声検索を開始したのが2008年ですが、その時に比べ、利用回数は右肩あがりに伸びています。&lt;/p&gt;

&lt;h2 id=&#34;タイピングが難しい中国語ではさらに伸長&#34;&gt;タイピングが難しい中国語ではさらに伸長&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//123.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;Baiduの利用状況を見ると、音声入力、音声読み上げともに伸びています。&lt;br /&gt;
スマートフォンにおける言語のタイピングのしやすさ、も音声入力への利用へ影響を与えそうです。&lt;/p&gt;

&lt;h2 id=&#34;1日に6-8回音声検索する&#34;&gt;1日に6-8回音声検索する&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//124.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;&lt;a href=&#34;http://www.soundhound.com/hound&#34;&gt;Hound&lt;/a&gt; (音声アシスタントアプリ) のデータによると、1日で6-8回音声検索を行うようです。&lt;br /&gt;
カテゴリとしては、「一般情報」「エンターテイメント」「地域情報」「アシスタント」の4つにまたがる、とのことです。&lt;/p&gt;

&lt;h2 id=&#34;2020年には音声検索が50-を超える&#34;&gt;2020年には音声検索が50%を超える&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//125.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;音声検索の利用について、過去、現在、未来をまとめています。&lt;br /&gt;
Adrew Ng氏は、2020年には検索の半分以上が音声か画像になる、と予測しています。&lt;/p&gt;

&lt;h2 id=&#34;ハンズフリー-画面フリー&#34;&gt;ハンズフリー &amp;amp; 画面フリー&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//127.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;音声インターフェースを使う理由のトップが、「手 (もしくは画面) がふさがっている時に便利だから」で、&lt;br /&gt;
利用シチュエーションとしては、「家」「車」「移動中」が大部分を占めています。&lt;/p&gt;

&lt;h2 id=&#34;プラットフォームは構築され-サードパーティの動きも速い&#34;&gt;プラットフォームは構築され、サードパーティの動きも速い&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//129.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;前のスライドで述べた「家」「車」「移動中」において、Amazon Alexaは様々なOEMを提供しています。&lt;br /&gt;
また、Alexaを拡張できるAlexa Skills Kitの開発も盛んになっています。&lt;/p&gt;

&lt;h2 id=&#34;ショッピングも迅速に&#34;&gt;ショッピングも迅速に&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//130.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;Amazonは、ショッピングをモバイルアプリから音声入力へ置き換えることを目指しています。&lt;/p&gt;

&lt;h2 id=&#34;amazon-echoの所有率は5&#34;&gt;Amazon Echoの所有率は5%&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//131.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;CIRPによると、AmazonEchoの所有者は5%で、認知度は61%とのことです。&lt;/p&gt;

&lt;h2 id=&#34;2016年は産業の変わり目となる&#34;&gt;2016年は産業の変わり目となる&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//133.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;コンピュータ産業の変わり目は後から振り返ると明確なものであると前置きした上で、iPhoneの売上が2015年にピークを迎えたことを分岐点と捉え、今後はAmazon Echoの売上が急激に伸びるのではないか、と締めくくっています。&lt;/p&gt;

&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;

&lt;p&gt;まとめは以上となります。&lt;br /&gt;
未来を見据えたときに、やはり「音声」は外せないキーワードとなってくるのではないでしょうか。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AIトレーニングはじめました</title>
      <link>http://blog.algolab.jp/post/2016/07/25/ai-training/</link>
      <pubDate>Mon, 25 Jul 2016 16:35:26 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/07/25/ai-training/</guid>
      <description>

&lt;p&gt;弊社では、ITエンジニア向けに機械学習スキルの習得をサポートする試み (AIトレーニング) をはじめました。&lt;br /&gt;
ここでは、1人目の受講者であるAさんの許可を頂き、実況中継という形で、その内容を随時公開していきたいと思います。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/25/ai-training//training.jpg&#34;/&gt;
  
  &lt;figcaption&gt;&lt;p&gt;http://www.ashinari.com/2009/07/18-024460.php&lt;/p&gt;&lt;/figcaption&gt;
  
&lt;/figure&gt;


&lt;h2 id=&#34;なぜやるのか&#34;&gt;なぜやるのか&lt;/h2&gt;

&lt;p&gt;一言で言えば、技術習得までの最短経路が人によって異なるため、です。&lt;/p&gt;

&lt;p&gt;機械学習がカバーする領域はとても広範囲に渡るため、どのようなことを身につけたいか (ゴール)、および、現状のスキルによって、習得経路が一本道ではありません。&lt;/p&gt;

&lt;p&gt;一方で、書籍などの教材は、体系的・網羅的に記述されているものの、個々に最適化されてはいません。&lt;/p&gt;

&lt;p&gt;そのため、機械学習スキルを身につけようとした際、何から手をつけて良いかわからない、入門書から始めてみたがゴールにどう繋がるか見えず挫折してしまった、というようなケースがよく見られます。&lt;/p&gt;

&lt;p&gt;そこで、個々に最適化した道筋を提示することで上記の問題が解決できるのではないか、と考え、AIトレーニングを開始することにしました。&lt;/p&gt;

&lt;h2 id=&#34;どうやってやるのか&#34;&gt;どうやってやるのか&lt;/h2&gt;

&lt;p&gt;基本的には、弊社は道筋を示す役割に徹し、独習という形で進めさせていただいています。&lt;/p&gt;

&lt;p&gt;具体的には、週1回程度オフラインでお打ち合わせをさせていただきながら、現時点での習得状況を踏まえ、道筋を柔軟に提示させていただきます。&lt;/p&gt;

&lt;p&gt;また、理解の助けになると判断した場合には、その場で講習という形を取る場合もあります。&lt;br /&gt;
そして、随時出てくる疑問点においては、オンラインでの質問を受け付けています。&lt;/p&gt;

&lt;p&gt;期間としては、内容にもよりますが、2ヶ月程度での習得を目指しています。&lt;/p&gt;

&lt;h2 id=&#34;次回予告&#34;&gt;次回予告&lt;/h2&gt;

&lt;p&gt;次回はAさんのキックオフの内容についてお届けします。&lt;br /&gt;
(2016/8/1 公開しました &amp;rarr; &lt;a href=&#34;http://blog.algolab.jp/post/2016/08/01/ai-training-kick-off/&#34;&gt;AIトレーニングキックオフ 〜ハムケツを認識したい〜&lt;/a&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ICML2016読み会まとめ</title>
      <link>http://blog.algolab.jp/post/2016/07/21/icml-2016-reading/</link>
      <pubDate>Fri, 22 Jul 2016 10:14:41 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/07/21/icml-2016-reading/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;http://connpass.com/event/34960/&#34;&gt;ICML2016読み会&lt;/a&gt; の内容をまとめました。&lt;br /&gt;
ニコ生配信URL：&lt;a href=&#34;http://live.nicovideo.jp/watch/lv268597918&#34;&gt;http://live.nicovideo.jp/watch/lv268597918&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;icml概要&#34;&gt;ICML概要&lt;/h2&gt;

&lt;p&gt;林浩平さん (&lt;a href=&#34;https://twitter.com/hayasick&#34;&gt;@hayasick&lt;/a&gt;
) / 産業技術総合研究所&lt;/p&gt;

&lt;p&gt;&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/2OrjGekQEu2Bgr&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;

&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ICMLはNIPSに次ぐ機械学習の国際会議&lt;/li&gt;
&lt;li&gt;ディープラーニングと、それに伴う最適化がトレンド&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;dropout-distillation&#34;&gt;Dropout distillation&lt;/h2&gt;

&lt;p&gt;佐野正太郎さん (&lt;a href=&#34;https://twitter.com/g_votte&#34;&gt;@g_votte&lt;/a&gt;
) / リクルートコミュニケーションズ&lt;/p&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/GSKKG9nwXe83zR&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;p&gt;論文: &lt;a href=&#34;http://jmlr.org/proceedings/papers/v48/bulo16.html&#34;&gt;http://jmlr.org/proceedings/papers/v48/bulo16.html&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Dropout is a popular stochastic regularization technique for deep neural networks that works by randomly dropping (i.e. zeroing) units from the network during training. This randomization process allows to implicitly train an ensemble of exponentially many networks sharing the same parametrization, which should be averaged at test time to deliver the final prediction. A typical workaround for this intractable averaging operation consists in scaling the layers undergoing dropout randomization. This simple rule called ’standard dropout’ is efficient, but might degrade the accuracy of the prediction. In this work we introduce a novel approach, coined ’dropout distillation’, that allows us to train a predictor in a way to better approximate the intractable, but preferable, averaging process, while keeping under control its computational efficiency. We are thus able to construct models that are as efficient as standard dropout, or even more efficient, while being more accurate. Experiments on standard benchmark datasets demonstrate the validity of our method, yielding consistent improvements over conventional dropout.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;Dropoutを学習に用いた場合、その予測において、時間と精度を両立することが難しかった&lt;/li&gt;
&lt;li&gt;Distillation (蒸留法) を応用することで、短時間で精度よく予測を行うモデルを構築した&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;「Dropout Distillation」Dropout を使って学習したモデルを利用して予測をするときの話。本当は Dropout の平均を取ると精度が上がるが遅い。そこで Distillation を使ったらよかったよという話。 &lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; Takuya Akiba (@iwiwi) &lt;a href=&#34;https://twitter.com/iwiwi/status/756002188770410496&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt; 来た。間違えて歌舞伎座行ってしまった（汗&lt;br&gt;dropout distillation、普通のドロップアウトより良いという話だが、学習時間のオーダーじゃなくて学習時間が同じ、で比べないとフェアじゃない気がする。平均化SGDあたりと実は等価だったりしないのかなあ&lt;/p&gt;&amp;mdash; Nakatani Shuyo (@shuyo) &lt;a href=&#34;https://twitter.com/shuyo/status/756002470573121536&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&#34;learning-convolutional-neural-networks-for-graphs&#34;&gt;Learning Convolutional Neural Networks for Graphs&lt;/h2&gt;

&lt;p&gt;秋葉拓哉さん (&lt;a href=&#34;https://twitter.com/iwiwi&#34;&gt;@iwiwi&lt;/a&gt;
) / Preferred Networks&lt;/p&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/3htE46MNnSfNQy&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;p&gt;論文: &lt;a href=&#34;http://jmlr.org/proceedings/papers/v48/niepert16.html&#34;&gt;http://jmlr.org/proceedings/papers/v48/niepert16.html&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Numerous important problems can be framed as learning from graph data. We propose a framework for learning convolutional neural networks for arbitrary graphs. These graphs may be undirected, directed, and with both discrete and continuous node and edge attributes. Analogous to image-based convolutional networks that operate on locally connected regions of the input, we present a general approach to extracting locally connected regions from graphs. Using established benchmark data sets, we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;グラフ構造 (化学化合物など) をCNNで学習させたいが、そのまま突っ込むことは難しい&lt;/li&gt;
&lt;li&gt;WL カーネルを応用したアルゴリズムを用いて、グラフ構造をテンソルに変換した
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;グラフをCNNに突っ込む話、これはCNNになってない的結論になってたが、k方向は近傍の展開なので、ちゃんとCNNの一般化になってる。CNNじゃなくてもいいよね、というのは確かに思ったが、最初に選んだ頂点数wをグラフごとに変えたいならCNNにするしか &lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; Nakatani Shuyo (@shuyo) &lt;a href=&#34;https://twitter.com/shuyo/status/756011698117419008&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;estimating-structured-vector-autoregressive-models&#34;&gt;Estimating Structured Vector Autoregressive Models&lt;/h2&gt;

&lt;p&gt;谷本啓さん (&lt;a href=&#34;https://twitter.com/akira_dev&#34;&gt;@akira_dev&lt;/a&gt;
) / NEC&lt;/p&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/uzp8O2l8b8LyZK&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;p&gt;論文: &lt;a href=&#34;http://jmlr.org/proceedings/papers/v48/melnyk16.html&#34;&gt;http://jmlr.org/proceedings/papers/v48/melnyk16.html&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Numerous important problems can be framed as learning from graph data. We propose a framework for learning convolutional neural networks for arbitrary graphs. These graphs may be undirected, directed, and with both discrete and continuous node and edge attributes. Analogous to image-based convolutional networks that operate on locally connected regions of the input, we present a general approach to extracting locally connected regions from graphs. Using established benchmark data sets, we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;一般のノルムでの正規化を用いた VARモデルの推定の非漸近的な解析を行った&lt;/li&gt;
&lt;li&gt;収束ルートはi.i.dと同じだった&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;meta-learning-with-memory-augmented-neural-networks&#34;&gt;Meta-Learning with Memory-Augmented Neural Networks&lt;/h2&gt;

&lt;p&gt;渡辺有祐さん / SONY&lt;/p&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/tj8ML4cdtq2M7s&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;p&gt;論文: &lt;a href=&#34;http://jmlr.org/proceedings/papers/v48/santoro16.html&#34;&gt;http://jmlr.org/proceedings/papers/v48/santoro16.html&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;Neural Turing Machine を One-Shot Learning に応用し、高い精度を得た&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt; transfer learning + contextual bandit てところか．この後，multi-step 行動最適化にするんだろうな．&lt;/p&gt;&amp;mdash; Ugo-Nama (@movingsloth) &lt;a href=&#34;https://twitter.com/movingsloth/status/756028694196396032&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;「Meta-Learning with Memory-Augmented Neural Networks」Neural Turing Machine の拡張を提案。One-Shot Learning のアルゴリズムを学習させると k 近傍法よりも高い性能。 &lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; Takuya Akiba (@iwiwi) &lt;a href=&#34;https://twitter.com/iwiwi/status/756031082303152130&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&#34;pixel-recurrent-neural-networks&#34;&gt;Pixel Recurrent Neural Networks&lt;/h2&gt;

&lt;p&gt;得居誠也さん (&lt;a href=&#34;https://twitter.com/beam2d&#34;&gt;@beam2d&lt;/a&gt;
) / Preferred Networks&lt;/p&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/3bJ5TFJmX7Bk1K&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;p&gt;論文: &lt;a href=&#34;http://jmlr.org/proceedings/papers/v48/oord16.html&#34;&gt;http://jmlr.org/proceedings/papers/v48/oord16.html&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;画像生成において、VAEやGANとは異なる自己回帰 (RNNに似たもの) のアプローチを用いたところ、綺麗な画像を生成できた&lt;/li&gt;
&lt;li&gt;並列化により、高速な勾配計算も両立した&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;PixelRNNの話、これに似てるなぁ &lt;a href=&#34;https://t.co/oI8oDBUm1e&#34;&gt;https://t.co/oI8oDBUm1e&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; snneko (@snneko) &lt;a href=&#34;https://twitter.com/snneko/status/756037448879136768&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;PixelRNN、出力は各チャンネル 256値の多クラス分類なのか。すげー。思いついてもやらない系だなあ。でもそれでくっきりした画像が生成できる可能性があるなら今度試してみるかなあ。&lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; Nakatani Shuyo (@shuyo) &lt;a href=&#34;https://twitter.com/shuyo/status/756038954550304768&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;定性的ってレベルじゃない単なる個人的な印象だけど、GAN系の生成する画像はノイジーでぼやっとしがちだが、何が写っているかはわかる。PixelRNN はくっきりした画像を生成するが、よく見たら何が写っているのかさっぱりわからん。 &lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; Nakatani Shuyo (@shuyo) &lt;a href=&#34;https://twitter.com/shuyo/status/756042609060040705&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&#34;dynamic-memory-networks-for-visual-and-textual-question-answering&#34;&gt;Dynamic Memory Networks for Visual and Textual Question Answering&lt;/h2&gt;

&lt;p&gt;花木健太郎さん &lt;a href=&#34;https://twitter.com/csstudyabroad&#34;&gt;@csstudyabroad&lt;/a&gt;
 / IBM&lt;/p&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/efLeWADDGXz14D&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;p&gt;論文: &lt;a href=&#34;http://jmlr.org/proceedings/papers/v48/xiong16.html&#34;&gt;http://jmlr.org/proceedings/papers/v48/xiong16.html&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Neural network architectures with memory and attention mechanisms exhibit certain reason- ing capabilities required for question answering. One such architecture, the dynamic memory net- work (DMN), obtained high accuracy on a variety of language tasks. However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images. Based on an analysis of the DMN, we propose several improvements to its memory and input modules. Together with these changes we introduce a novel input module for images in order to be able to answer visual questions. Our new DMN+ model improves the state of the art on both the Visual Question Answering dataset and the bAbI-10k text question-answering dataset without supporting fact supervision.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;Memory Network内のMemory Moduleを改良した&lt;/li&gt;
&lt;li&gt;テキストではなく、画像を用いたQAタスクでも精度が出た&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;Memory Network，なんかピンとこないんだよなあ。これでもかってくらいヒューリスティックを積んで積んで積みまくってくるからかなあ。 &lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; Nakatani Shuyo (@shuyo) &lt;a href=&#34;https://twitter.com/shuyo/status/756047518291460096&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Memory Networks for Language Understanding, ICML Tutorial 2016 &lt;a href=&#34;https://t.co/ssPEmP1Pf9&#34;&gt;https://t.co/ssPEmP1Pf9&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; Pnz (@penzant) &lt;a href=&#34;https://twitter.com/penzant/status/756050102670598144&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Theano Implementation of Dynamic Memory Networks for Visual and Textual Question Answering &lt;a href=&#34;https://t.co/a8ijZKcnwS&#34;&gt;https://t.co/a8ijZKcnwS&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; Pnz (@penzant) &lt;a href=&#34;https://twitter.com/penzant/status/756050917686816768&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt; &lt;a href=&#34;https://t.co/EOaOfKLH40&#34;&gt;https://t.co/EOaOfKLH40&lt;/a&gt; 黒魔術はここね🎵&lt;/p&gt;&amp;mdash; prototechno (@prototechno) &lt;a href=&#34;https://twitter.com/prototechno/status/756051774436356096&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&#34;generative-adversarial-text-to-image-synthesis&#34;&gt;Generative Adversarial Text to Image Synthesis&lt;/h2&gt;

&lt;p&gt;廣芝和之さん (&lt;a href=&#34;https://twitter.com/hiho_karuta&#34;&gt;@hiho_karuta&lt;/a&gt;
) / ドワンゴ&lt;/p&gt;

&lt;iframe allowfullscreen=&#34;allowfullscreen&#34; frameborder=&#34;0&#34; scrolling=&#34;no&#34; src=&#34;http://niconare.nicovideo.jp/embed_works/kn1626&#34; style=&#34;max-width: 100%;&#34; width=&#34;485&#34; height=&#34;413&#34;&gt;&lt;/iframe&gt;


&lt;p&gt;論文: &lt;a href=&#34;http://jmlr.org/proceedings/papers/v48/reed16.html&#34;&gt;http://jmlr.org/proceedings/papers/v48/reed16.html&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories such as faces, album covers, room interiors and flowers. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image modeling, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;GANのアーキテクチャを応用し、 文章から画像を生成することができた&lt;/li&gt;
&lt;li&gt;文章だけでは表せない情報は、スタイル、という概念を用いて吸収した&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;文字情報ではない情報（スタイル）を転写（ex. 背景を継承）&lt;br&gt;画像からスタイルを求める関数を学習&lt;br&gt;スタイルZと文章をGに与える&lt;br&gt;スライド &lt;a href=&#34;https://t.co/c5lB0JIjgg&#34;&gt;https://t.co/c5lB0JIjgg&lt;/a&gt;&lt;br&gt;実装 &lt;a href=&#34;https://t.co/JpFDDp7bJo&#34;&gt;https://t.co/JpFDDp7bJo&lt;/a&gt;&lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; Ryobot | りょぼっと (@_Ryobot) &lt;a href=&#34;https://twitter.com/_Ryobot/status/756057271877050369&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;「Generative Adversarial Text to Image Synthesis」GAN を応用して文章から画像を生成。生成側に画像と文章を入れ、判定側は画像が文章と一致しているかも学習させる。 &lt;a href=&#34;https://t.co/ipQyxAsInL&#34;&gt;https://t.co/ipQyxAsInL&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; Takuya Akiba (@iwiwi) &lt;a href=&#34;https://twitter.com/iwiwi/status/756058225657511936&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&#34;deep-speech-2-end-to-end-speech-recognition-in-english-and-mandarin&#34;&gt;Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin&lt;/h2&gt;

&lt;p&gt;西鳥羽二郎さん (&lt;a href=&#34;https://twitter.com/jnishi&#34;&gt;@jnishi&lt;/a&gt;
) / Preferred Infrastructure&lt;/p&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/z0e4lCCp712kue&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;p&gt;論文: &lt;a href=&#34;http://jmlr.org/proceedings/papers/v48/amodei16.html&#34;&gt;http://jmlr.org/proceedings/papers/v48/amodei16.html&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech–two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, enabling experiments that previously took weeks to now run in days. This allows us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;End to Endの音声認識モデルを構築し、人手による書き起こしよりも高い精度を得た&lt;/li&gt;
&lt;li&gt;異なる言語や、雑音のあるなしにも対応可能なモデルとなった&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt; これ試そ / “WING » Downloads » Chinese Microtext Dataset” &lt;a href=&#34;https://t.co/tnkejvjfZ2&#34;&gt;https://t.co/tnkejvjfZ2&lt;/a&gt;&lt;/p&gt;&amp;mdash; prototechno (@prototechno) &lt;a href=&#34;https://twitter.com/prototechno/status/756065132116008961&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;

&lt;p&gt;まとめは以上となります。&lt;br /&gt;
貴重な場を提供していただいた関係者の方々、ありがとうございました。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>R-env:連舞® ハンズオンで RoBoHoN（ロボホン）・ Sota（ソータ）と戯れてきた</title>
      <link>http://blog.algolab.jp/post/2016/07/14/r-env-hands-on/</link>
      <pubDate>Thu, 14 Jul 2016 17:40:56 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/07/14/r-env-hands-on/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://r-env.doorkeeper.jp/events/48273&#34;&gt;[R-env:連舞® Innovation Hub] R-env:連舞®×RoBoHoNでロボットサービス開発体験ハンズオン&lt;/a&gt; に参加してきましたので、その模様をレポートします。&lt;/p&gt;

&lt;h2 id=&#34;r-env-連舞-とは&#34;&gt;R-env:連舞® とは&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/14/r-env-hands-on//r-env.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;NTTが開発を行っている、様々なロボットやデバイスと連携し、 ビジュアルプログラミング環境を用いて誰でも簡単に開発を行うことのできるプラットフォームです。&lt;/p&gt;

&lt;h2 id=&#34;robohon-とは&#34;&gt;RoBoHoN とは&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/14/r-env-hands-on//robohon.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;2016/06/29にSDKが一般公開されたのが記憶に新しいですが、SHARPが開発しているロボット電話です。&lt;br /&gt;
Androidをベースとしており、標準機能の他に、HVML (Hyper Voice Markup Language) という独自の言語を用いてシナリオを作成することができるようです。&lt;br /&gt;
SDKは &lt;a href=&#34;https://robohon.com/&#34;&gt;公式ページ&lt;/a&gt; 内のマイページよりダウンロード可能です。&lt;/p&gt;

&lt;h2 id=&#34;r-env-と-robohon-の連携&#34;&gt;R-env と RoBoHoN の連携&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/14/r-env-hands-on//r-env-robohon.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;R-envとRoBoHoNなどの機器の間でWebSocketによる通信を行うことでインタラクションを実現しています。&lt;br /&gt;
上記の図は、R-envからRoBoHonに処理を依頼する例ですが、RoBoHoN側からイベント (持ち上げられた、など) をR-envへ通知することができます。&lt;br /&gt;
また、R-envには幾つもの機器を登録することができるので、例えばRoBoHoNに「エアコンつけて！」と頼むとエアコンが起動する、といったことも実現することが可能です。&lt;/p&gt;

&lt;h2 id=&#34;ハンズオン&#34;&gt;ハンズオン&lt;/h2&gt;

&lt;p&gt;今回のハンズオンでは、R-envおよびRoBoHoNの紹介の後、30分程度の自由な開発時間がありました。&lt;br /&gt;
Sotaも動かしても良い、とのことだったので、RoBoHonとSotaに簡単な会話をさせてみることにしました。&lt;/p&gt;

&lt;h2 id=&#34;シナリオを作成する&#34;&gt;シナリオを作成する&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/14/r-env-hands-on//screen_1.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;GUIでフローを定義していく形となります。&lt;br /&gt;
ボックスにイベントの定義、矢印にイベント遷移条件の定義を行います。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/14/r-env-hands-on//screen_2.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;イベントの詳細を定義する画面です。&lt;br /&gt;
ここでは、RoBoHonのプロジェクタを起動するイベントを定義しています。&lt;/p&gt;

&lt;h2 id=&#34;動いた&#34;&gt;動いた！&lt;/h2&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/A-ArkARJUpQ&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34; allowfullscreen frameborder=&#34;0&#34;&gt;&lt;/iframe&gt;
 &lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;RoBoHoN:&lt;/strong&gt; はじめまして、ロボホンです。&lt;br /&gt;
&lt;strong&gt;Sota:&lt;/strong&gt; はじめまして、ソータです。ロボホン、プロジェクタ写して！&lt;br /&gt;
&lt;strong&gt;RoBoHoN:&lt;/strong&gt; （プロジェクタを写す）&lt;br /&gt;
&lt;strong&gt;Sota:&lt;/strong&gt; ありがとう！僕からは音楽を流すね！（音楽を流す）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;最終的に上記のようなシナリオが完成しました！リアルにモノが動くので、感動もひとしおです！！&lt;/p&gt;

&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;

&lt;p&gt;手軽にロボットプログラミングが体験でき、非常に貴重な経験となりました。&lt;br /&gt;
ハンズオンは定期的に開催しているようなので、みなさんも体験してみてはいかがでしょうか？&lt;br /&gt;
&lt;a href=&#34;https://r-env.doorkeeper.jp/&#34;&gt;R-env:連舞® Innovation Hub&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>