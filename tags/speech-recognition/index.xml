<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Speech Recognition on ALGO GEEKS</title>
    <link>http://blog.algolab.jp/tags/speech-recognition/</link>
    <description>Recent content in Speech Recognition on ALGO GEEKS</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Wed, 28 Sep 2016 16:26:57 +0900</lastBuildDate>
    <atom:link href="http://blog.algolab.jp/tags/speech-recognition/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>AlexaスキルとLambdaファンクションはどのように連携しているか</title>
      <link>http://blog.algolab.jp/post/2016/09/28/alexa-color-expert/</link>
      <pubDate>Wed, 28 Sep 2016 16:26:57 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/09/28/alexa-color-expert/</guid>
      <description>

&lt;p&gt;こちらの記事の続きとなります。&lt;br /&gt;
&lt;a href=&#34;http://blog.algolab.jp/post/2016/08/29/alexa-skills-kit/&#34;&gt;Alexa Skills KitをAWS Lamdaから使う&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;前回はサンプルとして用意されている「Color Expert」のAlexaスキルをLambdaファンクションを利用して動かしてみました。&lt;br /&gt;
今回は「Color Expert」を例としてAlexaスキルとLambdaファンクションがどのように連携しているか説明したいと思います。&lt;/p&gt;

&lt;h2 id=&#34;概念図&#34;&gt;概念図&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/09/28/alexa-color-expert//flow.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;まず、スキルの起動から一連のやり取り（正常系）を表した図が上記のようになります。&lt;/p&gt;

&lt;p&gt;大きな構造として、Alexaの中にスキル(青色)がいくつもあるイメージをしてください。各スキルで実行できる処理はIntent (赤色) として定義されます。&lt;/p&gt;

&lt;p&gt;それでは順を追って、スキル起動 （①〜④）、MyColorIsIntent （⑤〜⑧）、WhatsMyColorIntent （⑨〜⑫）の3つに分けて説明していきます。&lt;/p&gt;

&lt;h2 id=&#34;スキル起動-①-④&#34;&gt;スキル起動 （①〜④）&lt;/h2&gt;

&lt;p&gt;ユーザが「Alexa, ask ○○○」と話しかけることで処理が始まります。○○○の部分はスキル名となります。
今回の場合は「Color Expert」 なので、下記のようになります。なお、このスキル名はAlexaが持っている全てのスキルを通じてユニークである必要があります。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;ユーザ:&lt;/strong&gt;「Alexa, ask color expert」&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Alexaがリクエスト①の音声をテキストに変換し、該当するスキルの起動リクエスト②がLambdaに送られます。その後、起動メッセージを含んだレスポンス③がAlexaに返り、Alexaがその起動メーセージを音声に変換し下記の応答④が返ります。
この一連の流れでColor expertのスキルが起動します。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Alexa:&lt;/strong&gt; 「Welcome to the Alexa Skills Kit sample. Please tell me your favorite color by saying, my favorite color is red」&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;mycolorisintent-⑤-⑧&#34;&gt;MyColorIsIntent （⑤〜⑧）&lt;/h2&gt;

&lt;p&gt;次にAlexaの言うとおり下記のように話しかけてみます（⑤）。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;ユーザ:&lt;/strong&gt; 「My favorite color is blue」&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;このとき予め設定されているどの Intent かを判断し、入力である「blue」を slots にセットします。この情報はJsonに変換され、Lambda にリクエスト⑥がされることになります。ちなみに、slots にセットされる情報は音声入力の精度を高めるために Custom Slot Types で定義した情報が参照されて決まります。
Lambdaリクエスト⑥は具体的に下記のようになります。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json:lambda_request&#34;&gt;{
  &amp;quot;session&amp;quot;: {
    &amp;quot;sessionId&amp;quot;: &amp;quot;SessionId.xxx&amp;quot;,
    &amp;quot;application&amp;quot;: {
      &amp;quot;applicationId&amp;quot;: &amp;quot;amzn1.ask.skill.xxx&amp;quot;
    },
    &amp;quot;attributes&amp;quot;: {},
    &amp;quot;user&amp;quot;: {
      &amp;quot;userId&amp;quot;: &amp;quot;amzn1.ask.account.xxx&amp;quot;
    },
    &amp;quot;new&amp;quot;: false
  },
  &amp;quot;request&amp;quot;: {
    &amp;quot;type&amp;quot;: &amp;quot;IntentRequest&amp;quot;,
    &amp;quot;requestId&amp;quot;: &amp;quot;EdwRequestId.xxx&amp;quot;,
    &amp;quot;locale&amp;quot;: &amp;quot;en-US&amp;quot;,
    &amp;quot;timestamp&amp;quot;: &amp;quot;2016-09-10T10:30:30Z&amp;quot;,
    &amp;quot;intent&amp;quot;: {
      &amp;quot;name&amp;quot;: &amp;quot;MyColorIsIntent&amp;quot;,
      &amp;quot;slots&amp;quot;: {
        &amp;quot;Color&amp;quot;: {
          &amp;quot;name&amp;quot;: &amp;quot;Color&amp;quot;,
          &amp;quot;value&amp;quot;: &amp;quot;blue&amp;quot;
        }
      }
    }
  },
  &amp;quot;version&amp;quot;: &amp;quot;1.0&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lambdaは上記リクエスト⑥を受け取り、予めNode.jsなどのソースコードで定義されている処理を動かします。ソースコードの一部を見てみると、下記で「MyColorIsIntent」を判別し、 &lt;code&gt;setColorInSession()&lt;/code&gt; で処理がされることになります。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript:function_onIntent&#34;&gt;function onIntent(intentRequest, session, callback) {
    console.log(&amp;quot;onIntent requestId=&amp;quot; + intentRequest.requestId +
        &amp;quot;, sessionId=&amp;quot; + session.sessionId);

    var intent = intentRequest.intent,
        intentName = intentRequest.intent.name;

    if (&amp;quot;MyColorIsIntent&amp;quot; === intentName) {
        setColorInSession(intent, session, callback);
    } else if (&amp;quot;WhatsMyColorIntent&amp;quot; === intentName) {
        getColorFromSession(intent, session, callback);
    } else if (&amp;quot;AMAZON.HelpIntent&amp;quot; === intentName) {
        getWelcomeResponse(callback);
    } else if (&amp;quot;AMAZON.StopIntent&amp;quot; === intentName || &amp;quot;AMAZON.CancelIntent&amp;quot; === intentName) {
        handleSessionEndRequest(callback);
    } else {
        throw &amp;quot;Invalid intent&amp;quot;;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;setColorInSession()&lt;/code&gt;のソースコードも見てみます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript:function_setColorInSession&#34;&gt;function setColorInSession(intent, session, callback) {
    var cardTitle = intent.name;
    var favoriteColorSlot = intent.slots.Color;
    var repromptText = &amp;quot;&amp;quot;;
    var sessionAttributes = {};
    var shouldEndSession = false;
    var speechOutput = &amp;quot;&amp;quot;;

    if (favoriteColorSlot) {
        var favoriteColor = favoriteColorSlot.value;
        sessionAttributes = createFavoriteColorAttributes(favoriteColor);
        speechOutput = &amp;quot;I now know your favorite color is &amp;quot; + favoriteColor + &amp;quot;. You can ask me &amp;quot; +
            &amp;quot;your favorite color by saying, what&#39;s my favorite color?&amp;quot;;
        repromptText = &amp;quot;You can ask me your favorite color by saying, what&#39;s my favorite color?&amp;quot;;
    } else {
        speechOutput = &amp;quot;I&#39;m not sure what your favorite color is. Please try again&amp;quot;;
        repromptText = &amp;quot;I&#39;m not sure what your favorite color is. You can tell me your &amp;quot; +
            &amp;quot;favorite color by saying, my favorite color is red&amp;quot;;
    }

    callback(sessionAttributes,
         buildSpeechletResponse(cardTitle, speechOutput, repromptText, shouldEndSession));
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;sessionAttributes&lt;/code&gt; に &lt;code&gt;favoriteColor&lt;/code&gt; をセットし、ここで書かれているレスポンスの文言などはJsonに変換されて⑦としてAlexaに返されることになります。
その後、Alexaがこのレスポンス⑦を受取り、下記のように返答⑧が返ります。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;入力を正しく受け取れた場合&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Alexa:&lt;/strong&gt; 「I now know your favorite color is red. You can ask me. your favorite color by saying, what&amp;rsquo;s my favorite color?」&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;入力を正しく受け取れなかった場合&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Alexa:&lt;/strong&gt; 「I&amp;rsquo;m not sure what your favorite color is. Please try again」&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ちなみに、正しく受け取れた場合のレスポンス⑦の内容は下記になります。入力を正しく受け取れた場合は⑨の流れに進みます。正しく受け取れなかった場合は⑤のもう一度好きな色を教える流れになります。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json:lambda_response&#34;&gt;{
  &amp;quot;version&amp;quot;: &amp;quot;1.0&amp;quot;,
  &amp;quot;response&amp;quot;: {
    &amp;quot;outputSpeech&amp;quot;: {
      &amp;quot;type&amp;quot;: &amp;quot;PlainText&amp;quot;,
      &amp;quot;text&amp;quot;: &amp;quot;I now know your favorite color is blue. You can ask me your favorite color by saying, what&#39;s my favorite color?&amp;quot;
    },
    &amp;quot;card&amp;quot;: {
      &amp;quot;content&amp;quot;: &amp;quot;SessionSpeechlet - I now know your favorite color is blue. You can ask me your favorite color by saying, what&#39;s my favorite color?&amp;quot;,
      &amp;quot;title&amp;quot;: &amp;quot;SessionSpeechlet - MyColorIsIntent&amp;quot;,
      &amp;quot;type&amp;quot;: &amp;quot;Simple&amp;quot;
    },
    &amp;quot;reprompt&amp;quot;: {
      &amp;quot;outputSpeech&amp;quot;: {
        &amp;quot;type&amp;quot;: &amp;quot;PlainText&amp;quot;,
        &amp;quot;text&amp;quot;: &amp;quot;You can ask me your favorite color by saying, what&#39;s my favorite color?&amp;quot;
      }
    },
    &amp;quot;shouldEndSession&amp;quot;: false
  },
  &amp;quot;sessionAttributes&amp;quot;: {
    &amp;quot;favoriteColor&amp;quot;: &amp;quot;blue&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;whatsmycolorintent-⑨-⑫&#34;&gt;WhatsMyColorIntent (⑨〜⑫)&lt;/h2&gt;

&lt;p&gt;ここもIntentの処理になるので、⑤〜⑧と処理の流れは同じになります(リクエストとレスポンスの具体的な内容は割愛)。ここでもAlexaの言うとおり下記のように話しかけてみます(⑨)。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;ユーザ:&lt;/strong&gt;「What&amp;rsquo;s my favorite color?」&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;そうすると⑥と同様にどのIntentかを判断し、WhatsMyColorIntentのLambdaリクエスト⑩が送られます。
⑦と同様に &lt;code&gt;onIntent()&lt;/code&gt; の判別処理がされ、今度は &lt;code&gt;getColorFromSession()&lt;/code&gt; に進みます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript:function_getColorFromSession&#34;&gt;function getColorFromSession(intent, session, callback) {
    var favoriteColor;
    var repromptText = null;
    var sessionAttributes = {};
    var shouldEndSession = false;
    var speechOutput = &amp;quot;&amp;quot;;

    if (session.attributes) {
        favoriteColor = session.attributes.favoriteColor;
    }

    if (favoriteColor) {
        speechOutput = &amp;quot;Your favorite color is &amp;quot; + favoriteColor + &amp;quot;. Goodbye.&amp;quot;;
        shouldEndSession = true;
    } else {
        speechOutput = &amp;quot;I&#39;m not sure what your favorite color is, you can say, my favorite color &amp;quot; +
            &amp;quot; is red&amp;quot;;
    }

    callback(sessionAttributes,
         buildSpeechletResponse(intent.name, speechOutput, repromptText, shouldEndSession));
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;favoriteColor&lt;/code&gt; がセット済みであればそれを含んだメッセージを作り、なければ再度好きな色を聞くメッセージを返すという流れになります。⑪としてレスポンスを返し、Alexaが音声化して下記のように応答⑫をします。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;好きな色がセットされている場合&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Alexa:&lt;/strong&gt; 「Your favorite color is blue. Goodbyde.」&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;好きな色がセットされてない場合&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Alexa:&lt;/strong&gt; 「I&amp;rsquo;m not sure what your favorite color is, you can say, my favorite color is red」&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;なお、正常系の場合、ソースコードにあるように &lt;code&gt;shouldEndSession&lt;/code&gt; を &lt;code&gt;true&lt;/code&gt; としているので、ここでセッションを終了し Color expert のスキルは終了となります。&lt;/p&gt;

&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;概念図とソースコードを交えてAlexaスキルとLambdaファンクションがどのように機能しているかを説明しました。&lt;/li&gt;
&lt;li&gt;AlexaスキルではユーザからのメッセージとIntentとの対応付けを制御し、Lambdaファンクションの方で各Intentの処理をする仕組みになっているのが分かりました。&lt;/li&gt;
&lt;li&gt;それでは、次回オリジナルのスキルを作ってみようと思います。&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>重み付き有限状態トランスデューサ (WFST) をOpenFstで作成する</title>
      <link>http://blog.algolab.jp/post/2016/09/13/openfst/</link>
      <pubDate>Tue, 13 Sep 2016 20:00:03 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/09/13/openfst/</guid>
      <description>

&lt;p&gt;音声認識などの分野では、重み付き有限状態トランスデューサ (WFST) が今でも広く用いられています。&lt;br /&gt;
ここではOpenFstを用いて簡単なサンプルを作成してみます。&lt;br /&gt;
&lt;a href=&#34;http://www.openfst.org&#34;&gt;http://www.openfst.org&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;有限状態トランスデューサ-fst-finite-state-transducer&#34;&gt;有限状態トランスデューサ (FST: Finite-State Transducer)&lt;/h2&gt;

&lt;p&gt;FSTとは、簡単に言えば、入力記号列に対して出力記号列を返す変換器です。&lt;br /&gt;
一番単純な例からみてみましょう。下図をご覧ください。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/09/13/openfst//fst_1.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;これは&lt;code&gt;a&lt;/code&gt;の入力に対して&lt;code&gt;A&lt;/code&gt;を返すFSTです。詳しく見ていきます。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/09/13/openfst//fst_2.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;FSTには、初期状態から始まり最終状態に遷移できたもののみ出力を行う、という制約があります。&lt;br /&gt;
そして、遷移の条件が&lt;code&gt;入力:出力&lt;/code&gt;という形で表現されます。&lt;br /&gt;
ですので、上記の例では&lt;code&gt;a&lt;/code&gt;以外の入力は受け付けないことになります。&lt;/p&gt;

&lt;p&gt;少し複雑になった例をみてみましょう。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/09/13/openfst//fst_3.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;これは&lt;code&gt;ab&lt;/code&gt;の入力に対して&lt;code&gt;AB&lt;/code&gt;を、&lt;code&gt;ba&lt;/code&gt;の入力に対して&lt;code&gt;BA&lt;/code&gt;を出力します。&lt;br /&gt;
例えば、&lt;code&gt;ab&lt;/code&gt;の入力に対しては、&lt;code&gt;0&lt;/code&gt; &amp;rarr; &lt;code&gt;1&lt;/code&gt; &amp;rarr; &lt;code&gt;2&lt;/code&gt; と遷移できるので、&lt;code&gt;AB&lt;/code&gt;を出力することになります。&lt;/p&gt;

&lt;h2 id=&#34;重み付き有限状態トランスデューサ-wfst-weighted-finite-state-transducer&#34;&gt;重み付き有限状態トランスデューサ (WFST: Weighted Finite-State Transducer)&lt;/h2&gt;

&lt;p&gt;FSTに重みを加えたものがWFSTとなります。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/09/13/openfst//wfst.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;&lt;code&gt;/&lt;/code&gt;以下の数値が重みを表し、WFSTは記号列と重みを出力します。&lt;br /&gt;
例えば&lt;code&gt;ab&lt;/code&gt;の入力に対しては、&lt;code&gt;AB&lt;/code&gt;とともに重み&lt;code&gt;4.5 (= 0.5 + 1.5 + 3.0)&lt;/code&gt;を出力することになります。&lt;/p&gt;

&lt;h2 id=&#34;openfst&#34;&gt;OpenFst&lt;/h2&gt;

&lt;p&gt;それではOpenFstを用いて実際にWFSTを作成してみましょう。&lt;/p&gt;

&lt;h3 id=&#34;インストール&#34;&gt;インストール&lt;/h3&gt;

&lt;p&gt;ダウンロード及びインストール方法については下記の公式ドキュメントをご参照ください。&lt;br /&gt;
&lt;a href=&#34;http://www.openfst.org/twiki/bin/view/FST/FstDownload&#34;&gt;http://www.openfst.org/twiki/bin/view/FST/FstDownload&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;筆者の環境では、Kaldiの導入時に同時にインストールを行いました。&lt;br /&gt;
&lt;a href=&#34;http://blog.algolab.jp/post/2016/08/31/kaldi/&#34;&gt;Kaldiで音声を学習させる 〜ディープラーニングを用いた音声認識ツールキット〜&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;以下のコマンドが叩ければ、WFST作成に進むことができます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ fstcompile --help
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;wfst作成&#34;&gt;WFST作成&lt;/h3&gt;

&lt;p&gt;ここからは下記のリンクを参考に進めます。&lt;br /&gt;
&lt;a href=&#34;http://www.openfst.org/twiki/bin/view/FST/FstQuickTour&#34;&gt;http://www.openfst.org/twiki/bin/view/FST/FstQuickTour&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://kaldi-asr.org/doc/tutorial_looking.html&#34;&gt;http://kaldi-asr.org/doc/tutorial_looking.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;具体的には下図に示すWFSTを作成していきます。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/09/13/openfst//binary.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;FSTファイルはテキストで表現できます。下記のように定義していきます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# arc format: src dest ilabel olabel [weight]
# final state format: state [weight]
# lines may occur in any order except initial state must be first line
# unspecified weights default to 0.0 (for the library-default Weight type)
$ cat &amp;gt;text.fst &amp;lt;&amp;lt;EOF
0 1 a x .5
0 1 b y 1.5
1 2 c z 2.5
2 3.5
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;入力記号列は内部的には数値で表現するため、その定義を行います。&lt;code&gt;&amp;lt;eps&amp;gt;&lt;/code&gt;は空を表します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ cat &amp;gt;isyms.txt &amp;lt;&amp;lt;EOF
&amp;lt;eps&amp;gt; 0
a 1
b 2
c 3
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同様に出力記号列の定義も行います。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ cat &amp;gt;osyms.txt &amp;lt;&amp;lt;EOF
&amp;lt;eps&amp;gt; 0
x 1
y 2
z 3
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;バイナリ形式にコンパイルします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ fstcompile --isymbols=isyms.txt --osymbols=osyms.txt text.fst binary.fst
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これでWFSTが作成できました。&lt;/p&gt;

&lt;h3 id=&#34;wfstの演算&#34;&gt;WFSTの演算&lt;/h3&gt;

&lt;p&gt;上記は簡単な例ですが、様々なモデルをWFSTで表現することができます。&lt;br /&gt;
そして、WFST形式で表現すると、各種演算が可能になるというメリットがあります。&lt;/p&gt;

&lt;p&gt;簡単な演算をしてみましょう。2つのWFSTを合成する演算してみます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ fstcompose binary.fst binary.fst binary2.fst
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;テキスト形式で確認してみます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ fstprint --isymbols=isyms.txt --osymbols=osyms.txt binary2.fst
0 1 a x 1
0 1 b y 3
1 2 c z 5
2 7
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;元に比べて重みが倍になっていることがわかります。&lt;br /&gt;
また、&lt;a href=&#34;http://www.graphviz.org&#34;&gt;Graphviz&lt;/a&gt; で操作可能なdot形式で出力することもできます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ fstdraw --isymbols=isyms.txt --osymbols=osyms.txt binary2.fst binary2.dot
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Graphvizがインストールされている場合は、下記コマンドでpngに形式に出力することができます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ dot -Tpng binary2.dot &amp;gt; binary2.png
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下の画像が出力されました。
&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/09/13/openfst//binary2.png&#34;/&gt;
  
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;その他詳しい操作については公式ドキュメントをご参照ください。&lt;br /&gt;
&lt;a href=&#34;http://www.openfst.org/&#34;&gt;http://www.openfst.org/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kaldiで音声を学習させる 〜ディープラーニングを用いた音声認識ツールキット〜</title>
      <link>http://blog.algolab.jp/post/2016/08/31/kaldi/</link>
      <pubDate>Wed, 31 Aug 2016 14:39:10 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/08/31/kaldi/</guid>
      <description>

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/31/kaldi//kaldi_text_and_logo.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;h2 id=&#34;kaldiとは&#34;&gt;Kaldiとは&lt;/h2&gt;

&lt;p&gt;C++で書かれた音声認識ツールキットで、Apache Licence 2.0で公開されています。&lt;br /&gt;
音響モデルにDNN (Deep Neural Network) を用いているのが特長です。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://kaldi-asr.org/&#34;&gt;http://kaldi-asr.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;今回はKaldiを動作させ、yesかnoの音声を判別するモデルを学習させてみます。&lt;/p&gt;

&lt;h2 id=&#34;環境&#34;&gt;環境&lt;/h2&gt;

&lt;p&gt;Vagrant上のUbuntu 16.04 LTSを用いています。
&lt;pre&gt;&lt;code class=&#34;language-sh hljs bash&#34;&gt;$ cat /etc/lsb-release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=&lt;span class=&#34;hljs-number&#34;&gt;16.04&lt;/span&gt;
DISTRIB_CODENAME=xenial
DISTRIB_DESCRIPTION=&lt;span class=&#34;hljs-string&#34;&gt;&#34;Ubuntu 16.04.1 LTS&#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh hljs bash&#34;&gt;$ uname &lt;span class=&#34;hljs-operator&#34;&gt;-a&lt;/span&gt;
Linux vagrant &lt;span class=&#34;hljs-number&#34;&gt;4.4&lt;/span&gt;.&lt;span class=&#34;hljs-number&#34;&gt;0&lt;/span&gt;-&lt;span class=&#34;hljs-number&#34;&gt;31&lt;/span&gt;-generic &lt;span class=&#34;hljs-comment&#34;&gt;#50-Ubuntu SMP Wed Jul 13 00:07:12 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/p&gt;

&lt;h2 id=&#34;kaldiのダウンロード&#34;&gt;Kaldiのダウンロード&lt;/h2&gt;

&lt;p&gt;Githubよりダウンロードします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ git clone https://github.com/kaldi-asr/kaldi.git
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;インストール&#34;&gt;インストール&lt;/h2&gt;

&lt;p&gt;インストール方法は&lt;code&gt;INSTALL&lt;/code&gt;ファイルに最新情報が記載されているので、それに従います。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ cd kaldi
$ cat INSTALL
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;This is the official Kaldi INSTALL. Look also at INSTALL.md for the git mirror installation.&lt;br /&gt;
[for native Windows install, see windows/INSTALL]&lt;/p&gt;

&lt;p&gt;(1)&lt;br /&gt;
go to tools/  and follow INSTALL instructions there.&lt;/p&gt;

&lt;p&gt;(2)&lt;br /&gt;
go to src/ and follow INSTALL instructions there.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code&gt;tools&lt;/code&gt;および&lt;code&gt;src&lt;/code&gt;フォルダの&lt;code&gt;INSTALL&lt;/code&gt;を見れば良いようなので、まず&lt;code&gt;tools&lt;/code&gt;から確認していきます。&lt;/p&gt;

&lt;h2 id=&#34;toolsのインストール&#34;&gt;toolsのインストール&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ cd tools
$ cat INSTALL
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;To install the most important prerequisites for Kaldi:&lt;/p&gt;

&lt;p&gt;&amp;nbsp;first do&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&amp;nbsp;extras/check_dependencies.sh&lt;/p&gt;

&lt;p&gt;to see if there are any system-level installations or modifications you need to do.&lt;br /&gt;
Check the output carefully: there are some things that will make your life a lot&lt;br /&gt;
easier if you fix them at this stage.&lt;/p&gt;

&lt;p&gt;Then run&lt;/p&gt;

&lt;p&gt;&amp;nbsp;make&lt;/p&gt;

&lt;p&gt;If you have multiple CPUs and want to speed things up, you can do a parallel&lt;br /&gt;
build by supplying the &amp;ldquo;-j&amp;rdquo; option to make, e.g. to use 4 CPUs:&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&amp;nbsp;make -j 4&lt;/p&gt;

&lt;p&gt;By default, Kaldi builds against OpenFst-1.3.4. If you want to build against&lt;br /&gt;
OpenFst-1.4, edit the Makefile in this folder. Note that this change requires&lt;br /&gt;
a relatively new compiler with C++11 support, e.g. gcc &amp;gt;= 4.6, clang &amp;gt;= 3.0.&lt;/p&gt;

&lt;p&gt;In extras/, there are also various scripts to install extra bits and pieces that&lt;br /&gt;
are used by individual example scripts.  If an example script needs you to run&lt;br /&gt;
one of those scripts, it will tell you what to do.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;概要は以下の通りです。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;extras/check_dependencies.sh&lt;/code&gt;で依存関係をチェックする&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make&lt;/code&gt;コマンドでインストールを行う

&lt;ul&gt;
&lt;li&gt;マルチコアのCPUの場合は&lt;code&gt;j&lt;/code&gt;オプションをつけることでインストールが並列化できる (早くなる)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;依存関係のチェック&#34;&gt;依存関係のチェック&lt;/h3&gt;

&lt;p&gt;スクリプトを用いて依存関係をチェックします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ extras/check_dependencies.sh

extras/check_dependencies.sh: automake is not installed.
extras/check_dependencies.sh: autoconf is not installed.
extras/check_dependencies.sh: neither libtoolize nor glibtoolize is installed
extras/check_dependencies.sh: subversion is not installed
extras/check_dependencies.sh: default or create an bash alias for kaldi scripts to run correctly
extras/check_dependencies.sh: we recommend that you run (our best guess):
 sudo apt-get install  automake autoconf libtool subversion
You should probably do:
 sudo apt-get install libatlas3-base
/bin/sh is linked to dash, and currently some of the scripts will not run
properly.  We recommend to run:
 sudo ln -s -f bash /bin/sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;サジェストされた通りに進めます。&lt;br /&gt;
(環境によって出てくるメッセージが異なるのでご注意下さい)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ sudo apt-get install automake autoconf libtool subversion
$ sudo apt-get install -y libatlas3-base
$ sudo ln -s -f bash /bin/sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再度依存関係をチェックすると、OKとなりました。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ extras/check_dependencies.sh
extras/check_dependencies.sh: all OK.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;インストール-1&#34;&gt;インストール&lt;/h3&gt;

&lt;p&gt;まず、手元の環境のCPUコア数を調べます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ nproc
4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;筆者の環境は4コアだったので、&lt;code&gt;j&lt;/code&gt;オプションを用いて並列インストールを行います。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ sudo make -j 4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下のライブラリがインストールされます。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;OpenFst

&lt;ul&gt;
&lt;li&gt;重み付き有限状態トランスデューサー (WFST) を扱うライブラリ&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;sph2pipe

&lt;ul&gt;
&lt;li&gt;SPHEREファイルのコンバータ&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;sclite

&lt;ul&gt;
&lt;li&gt;音声認識結果をスコアリングするためのライブラリ&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;ATLAS

&lt;ul&gt;
&lt;li&gt;線形代数ライブラリ&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;CLAPACK

&lt;ul&gt;
&lt;li&gt;線形代数ライブラリ&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;オプション-言語モデルツールキットのインストール&#34;&gt;(オプション) 言語モデルツールキットのインストール&lt;/h3&gt;

&lt;p&gt;また、言語モデルのツールキット (IRSTLM や SRILM) を使用する場合は追加でインストールします。&lt;/p&gt;

&lt;h4 id=&#34;irstlm&#34;&gt;IRSTLM&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ extras/install_irstlm.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;srlm&#34;&gt;SRLM&lt;/h4&gt;

&lt;p&gt;下記からファイルをダウンロードし、&lt;code&gt;srilm.tgz&lt;/code&gt;というファイル名にした上で、&lt;code&gt;tools/&lt;/code&gt;直下に配置します。&lt;br /&gt;
&lt;a href=&#34;http://www.speech.sri.com/projects/srilm/download.html&#34;&gt;http://www.speech.sri.com/projects/srilm/download.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;また、インストールにはGNU awkが必要なので導入します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ sudo apt-get install -y gawk
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;本体をインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ extras/install_srilm.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;srcのインストール&#34;&gt;srcのインストール&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ cd ../src
$ cat INSTALL
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;These instructions are valid for UNIX-like systems (these steps have&lt;br /&gt;
been run on various Linux distributions; Darwin; Cygwin).  For native Windows&lt;br /&gt;
compilation, see ../windows/INSTALL.&lt;/p&gt;

&lt;p&gt;You must first have completed the installation steps in ../tools/INSTALL&lt;br /&gt;
(compiling OpenFst; getting ATLAS and CLAPACK headers).&lt;/p&gt;

&lt;p&gt;The installation instructions are:&lt;br /&gt;
./configure&lt;br /&gt;
make depend&lt;br /&gt;
make&lt;/p&gt;

&lt;p&gt;Note that &amp;ldquo;make&amp;rdquo; takes a long time; you can speed it up by running make&lt;br /&gt;
in parallel if you have multiple CPUs, for instance&lt;br /&gt;
 make depend -j 8&lt;br /&gt;
 make -j 8&lt;br /&gt;
For more information, see documentation at &lt;a href=&#34;http://kaldi-asr.org/doc/&#34;&gt;http://kaldi-asr.org/doc/&lt;/a&gt;&lt;br /&gt;
and click on &amp;ldquo;The build process (how Kaldi is compiled)&amp;rdquo;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;以下の3つのコマンドを叩けば良いようなので、一つずつ叩いていきます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ ./configure
$ sudo make depend -j 4
$ sudo make -j 4
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;サンプルの動作確認&#34;&gt;サンプルの動作確認&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;egs&lt;/code&gt;以下にサンプルが公開されています。&lt;br /&gt;
ここでは、&lt;code&gt;yes&lt;/code&gt;と&lt;code&gt;no&lt;/code&gt;を判別する非常に小さなタスクを学習させてみます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd ../egs/yesno
cat README.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;The &amp;ldquo;yesno&amp;rdquo; corpus is a very small dataset of recordings of one individual&lt;br /&gt;
saying yes or no multiple times per recording, in Hebrew.  It is available from&lt;br /&gt;
&lt;a href=&#34;http://www.openslr.org/1&#34;&gt;http://www.openslr.org/1&lt;/a&gt;.&lt;br /&gt;
It is mainly included here as an easy way to test out the Kaldi scripts.&lt;/p&gt;

&lt;p&gt;The test set is perfectly recognized at the monophone stage, so the dataset is&lt;br /&gt;
not exactly challenging.&lt;/p&gt;

&lt;p&gt;The scripts are in s5/.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ヘブライ語で&lt;code&gt;yes&lt;/code&gt;と&lt;code&gt;no&lt;/code&gt;を喋っているコーパスを学習データとして用いるようです。&lt;br /&gt;
&lt;code&gt;s5&lt;/code&gt;フォルダに動作用のスクリプトがあるので、動かしてみます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ cd s5
$ sh run.sh
...
%WER 0.00 [ 0 / 232, 0 ins, 0 del, 0 sub ] exp/mono0a/decode_test_yesno/wer_10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;WER (単語誤り率) が 0% という結果となりました。&lt;/p&gt;

&lt;h2 id=&#34;次回予告&#34;&gt;次回予告&lt;/h2&gt;

&lt;p&gt;次回はサンプルのソースコードを追ってみたいと思います。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Alexa Skills KitをAWS Lamdaから使う</title>
      <link>http://blog.algolab.jp/post/2016/08/29/alexa-skills-kit/</link>
      <pubDate>Mon, 29 Aug 2016 16:10:04 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/08/29/alexa-skills-kit/</guid>
      <description>

&lt;p&gt;こちらの記事の続きとなります。&lt;br /&gt;
&lt;a href=&#34;http://blog.algolab.jp/post/2016/08/11/raspberry-pi-alexa/&#34;&gt;Amazon Echoを6,000円で自作する 〜Raspberry Pi 3 + Alexa Voice Services (AVS)〜&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;前回はRaspberry PiからAVS (Alexa Voice Services) を使ってみましたが、今回は、Alexa Skills Kitを使ってみたいと思います。&lt;/p&gt;

&lt;h2 id=&#34;完成したもの&#34;&gt;完成したもの&lt;/h2&gt;

&lt;p&gt;Alexaが自分の好みの色を覚えてくれるようになりました。

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/HAOPIuFDdik&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34; allowfullscreen frameborder=&#34;0&#34;&gt;&lt;/iframe&gt;
 &lt;/div&gt;
&lt;/p&gt;

&lt;h2 id=&#34;alexa-skill-kitとは&#34;&gt;Alexa Skill Kitとは&lt;/h2&gt;

&lt;p&gt;AVSには好みの機能を追加できるSkillという機能があり、「カスタムスキル」と「スマートホームスキル」の2種類を登録することができます。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/understanding-the-different-types-of-skills&#34;&gt;https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/understanding-the-different-types-of-skills&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;カスタムスキル&#34;&gt;カスタムスキル&lt;/h3&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//custom-skill.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;ul&gt;
&lt;li&gt;ピザを注文したり、タクシーを呼んだり色々なことができる&lt;/li&gt;
&lt;li&gt;Invocation Name (スキルの呼び名) で呼び出す&lt;/li&gt;
&lt;li&gt;リクエストは「intent」としてマッピングされる

&lt;ul&gt;
&lt;li&gt;ピザの注文 &amp;rarr; OrderPizza intent&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;スマートホームスキル&#34;&gt;スマートホームスキル&lt;/h3&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//smart-home-skill.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;ul&gt;
&lt;li&gt;Smart home device (灯りやエアコンなど) を操作できる&lt;/li&gt;
&lt;li&gt;Invocation Nameで呼び出すのは不要&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;今回作るもの&#34;&gt;今回作るもの&lt;/h2&gt;

&lt;p&gt;公式の &lt;a href=&#34;https://developer.amazon.com/appsandservices/solutions/alexa/alexa-skills-kit/docs/developing-an-alexa-skill-as-a-lambda-function&#34;&gt;ドキュメント&lt;/a&gt; と &lt;a href=&#34;https://developer.amazon.com/public/community/post/TxDJWS16KUPVKO/New-Alexa-Skills-Kit-Template-Build-a-Trivia-Skill-in-under-an-Hour&#34;&gt;ポスト&lt;/a&gt; を参考に、今回は「Color Expert」のSkillを使ってみます。&lt;br /&gt;
Alexa SkillsはLambdaファンクション上で実行されるので、AWS LambdaとAlexa Skillsの設定が必要になります。&lt;/p&gt;

&lt;h2 id=&#34;aws-lambdaの作成&#34;&gt;AWS Lambdaの作成&lt;/h2&gt;

&lt;p&gt;AWSマネジメントコンソールにログインし、&lt;a href=&#34;https://console.aws.amazon.com/lambda/home&#34;&gt;Lambda&lt;/a&gt; のページを開きます。&lt;/p&gt;

&lt;p&gt;リージョンがバージニア北部(US East (N. Virginia))になっていることを確認し、なっていなければ変更します。Lambdaファンクションを利用してAlexa Skillsを使うのに、現在他のリージョンはサポートされていません。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-1.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;&lt;code&gt;Create a Lambda function&lt;/code&gt;をクリックするとBlueprint一覧画面になります。ここから&lt;code&gt;alexa-skills-kit-color-expert&lt;/code&gt;を選択します。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-2.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;Lambdaファンクションを呼び出すトリガーの選択画面になるので、灰色の点線のボックスをクリックし、&lt;code&gt;Alexa Skills Kit&lt;/code&gt;を選び&lt;code&gt;Next&lt;/code&gt;をクリックします。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-3.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;Lambdaファンクションの構成画面になります。Nameには「colorExpertTest」などと入力します。&lt;/p&gt;

&lt;p&gt;RoleにはLambdaを使うのが初めてであれば、&lt;code&gt;Create new role from template(s)&lt;/code&gt;から新しくRoleを作成し、Role Nameには「lambda_basic_execution」などと入力します。&lt;/p&gt;

&lt;p&gt;Policy templatesには&lt;code&gt;AMI read-only permissions&lt;/code&gt;などを選択すればOKです。&lt;/p&gt;

&lt;p&gt;Lambda function codeなど他の項目はデフォルトのままでも問題ありません。&lt;/p&gt;

&lt;p&gt;一通り入力・変更が終わったら&lt;code&gt;Next&lt;/code&gt;をクリックします。&lt;/p&gt;

&lt;p&gt;そうすると、下記のような確認画面になります。問題なければ&lt;code&gt;Create function&lt;/code&gt;をクリックします。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-4.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;トリガーのテスト画面になります。
&lt;code&gt;Test&lt;/code&gt;をクリック &amp;rarr; &lt;code&gt;Alexa Start Session&lt;/code&gt;を選択 &amp;rarr; &lt;code&gt;save and test&lt;/code&gt;をクリックと進むとTestが走ります。
実行結果がSuceededとなること、ログ出力に先ほどのLambda function codeの出力結果が表示されていればOKです。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-5.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;これで作成は完了です。最後にLambdaファンクションの呼び出し先となるARNをメモしておきます。上記スクリーンショットで右上の一部灰色でマスクしている文字列です。&lt;/p&gt;

&lt;h2 id=&#34;alexa-skillの作成&#34;&gt;Alexa Skillの作成&lt;/h2&gt;

&lt;p&gt;Raspberry Piが登録されているアカウントでAmazon Developer Consoleにログインし、&lt;a href=&#34;https://developer.amazon.com/edw/home.html&#34;&gt;Alexa&lt;/a&gt; のページに進みます。&lt;/p&gt;

&lt;p&gt;Alexa Skills Kitの&lt;code&gt;Get Started&lt;/code&gt;をクリックします。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-6.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;&lt;code&gt;Add a New Skill&lt;/code&gt;から新規にSkillを登録します。実際に話しかけて呼び出すときの名前となるInvocation Nameには「color expert」と入力して、&lt;code&gt;Next&lt;/code&gt;をクリックします。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-7.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;Interaction Modelの定義画面になります。これがAlexaに話しかけてやり取りをする内容になります。今回は&lt;a href=&#34;https://developer.amazon.com/appsandservices/solutions/alexa/alexa-skills-kit/docs/developing-an-alexa-skill-as-a-lambda-function&#34;&gt;公式ドキュメント&lt;/a&gt;のとおりにIntent Schame, Custom Slot Types, Sample Utterancesを下記のようにします。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-8.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;Intent_Schema&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  &amp;quot;intents&amp;quot;: [
    {
      &amp;quot;intent&amp;quot;: &amp;quot;MyColorIsIntent&amp;quot;,
      &amp;quot;slots&amp;quot;: [
        {
          &amp;quot;name&amp;quot;: &amp;quot;Color&amp;quot;,
          &amp;quot;type&amp;quot;: &amp;quot;LIST_OF_COLORS&amp;quot;
        }
      ]
    },
    {
      &amp;quot;intent&amp;quot;: &amp;quot;WhatsMyColorIntent&amp;quot;
    },
    {
      &amp;quot;intent&amp;quot;: &amp;quot;AMAZON.HelpIntent&amp;quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Custom_Slot_Type&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;LIST_OF_COLORS
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Custom_Slot_Type_Values&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;green
red
blue
orange
gold
silver
yellow
black
white
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sample_Utterances&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;WhatsMyColorIntent what&#39;s my favorite color
WhatsMyColorIntent what is my favorite color
WhatsMyColorIntent what&#39;s my color
WhatsMyColorIntent what is my color
WhatsMyColorIntent my color
WhatsMyColorIntent my favorite color
WhatsMyColorIntent get my color
WhatsMyColorIntent get my favorite color
WhatsMyColorIntent give me my favorite color
WhatsMyColorIntent give me my color
WhatsMyColorIntent what my color is
WhatsMyColorIntent what my favorite color is
WhatsMyColorIntent yes
WhatsMyColorIntent yup
WhatsMyColorIntent sure
WhatsMyColorIntent yes please
MyColorIsIntent my favorite color is {Color}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;次にEndpointなどの設定画面になります。先ほどメモしておいたARNを入力します。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-9.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;次にTest画面になります。Enter Utteranceに先ほどSample Utteranceに定義した文章を入力して&lt;code&gt;Ask color expert&lt;/code&gt;をクリックします。するとLambdaで処理が実行されて返答される文章などを含んだレスポンスが返ってきます。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-10.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;残りの設定項目に Publishing infomation, Privacy &amp;amp; Compliance がありますが、これらはAlexa Skillをpubulishingするときに必要で、手元の実機での実行には必要ないので今回は割愛します。&lt;/p&gt;

&lt;h1 id=&#34;動作確認&#34;&gt;動作確認&lt;/h1&gt;

&lt;p&gt;まずAmazon Developer Consoleと同じアカウントでAmazon Alexaにログインして&lt;a href=&#34;http://alexa.amazon.com/spa/index.html#skills/your-skills&#34;&gt;Skill一覧画面&lt;/a&gt;から先ほど作成したSkillがあることを確認します。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-11.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;あとは下記の動画のように話しかけて動作するか確認します。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>iOSで音声認識 〜Speech Frameworkを試す〜</title>
      <link>http://blog.algolab.jp/post/2016/08/16/speech-framework/</link>
      <pubDate>Tue, 16 Aug 2016 19:34:57 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/08/16/speech-framework/</guid>
      <description>

&lt;p&gt;2016/08/16現在、まだβ版という位置づけですが、Apple謹製の音声認識エンジン (Speech Framework) が公開されています。今回は、下記のサンプルコードを動作させてみます。
&lt;a href=&#34;https://developer.apple.com/library/prerelease/content/samplecode/SpeakToMe/Introduction/Intro.html&#34;&gt;https://developer.apple.com/library/prerelease/content/samplecode/SpeakToMe/Introduction/Intro.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;動作を確認するにはXcode 8.0以降、iOS 10.0 以降が必要なので、環境を整えるところから始めます。&lt;/p&gt;

&lt;h2 id=&#34;apple-developer-program-へ登録&#34;&gt;Apple Developer Program へ登録&lt;/h2&gt;

&lt;p&gt;諸々インストールするためには、Developer登録が必須なので、以下より登録を行います。
&lt;a href=&#34;https://developer.apple.com/programs/jp/&#34;&gt;https://developer.apple.com/programs/jp/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;xcode-8-betaをmacにインストール&#34;&gt;Xcode 8 betaをMacにインストール&lt;/h2&gt;

&lt;p&gt;Macから下記ページへアクセスし、Xcode 8 betaをダウンロード、インストールします。&lt;br /&gt;
&lt;a href=&#34;https://developer.apple.com/download/&#34;&gt;https://developer.apple.com/download/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;ios-10-betaをiphone-にインストール&#34;&gt;iOS 10 betaをiPhone にインストール&lt;/h2&gt;

&lt;p&gt;iPhoneか下記ページへアクセスし、iOS 10 betaをダウンロード、インストールします。&lt;br /&gt;
&lt;a href=&#34;https://developer.apple.com/download/&#34;&gt;https://developer.apple.com/download/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;サンプルコードをダウンロード&#34;&gt;サンプルコードをダウンロード&lt;/h2&gt;

&lt;p&gt;下記ページより、サンプルコードをダウンロードします。
&lt;a href=&#34;https://developer.apple.com/library/prerelease/content/samplecode/SpeakToMe/Introduction/Intro.html&#34;&gt;https://developer.apple.com/library/prerelease/content/samplecode/SpeakToMe/Introduction/Intro.html&lt;/a&gt;&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/16/speech-framework//speak_to_me.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;h2 id=&#34;署名の確認&#34;&gt;署名の確認&lt;/h2&gt;

&lt;p&gt;サンプルコードを開き、&lt;code&gt;TARGETS -&amp;gt; General -&amp;gt; Signing&lt;/code&gt;にDeveloper登録を行っているTeamが選択されているか確認します。
(ここが正しく設定されていないと実機でのビルドに失敗します)&lt;/p&gt;

&lt;h2 id=&#34;日本語対応&#34;&gt;日本語対応&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;ViewController.swift&lt;/code&gt;の15行目、言語指定のコードを&lt;code&gt;en-US&lt;/code&gt;から&lt;code&gt;ja-JP&lt;/code&gt;に書き換えます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: &amp;quot;ja-JP&amp;quot;))!
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;認識精度を確認&#34;&gt;認識精度を確認&lt;/h2&gt;

&lt;p&gt;実機で動作させ、認識精度を確認してみます。まず、「吾輩は猫である」を認識させてみます。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;吾輩は猫である。名前はまだ無い。どこで生れたかとんと見当がつかぬ。何でも薄暗いじめじめした所でニャーニャー泣いていた事だけは記憶している。吾輩はここで始めて人間というものを見た。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;認識結果がこちら。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;吾輩は猫である。名前はまだない。どこで生まれたかとんと見当がつかん。何でも薄暗いじめじめした所でニャンニャン泣いていた事だけは記憶している。吾輩はここで始めて人間というものを見た。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ほぼ正解と言っていい認識精度です。ここまで精度が高いとは正直驚きです。&lt;/p&gt;

&lt;h2 id=&#34;次回予告&#34;&gt;次回予告&lt;/h2&gt;

&lt;p&gt;次回はサンプルコードの中身を追ってみようと思います。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Amazon Echoを6,000円で自作する 〜Raspberry Pi 3 &#43; Alexa Voice Services (AVS)〜</title>
      <link>http://blog.algolab.jp/post/2016/08/11/raspberry-pi-alexa/</link>
      <pubDate>Thu, 11 Aug 2016 19:08:44 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/08/11/raspberry-pi-alexa/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;http://blog.algolab.jp/post/2016/07/29/mary-meeker-2016/&#34;&gt;音声は新しいパラダイムシフトになる 〜2016年度版メアリー・ミーカー氏レポートまとめ〜&lt;/a&gt; でも触れたように、次世代デバイスとしてAmazon Echoは注目するべき存在です。&lt;/p&gt;

&lt;p&gt;しかしながら、日本では技適の関係で未だ使用できません。&lt;br /&gt;
ただ、Alexa Voice Services (AVS) というものが公開されており、Amazon Echoを様々なデバイスで動作させることが可能です。&lt;/p&gt;

&lt;p&gt;今回は、Raspberry Pi 3からAVSを利用できるようにしました。&lt;br /&gt;
セットアップについては下記にある通りですが、低予算での最低限の手順をまとめてみます。
&lt;a href=&#34;https://github.com/amzn/alexa-avs-raspberry-pi&#34;&gt;https://github.com/amzn/alexa-avs-raspberry-pi&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;完成したもの&#34;&gt;完成したもの&lt;/h2&gt;

&lt;p&gt;いきなり動画ですが、こんな感じで動きます。英語で話かけると、リクエストを解釈して実行してくれたり、音声で応答してくれて面白いです。

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/fWubPL5_YaU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34; allowfullscreen frameborder=&#34;0&#34;&gt;&lt;/iframe&gt;
 &lt;/div&gt;
&lt;/p&gt;

&lt;h2 id=&#34;用意したもの&#34;&gt;用意したもの&lt;/h2&gt;

&lt;p&gt;音声入力にUSBマイクロフォンが必要なので、Raspberry Pi 3と併せて購入。他はありあわせで用意しました。&lt;br /&gt;
Raspberry Pi用のディスプレイを用意してもよいですが、今回はVNC server (Linux版リモートデスクトップ) を使います。&lt;/p&gt;

&lt;h3 id=&#34;買ったもの&#34;&gt;買ったもの&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Raspberry Pi 3 (4,800円)

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.co.jp/gp/product/B01D1FR2WE/&#34;&gt;https://www.amazon.co.jp/gp/product/B01D1FR2WE/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;USBマイクロフォン (1,600円)

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.co.jp/gp/product/B0027WPY82&#34;&gt;https://www.amazon.co.jp/gp/product/B0027WPY82&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;ありあわせ&#34;&gt;ありあわせ&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Micro SDカード

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.co.jp/dp/B00CDJNOX6/&#34;&gt;https://www.amazon.co.jp/dp/B00CDJNOX6/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Micro-USB (A-MicroB) ケーブル&lt;/li&gt;
&lt;li&gt;スピーカー&lt;/li&gt;
&lt;li&gt;LANケーブル&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;raspberry-pi-を起動する&#34;&gt;Raspberry Pi を起動する&lt;/h2&gt;

&lt;h3 id=&#34;osイメージの準備&#34;&gt;OSイメージの準備&lt;/h3&gt;

&lt;p&gt;以下の記事を参考に進めました。&lt;br /&gt;
&lt;a href=&#34;http://qiita.com/onlyindreams/items/acc70807b69b43e176bf&#34;&gt;Raspberry Pi 3にRaspbianをインストール(Mac OS X を使用)&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Rasbian Jessie は &lt;code&gt;2016-05-27&lt;/code&gt; リリースのものを用いました&lt;/li&gt;
&lt;li&gt;ddコマンドのオプションで、ブロックサイズは大文字 (&lt;code&gt;bs=1M&lt;/code&gt;) で指定しました&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;起動手順&#34;&gt;起動手順&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;MicroSD、LAN、 USBマイクロフォン、スピーカーを接続しておきます。&lt;/li&gt;
&lt;li&gt;電源用としてUSBケーブルを挿すとBIOSが起動します。今回はOSであるRaspbian Jessieも自動で起動しました。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;必要なアカウント-ライブラリの準備&#34;&gt;必要なアカウント・ライブラリの準備&lt;/h2&gt;

&lt;p&gt;AVSを利用するために必要なものを諸々準備します。&lt;/p&gt;

&lt;h3 id=&#34;amazon-developer-アカウントの登録&#34;&gt;Amazon Developer アカウントの登録&lt;/h3&gt;

&lt;p&gt;下記よりアカウントを登録します。登録済みであれば不要です。&lt;br /&gt;
&lt;a href=&#34;https://developer.amazon.com/login.html&#34;&gt;https://developer.amazon.com/login.html&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;サンプルアプリのダウンロード&#34;&gt;サンプルアプリのダウンロード&lt;/h3&gt;

&lt;p&gt;公式のGithub上にある &lt;a href=&#34;https://github.com/amzn/alexa-avs-raspberry-pi/archive/master.zip&#34;&gt;Sample app&lt;/a&gt; をダウンロード&amp;amp;解凍して下記のようにデスクトップなどのパスに保存します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;/home/pi/Desktop/alexa-avs-raspberry-pi-master/
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;vnc-serverのインストール&#34;&gt;VNC Serverのインストール&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# install
$ sudo apt-get install tightvncserver
# run
$ tightvncserver
# auto run setup
$ vi /home/pi/.config/tightvnc.desktop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;tightvnc.desktop&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[Desktop Entry]
Type=Application
Name=TightVNC
Exec=vncserver :1
StartupNotify=false
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;vncでraspberry-piへアクセス&#34;&gt;VNCでRaspberry Piへアクセス&lt;/h3&gt;

&lt;p&gt;Macからアクセスする手順は &lt;a href=&#34;http://blog.algolab.jp/post/2016/08/22/ubuntu-tightvnc-server/#アクセス&#34;&gt;こちら&lt;/a&gt; をご参照ください。&lt;/p&gt;

&lt;h3 id=&#34;vlcのインストール&#34;&gt;VLCのインストール&lt;/h3&gt;

&lt;p&gt;VLC media playerをインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# install
$ sudo apt-get install vlc-nox vlc-data
# add env vars
$ echo &amp;quot;export LD_LIBRARY_PATH=/usr/lib/vlc:$LD_LIBRARY_PATH&amp;quot; &amp;gt;&amp;gt; ~/.bashrc
$ echo &amp;quot;export VLC_PLUGIN_PATH=/usr/lib/vlc/plugins&amp;quot; &amp;gt;&amp;gt; ~/.bashrc
$ soure ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;nodeとnpmのインストール&#34;&gt;NodeとNPMのインストール&lt;/h3&gt;

&lt;p&gt;後に出てくるサーバーの起動に必要なNodeとNPMをインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# apt-get update &amp;amp; upgrade. It takes about 15 min.
$ sudo apt-get update &amp;amp;&amp;amp; sudo apt-get upgrade
# install nodejs
$ curl -sL https://deb.nodesource.com/setup | sudo bash -
$ sudo apt-get install nodejs
$ cd /home/pi/Desktop/alexa-avs-raspberry-pi-master/samples/companionService
$ npm install
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;jdkとmavenのインストール&#34;&gt;JDKとMavenのインストール&lt;/h3&gt;

&lt;p&gt;公式DocはMavenの環境変数は &lt;code&gt;/etc/profile.d/maven.sh&lt;/code&gt; に追加する方法ですが、うまくいかなかったので手っ取り早く &lt;code&gt;bashrc&lt;/code&gt; に追加して進めました。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# java
$ cd /home/pi/Desktop/alexa-avs-raspberry-pi-master/samples/javaclient
$ ./install-java8.sh
# maven
$ wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz
$ sudo tar xvf apache-maven-3.3.9-bin.tar.gz  -C /opt
# add maven_vars
$ echo &amp;quot;export M2_HOME=/opt/apache-maven-3.3.9&amp;quot; &amp;gt;&amp;gt; ~/.bashrc
$ echo &amp;quot;export PATH=$PATH:$M2_HOME/bin&amp;quot; &amp;gt;&amp;gt; ~/.bashrc
$ source ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;証明書生成スクリプトを実行&#34;&gt;証明書生成スクリプトを実行&lt;/h3&gt;

&lt;p&gt;プロダクトID、シリアル番号、パスワードの3つを入力します。今回はパスワードは空のままで進めます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ /home/pi/Desktop/alexa-avs-raspberry-pi-master/samples/javaclient/generate.sh
&amp;gt; product ID: my_device
&amp;gt; Serial Number: 123456
&amp;gt; Password: [blank]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;クライアントidとclientsecretを発行&#34;&gt;クライアントIDとClientSecretを発行&lt;/h3&gt;

&lt;p&gt;ここは &lt;a href=&#34;https://github.com/amzn/alexa-avs-raspberry-pi#user-content-6---getting-started-with-alexa-voice-service&#34;&gt;公式Doc&lt;/a&gt; の画像のとおり進めればよいです。&lt;/p&gt;

&lt;h3 id=&#34;サーバとクライアントを起動&#34;&gt;サーバとクライアントを起動&lt;/h3&gt;

&lt;p&gt;下記のとおりサーバを起動します。 &lt;code&gt;config.js&lt;/code&gt; には先ほど発行したクライアントIDとClientSecretを入力しておきます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# setup clientId and ClientSecret
$ vi /home/pi/Desktop/alexa-avs-raspberry-pi-master/samples/companionService/config.js
$ cd /home/pi/Desktop/alexa-avs-raspberry-pi-master/samples/companionService
$ npm start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;続いてクライアントも起動します。起動するとGUIも一緒に立ち上がります。 &lt;code&gt;DISPLAY=:1.0&lt;/code&gt; はVNC経由の場合の指定です。外部ディスプレイを使う場合は &lt;code&gt;DISPLAY=:0.0&lt;/code&gt; です。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ cd /home/pi/Desktop/alexa-avs-raspberry-pi-master/samples/javaclient
$ mvn install
$ export DISPLAY=:1.0
$ mvn exec:exec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;GUIに出てくるURLにアクセスしてデバイスの登録になります。ここも &lt;a href=&#34;https://github.com/amzn/alexa-avs-raspberry-pi#user-content-10---obtain-authorsization-from-login-with-amazon&#34;&gt;公式Doc&lt;/a&gt; の画像のとおりです。以上が終わると、AVSを利用できます。&lt;/p&gt;

&lt;h2 id=&#34;次回予告&#34;&gt;次回予告&lt;/h2&gt;

&lt;p&gt;次回はAlexa Skillsを登録して使ってみようと思います。乞うご期待。Don&amp;rsquo;t miss out!&lt;br /&gt;
(2016/08/25 公開しました &amp;rarr; &lt;a href=&#34;http://blog.algolab.jp/post/2016/08/29/alexa-skills-kit/&#34;&gt;Alexa Skills KitをAWS Lamdaから使う&lt;/a&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>音声は新しいパラダイムシフトになる 〜2016年度版メアリー・ミーカー氏レポートまとめ〜</title>
      <link>http://blog.algolab.jp/post/2016/07/29/mary-meeker-2016/</link>
      <pubDate>Fri, 29 Jul 2016 11:47:41 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/07/29/mary-meeker-2016/</guid>
      <description>

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/D0N5V1PjTsIasR&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;p&gt;&lt;strong&gt;「音声インターフェースは新しいパラダイムシフトになる」&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;伝説のアナリスト、メアリー・ミーカー氏は、 &lt;a href=&#34;http://www.kpcb.com/internet-trends&#34;&gt;インターネット・トレンド&lt;/a&gt; 2016年度版の中で述べています。&lt;br /&gt;
ここでは、レポートの中から、音声に関するものをまとめていきます。&lt;/p&gt;

&lt;h2 id=&#34;インターフェースの技術革新は10年毎に起きる&#34;&gt;インターフェースの技術革新は10年毎に起きる&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//114.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;ヒューマンインターフェースの歴史を振り返ってみると、ここ半世紀においては10年単位で技術革新が起きていることが分かります。&lt;br /&gt;
iPhoneによる、タッチ + カメラインターフェースが登場したのが 2007年。&lt;br /&gt;
次の10年では、SiriやAmazon Echoに代表される音声インターフェースが技術革新を起こすだろう、と予測しています。&lt;/p&gt;

&lt;h2 id=&#34;音声は最も効率の良い入力方法である&#34;&gt;音声は最も効率の良い入力方法である&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//116.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;なぜ音声か、という問いに対して、メリットと独自性の観点から理由を述べています。&lt;br /&gt;
何より、「早い」「簡単」というのが音声インターフェースのメリットでしょう。&lt;br /&gt;
また、煩雑なGUIを必要とせず、低コストで場所をとらないことから、IoTとも相性が良い、としています。&lt;/p&gt;

&lt;h2 id=&#34;音声認識は人間並みに進歩&#34;&gt;音声認識は人間並みに進歩&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//118.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;Googleの研究成果によると、語彙数、認識精度ともに年々向上しています。&lt;/p&gt;

&lt;h2 id=&#34;特に-認識精度はここ数年で急激に向上&#34;&gt;特に、認識精度はここ数年で急激に向上&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//119.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;数年前までは良くて80%程度だったものが、最近は90%を優に超えてきているのが分かります。&lt;br /&gt;
人工知能研究の権威である、BaiduのAndrew Ng氏は、精度が 99% を超えるとゲームチェンジャーになる (= 世界が変わる) と述べています。&lt;br /&gt;
技術進歩の鍵となるのはディープラーニングで、音声認識分野においては、Baiduが一歩リードしている印象です。&lt;br /&gt;
Baidu の論文については、下記の記事内でも取り上げていますのでご参照ください。&lt;br /&gt;
&lt;a href=&#34;http://blog.algolab.jp/post/2016/07/21/icml-2016-reading/#deep-speech-2-end-to-end-speech-recognition-in-english-and-mandarin&#34;&gt;ICML2016読み会 まとめ&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;音声アシスタントの利用は技術の進歩が牽引&#34;&gt;音声アシスタントの利用は技術の進歩が牽引&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//121.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;入力インターフェースがキーボードから音声に置き換わるのはまだ少し早いと前置きをしながら、利用状況についてまとめています。&lt;br /&gt;
音声アシスタントの利用者は2015年時点で65%で、使い始めるきっかけとしては、ソフトウェア技術の進歩の理由が一番とのことです。&lt;/p&gt;

&lt;h2 id=&#34;音声検索の利用は開始時点の35倍に&#34;&gt;音声検索の利用は開始時点の35倍に&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//122.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;iPhoneおよびGoogleが音声検索を開始したのが2008年ですが、その時に比べ、利用回数は右肩あがりに伸びています。&lt;/p&gt;

&lt;h2 id=&#34;タイピングが難しい中国語ではさらに伸長&#34;&gt;タイピングが難しい中国語ではさらに伸長&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//123.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;Baiduの利用状況を見ると、音声入力、音声読み上げともに伸びています。&lt;br /&gt;
スマートフォンにおける言語のタイピングのしやすさ、も音声入力への利用へ影響を与えそうです。&lt;/p&gt;

&lt;h2 id=&#34;1日に6-8回音声検索する&#34;&gt;1日に6-8回音声検索する&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//124.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;&lt;a href=&#34;http://www.soundhound.com/hound&#34;&gt;Hound&lt;/a&gt; (音声アシスタントアプリ) のデータによると、1日で6-8回音声検索を行うようです。&lt;br /&gt;
カテゴリとしては、「一般情報」「エンターテイメント」「地域情報」「アシスタント」の4つにまたがる、とのことです。&lt;/p&gt;

&lt;h2 id=&#34;2020年には音声検索が50-を超える&#34;&gt;2020年には音声検索が50%を超える&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//125.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;音声検索の利用について、過去、現在、未来をまとめています。&lt;br /&gt;
Adrew Ng氏は、2020年には検索の半分以上が音声か画像になる、と予測しています。&lt;/p&gt;

&lt;h2 id=&#34;ハンズフリー-画面フリー&#34;&gt;ハンズフリー &amp;amp; 画面フリー&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//127.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;音声インターフェースを使う理由のトップが、「手 (もしくは画面) がふさがっている時に便利だから」で、&lt;br /&gt;
利用シチュエーションとしては、「家」「車」「移動中」が大部分を占めています。&lt;/p&gt;

&lt;h2 id=&#34;プラットフォームは構築され-サードパーティの動きも速い&#34;&gt;プラットフォームは構築され、サードパーティの動きも速い&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//129.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;前のスライドで述べた「家」「車」「移動中」において、Amazon Alexaは様々なOEMを提供しています。&lt;br /&gt;
また、Alexaを拡張できるAlexa Skills Kitの開発も盛んになっています。&lt;/p&gt;

&lt;h2 id=&#34;ショッピングも迅速に&#34;&gt;ショッピングも迅速に&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//130.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;Amazonは、ショッピングをモバイルアプリから音声入力へ置き換えることを目指しています。&lt;/p&gt;

&lt;h2 id=&#34;amazon-echoの所有率は5&#34;&gt;Amazon Echoの所有率は5%&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//131.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;CIRPによると、AmazonEchoの所有者は5%で、認知度は61%とのことです。&lt;/p&gt;

&lt;h2 id=&#34;2016年は産業の変わり目となる&#34;&gt;2016年は産業の変わり目となる&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//133.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;コンピュータ産業の変わり目は後から振り返ると明確なものであると前置きした上で、iPhoneの売上が2015年にピークを迎えたことを分岐点と捉え、今後はAmazon Echoの売上が急激に伸びるのではないか、と締めくくっています。&lt;/p&gt;

&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;

&lt;p&gt;まとめは以上となります。&lt;br /&gt;
未来を見据えたときに、やはり「音声」は外せないキーワードとなってくるのではないでしょうか。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>