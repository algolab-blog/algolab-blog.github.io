<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Speech Recognition on ALGO GEEKS</title>
    <link>http://blog.algolab.jp/tags/speech-recognition/</link>
    <description>Recent content in Speech Recognition on ALGO GEEKS</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Wed, 31 Aug 2016 14:39:10 +0900</lastBuildDate>
    <atom:link href="http://blog.algolab.jp/tags/speech-recognition/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Kaldiで音声を学習させる 〜ディープラーニングを用いた音声認識ツールキット〜</title>
      <link>http://blog.algolab.jp/post/2016/08/31/kaldi/</link>
      <pubDate>Wed, 31 Aug 2016 14:39:10 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/08/31/kaldi/</guid>
      <description>

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/31/kaldi//kaldi_text_and_logo.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;h2 id=&#34;kaldiとは&#34;&gt;Kaldiとは&lt;/h2&gt;

&lt;p&gt;C++で書かれた音声認識ツールキットで、Apache Licence 2.0で公開されています。&lt;br /&gt;
音響モデルにDNN (Deep Neural Network) を用いているのが特長です。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://kaldi-asr.org/&#34;&gt;http://kaldi-asr.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;今回はKaldiを動作させ、yesかnoの音声を判別するモデルを学習させてみます。&lt;/p&gt;

&lt;h2 id=&#34;環境&#34;&gt;環境&lt;/h2&gt;

&lt;p&gt;Vagrant上のUbuntu 16.04 LTSを用いています。
&lt;pre&gt;&lt;code class=&#34;language-sh hljs bash&#34;&gt;$ cat /etc/lsb-release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=&lt;span class=&#34;hljs-number&#34;&gt;16.04&lt;/span&gt;
DISTRIB_CODENAME=xenial
DISTRIB_DESCRIPTION=&lt;span class=&#34;hljs-string&#34;&gt;&#34;Ubuntu 16.04.1 LTS&#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh hljs bash&#34;&gt;$ uname &lt;span class=&#34;hljs-operator&#34;&gt;-a&lt;/span&gt;
Linux vagrant &lt;span class=&#34;hljs-number&#34;&gt;4.4&lt;/span&gt;.&lt;span class=&#34;hljs-number&#34;&gt;0&lt;/span&gt;-&lt;span class=&#34;hljs-number&#34;&gt;31&lt;/span&gt;-generic &lt;span class=&#34;hljs-comment&#34;&gt;#50-Ubuntu SMP Wed Jul 13 00:07:12 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/p&gt;

&lt;h2 id=&#34;kaldiのダウンロード&#34;&gt;Kaldiのダウンロード&lt;/h2&gt;

&lt;p&gt;Githubよりダウンロードします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ git clone https://github.com/kaldi-asr/kaldi.git
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;インストール&#34;&gt;インストール&lt;/h2&gt;

&lt;p&gt;インストール方法は&lt;code&gt;INSTALL&lt;/code&gt;ファイルに最新情報が記載されているので、それに従います。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ cd kaldi
$ cat INSTALL
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;This is the official Kaldi INSTALL. Look also at INSTALL.md for the git mirror installation.&lt;br /&gt;
[for native Windows install, see windows/INSTALL]&lt;/p&gt;

&lt;p&gt;(1)&lt;br /&gt;
go to tools/  and follow INSTALL instructions there.&lt;/p&gt;

&lt;p&gt;(2)&lt;br /&gt;
go to src/ and follow INSTALL instructions there.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code&gt;tools&lt;/code&gt;および&lt;code&gt;src&lt;/code&gt;フォルダの&lt;code&gt;INSTALL&lt;/code&gt;を見れば良いようなので、まず&lt;code&gt;tools&lt;/code&gt;から確認していきます。&lt;/p&gt;

&lt;h2 id=&#34;toolsのインストール&#34;&gt;toolsのインストール&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ cd tools
$ cat INSTALL
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;To install the most important prerequisites for Kaldi:&lt;/p&gt;

&lt;p&gt;&amp;nbsp;first do&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&amp;nbsp;extras/check_dependencies.sh&lt;/p&gt;

&lt;p&gt;to see if there are any system-level installations or modifications you need to do.&lt;br /&gt;
Check the output carefully: there are some things that will make your life a lot&lt;br /&gt;
easier if you fix them at this stage.&lt;/p&gt;

&lt;p&gt;Then run&lt;/p&gt;

&lt;p&gt;&amp;nbsp;make&lt;/p&gt;

&lt;p&gt;If you have multiple CPUs and want to speed things up, you can do a parallel&lt;br /&gt;
build by supplying the &amp;ldquo;-j&amp;rdquo; option to make, e.g. to use 4 CPUs:&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&amp;nbsp;make -j 4&lt;/p&gt;

&lt;p&gt;By default, Kaldi builds against OpenFst-1.3.4. If you want to build against&lt;br /&gt;
OpenFst-1.4, edit the Makefile in this folder. Note that this change requires&lt;br /&gt;
a relatively new compiler with C++11 support, e.g. gcc &amp;gt;= 4.6, clang &amp;gt;= 3.0.&lt;/p&gt;

&lt;p&gt;In extras/, there are also various scripts to install extra bits and pieces that&lt;br /&gt;
are used by individual example scripts.  If an example script needs you to run&lt;br /&gt;
one of those scripts, it will tell you what to do.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;概要は以下の通りです。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;extras/check_dependencies.sh&lt;/code&gt;で依存関係をチェックする&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make&lt;/code&gt;コマンドでインストールを行う

&lt;ul&gt;
&lt;li&gt;マルチコアのCPUの場合は&lt;code&gt;j&lt;/code&gt;オプションをつけることでインストールが並列化できる (早くなる)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;依存関係のチェック&#34;&gt;依存関係のチェック&lt;/h3&gt;

&lt;p&gt;スクリプトを用いて依存関係をチェックします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ extras/check_dependencies.sh

extras/check_dependencies.sh: automake is not installed.
extras/check_dependencies.sh: autoconf is not installed.
extras/check_dependencies.sh: neither libtoolize nor glibtoolize is installed
extras/check_dependencies.sh: subversion is not installed
extras/check_dependencies.sh: default or create an bash alias for kaldi scripts to run correctly
extras/check_dependencies.sh: we recommend that you run (our best guess):
 sudo apt-get install  automake autoconf libtool subversion
You should probably do:
 sudo apt-get install libatlas3-base
/bin/sh is linked to dash, and currently some of the scripts will not run
properly.  We recommend to run:
 sudo ln -s -f bash /bin/sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;サジェストされた通りに進めます。&lt;br /&gt;
(環境によって出てくるメッセージが異なるのでご注意下さい)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ sudo apt-get install automake autoconf libtool subversion
$ sudo apt-get install -y libatlas3-base
$ sudo ln -s -f bash /bin/sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再度依存関係をチェックすると、OKとなりました。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ extras/check_dependencies.sh
extras/check_dependencies.sh: all OK.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;インストール-1&#34;&gt;インストール&lt;/h3&gt;

&lt;p&gt;まず、手元の環境のCPUコア数を調べます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ nproc
4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;筆者の環境は4コアだったので、&lt;code&gt;j&lt;/code&gt;オプションを用いて並列インストールを行います。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ sudo make -j 4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下のライブラリがインストールされます。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;OpenFst

&lt;ul&gt;
&lt;li&gt;重み付き有限状態トランスデューサー (WFST) を扱うライブラリ&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;sph2pipe

&lt;ul&gt;
&lt;li&gt;SPHEREファイルのコンバータ&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;sclite

&lt;ul&gt;
&lt;li&gt;音声認識結果をスコアリングするためのライブラリ&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;ATLAS

&lt;ul&gt;
&lt;li&gt;線形代数ライブラリ&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;CLAPACK

&lt;ul&gt;
&lt;li&gt;線形代数ライブラリ&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;オプション-言語モデルツールキットのインストール&#34;&gt;(オプション) 言語モデルツールキットのインストール&lt;/h3&gt;

&lt;p&gt;また、言語モデルのツールキット (IRSTLM や SRILM) を使用する場合は追加でインストールします。&lt;/p&gt;

&lt;h4 id=&#34;irstlm&#34;&gt;IRSTLM&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ extras/install_irstlm.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;srlm&#34;&gt;SRLM&lt;/h4&gt;

&lt;p&gt;下記からファイルをダウンロードし、&lt;code&gt;srilm.tgz&lt;/code&gt;というファイル名にした上で、&lt;code&gt;tools/&lt;/code&gt;直下に配置します。&lt;br /&gt;
&lt;a href=&#34;http://www.speech.sri.com/projects/srilm/download.html&#34;&gt;http://www.speech.sri.com/projects/srilm/download.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;また、インストールにはGNU awkが必要なので導入します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ sudo apt-get install -y gawk
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;本体をインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ extras/install_srilm.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;srcのインストール&#34;&gt;srcのインストール&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ cd ../src
$ cat INSTALL
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;These instructions are valid for UNIX-like systems (these steps have&lt;br /&gt;
been run on various Linux distributions; Darwin; Cygwin).  For native Windows&lt;br /&gt;
compilation, see ../windows/INSTALL.&lt;/p&gt;

&lt;p&gt;You must first have completed the installation steps in ../tools/INSTALL&lt;br /&gt;
(compiling OpenFst; getting ATLAS and CLAPACK headers).&lt;/p&gt;

&lt;p&gt;The installation instructions are:&lt;br /&gt;
./configure&lt;br /&gt;
make depend&lt;br /&gt;
make&lt;/p&gt;

&lt;p&gt;Note that &amp;ldquo;make&amp;rdquo; takes a long time; you can speed it up by running make&lt;br /&gt;
in parallel if you have multiple CPUs, for instance&lt;br /&gt;
 make depend -j 8&lt;br /&gt;
 make -j 8&lt;br /&gt;
For more information, see documentation at &lt;a href=&#34;http://kaldi-asr.org/doc/&#34;&gt;http://kaldi-asr.org/doc/&lt;/a&gt;&lt;br /&gt;
and click on &amp;ldquo;The build process (how Kaldi is compiled)&amp;rdquo;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;以下の3つのコマンドを叩けば良いようなので、一つずつ叩いていきます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ ./configure
$ sudo make depend -j 4
$ sudo make -j 4
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;サンプルの動作確認&#34;&gt;サンプルの動作確認&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;egs&lt;/code&gt;以下にサンプルが公開されています。&lt;br /&gt;
ここでは、&lt;code&gt;yes&lt;/code&gt;と&lt;code&gt;no&lt;/code&gt;を判別する非常に小さなタスクを学習させてみます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd ../egs/yesno
cat README.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;The &amp;ldquo;yesno&amp;rdquo; corpus is a very small dataset of recordings of one individual&lt;br /&gt;
saying yes or no multiple times per recording, in Hebrew.  It is available from&lt;br /&gt;
&lt;a href=&#34;http://www.openslr.org/1&#34;&gt;http://www.openslr.org/1&lt;/a&gt;.&lt;br /&gt;
It is mainly included here as an easy way to test out the Kaldi scripts.&lt;/p&gt;

&lt;p&gt;The test set is perfectly recognized at the monophone stage, so the dataset is&lt;br /&gt;
not exactly challenging.&lt;/p&gt;

&lt;p&gt;The scripts are in s5/.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ヘブライ語で&lt;code&gt;yes&lt;/code&gt;と&lt;code&gt;no&lt;/code&gt;を喋っているコーパスを学習データとして用いるようです。&lt;br /&gt;
&lt;code&gt;s5&lt;/code&gt;フォルダに動作用のスクリプトがあるので、動かしてみます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ cd s5
$ sh run.sh
...
%WER 0.00 [ 0 / 232, 0 ins, 0 del, 0 sub ] exp/mono0a/decode_test_yesno/wer_10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;WER (単語誤り率) が 0% という結果となりました。&lt;/p&gt;

&lt;h2 id=&#34;次回予告&#34;&gt;次回予告&lt;/h2&gt;

&lt;p&gt;次回はサンプルのソースコードを追ってみたいと思います。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Alexa Skills KitをAWS Lamdaから使う</title>
      <link>http://blog.algolab.jp/post/2016/08/29/alexa-skills-kit/</link>
      <pubDate>Mon, 29 Aug 2016 16:10:04 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/08/29/alexa-skills-kit/</guid>
      <description>

&lt;p&gt;こちらの記事の続きとなります。&lt;br /&gt;
&lt;a href=&#34;http://blog.algolab.jp/post/2016/08/11/raspberry-pi-alexa/&#34;&gt;Amazon Echoを6,000円で自作する 〜Raspberry Pi 3 + Alexa Voice Services (AVS)〜&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;前回はRaspberry PiからAVS (Alexa Voice Services) を使ってみましたが、今回は、Alexa Skills Kitを使ってみたいと思います。&lt;/p&gt;

&lt;h2 id=&#34;完成したもの&#34;&gt;完成したもの&lt;/h2&gt;

&lt;p&gt;Alexaが自分の好みの色を覚えてくれるようになりました。

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/HAOPIuFDdik&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34; allowfullscreen frameborder=&#34;0&#34;&gt;&lt;/iframe&gt;
 &lt;/div&gt;
&lt;/p&gt;

&lt;h2 id=&#34;alexa-skill-kitとは&#34;&gt;Alexa Skill Kitとは&lt;/h2&gt;

&lt;p&gt;AVSには好みの機能を追加できるSkillという機能があり、「カスタムスキル」と「スマートホームスキル」の2種類を登録することができます。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/understanding-the-different-types-of-skills&#34;&gt;https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/understanding-the-different-types-of-skills&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;カスタムスキル&#34;&gt;カスタムスキル&lt;/h3&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//custom-skill.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;ul&gt;
&lt;li&gt;ピザを注文したり、タクシーを呼んだり色々なことができる&lt;/li&gt;
&lt;li&gt;Invocation Name (スキルの呼び名) で呼び出す&lt;/li&gt;
&lt;li&gt;リクエストは「intent」としてマッピングされる

&lt;ul&gt;
&lt;li&gt;ピザの注文 &amp;rarr; OrderPizza intent&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;スマートホームスキル&#34;&gt;スマートホームスキル&lt;/h3&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//smart-home-skill.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;ul&gt;
&lt;li&gt;Smart home device (灯りやエアコンなど) を操作できる&lt;/li&gt;
&lt;li&gt;Invocation Nameで呼び出すのは不要&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;今回作るもの&#34;&gt;今回作るもの&lt;/h2&gt;

&lt;p&gt;公式の &lt;a href=&#34;https://developer.amazon.com/appsandservices/solutions/alexa/alexa-skills-kit/docs/developing-an-alexa-skill-as-a-lambda-function&#34;&gt;ドキュメント&lt;/a&gt; と &lt;a href=&#34;https://developer.amazon.com/public/community/post/TxDJWS16KUPVKO/New-Alexa-Skills-Kit-Template-Build-a-Trivia-Skill-in-under-an-Hour&#34;&gt;ポスト&lt;/a&gt; を参考に、今回は「Color Expert」のSkillを使ってみます。&lt;br /&gt;
Alexa SkillsはLambdaファンクション上で実行されるので、AWS LambdaとAlexa Skillsの設定が必要になります。&lt;/p&gt;

&lt;h2 id=&#34;aws-lambdaの作成&#34;&gt;AWS Lambdaの作成&lt;/h2&gt;

&lt;p&gt;AWSマネジメントコンソールにログインし、&lt;a href=&#34;https://console.aws.amazon.com/lambda/home&#34;&gt;Lambda&lt;/a&gt; のページを開きます。&lt;/p&gt;

&lt;p&gt;リージョンがバージニア北部(US East (N. Virginia))になっていることを確認し、なっていなければ変更します。Lambdaファンクションを利用してAlexa Skillsを使うのに、現在他のリージョンはサポートされていません。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-1.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;&lt;code&gt;Create a Lambda function&lt;/code&gt;をクリックするとBlueprint一覧画面になります。ここから&lt;code&gt;alexa-skills-kit-color-expert&lt;/code&gt;を選択します。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-2.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;Lambdaファンクションを呼び出すトリガーの選択画面になるので、灰色の点線のボックスをクリックし、&lt;code&gt;Alexa Skills Kit&lt;/code&gt;を選び&lt;code&gt;Next&lt;/code&gt;をクリックします。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-3.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;Lambdaファンクションの構成画面になります。Nameには「colorExpertTest」などと入力します。&lt;/p&gt;

&lt;p&gt;RoleにはLambdaを使うのが初めてであれば、&lt;code&gt;Create new role from template(s)&lt;/code&gt;から新しくRoleを作成し、Role Nameには「lambda_basic_execution」などと入力します。&lt;/p&gt;

&lt;p&gt;Policy templatesには&lt;code&gt;AMI read-only permissions&lt;/code&gt;などを選択すればOKです。&lt;/p&gt;

&lt;p&gt;Lambda function codeなど他の項目はデフォルトのままでも問題ありません。&lt;/p&gt;

&lt;p&gt;一通り入力・変更が終わったら&lt;code&gt;Next&lt;/code&gt;をクリックします。&lt;/p&gt;

&lt;p&gt;そうすると、下記のような確認画面になります。問題なければ&lt;code&gt;Create function&lt;/code&gt;をクリックします。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-4.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;トリガーのテスト画面になります。
&lt;code&gt;Test&lt;/code&gt;をクリック &amp;rarr; &lt;code&gt;Alexa Start Session&lt;/code&gt;を選択 &amp;rarr; &lt;code&gt;save and test&lt;/code&gt;をクリックと進むとTestが走ります。
実行結果がSuceededとなること、ログ出力に先ほどのLambda function codeの出力結果が表示されていればOKです。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-5.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;これで作成は完了です。最後にLambdaファンクションの呼び出し先となるARNをメモしておきます。上記スクリーンショットで右上の一部灰色でマスクしている文字列です。&lt;/p&gt;

&lt;h2 id=&#34;alexa-skillの作成&#34;&gt;Alexa Skillの作成&lt;/h2&gt;

&lt;p&gt;Raspberry Piが登録されているアカウントでAmazon Developer Consoleにログインし、&lt;a href=&#34;https://developer.amazon.com/edw/home.html&#34;&gt;Alexa&lt;/a&gt; のページに進みます。&lt;/p&gt;

&lt;p&gt;Alexa Skills Kitの&lt;code&gt;Get Started&lt;/code&gt;をクリックします。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-6.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;&lt;code&gt;Add a New Skill&lt;/code&gt;から新規にSkillを登録します。実際に話しかけて呼び出すときの名前となるInvocation Nameには「color expert」と入力して、&lt;code&gt;Next&lt;/code&gt;をクリックします。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-7.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;Interaction Modelの定義画面になります。これがAlexaに話しかけてやり取りをする内容になります。今回は&lt;a href=&#34;https://developer.amazon.com/appsandservices/solutions/alexa/alexa-skills-kit/docs/developing-an-alexa-skill-as-a-lambda-function&#34;&gt;公式ドキュメント&lt;/a&gt;のとおりにIntent Schame, Custom Slot Types, Sample Utterancesを下記のようにします。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-8.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;Intent_Schema&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  &amp;quot;intents&amp;quot;: [
    {
      &amp;quot;intent&amp;quot;: &amp;quot;MyColorIsIntent&amp;quot;,
      &amp;quot;slots&amp;quot;: [
        {
          &amp;quot;name&amp;quot;: &amp;quot;Color&amp;quot;,
          &amp;quot;type&amp;quot;: &amp;quot;LIST_OF_COLORS&amp;quot;
        }
      ]
    },
    {
      &amp;quot;intent&amp;quot;: &amp;quot;WhatsMyColorIntent&amp;quot;
    },
    {
      &amp;quot;intent&amp;quot;: &amp;quot;AMAZON.HelpIntent&amp;quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Custom_Slot_Type&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;LIST_OF_COLORS
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Custom_Slot_Type_Values&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;green
red
blue
orange
gold
silver
yellow
black
white
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sample_Utterances&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;WhatsMyColorIntent what&#39;s my favorite color
WhatsMyColorIntent what is my favorite color
WhatsMyColorIntent what&#39;s my color
WhatsMyColorIntent what is my color
WhatsMyColorIntent my color
WhatsMyColorIntent my favorite color
WhatsMyColorIntent get my color
WhatsMyColorIntent get my favorite color
WhatsMyColorIntent give me my favorite color
WhatsMyColorIntent give me my color
WhatsMyColorIntent what my color is
WhatsMyColorIntent what my favorite color is
WhatsMyColorIntent yes
WhatsMyColorIntent yup
WhatsMyColorIntent sure
WhatsMyColorIntent yes please
MyColorIsIntent my favorite color is {Color}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;次にEndpointなどの設定画面になります。先ほどメモしておいたARNを入力します。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-9.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;次にTest画面になります。Enter Utteranceに先ほどSample Utteranceに定義した文章を入力して&lt;code&gt;Ask color expert&lt;/code&gt;をクリックします。するとLambdaで処理が実行されて返答される文章などを含んだレスポンスが返ってきます。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-10.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;残りの設定項目に Publishing infomation, Privacy &amp;amp; Compliance がありますが、これらはAlexa Skillをpubulishingするときに必要で、手元の実機での実行には必要ないので今回は割愛します。&lt;/p&gt;

&lt;h1 id=&#34;動作確認&#34;&gt;動作確認&lt;/h1&gt;

&lt;p&gt;まずAmazon Developer Consoleと同じアカウントでAmazon Alexaにログインして&lt;a href=&#34;http://alexa.amazon.com/spa/index.html#skills/your-skills&#34;&gt;Skill一覧画面&lt;/a&gt;から先ほど作成したSkillがあることを確認します。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/29/alexa-skills-kit//alexa-skills-kit-11.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;あとは下記の動画のように話しかけて動作するか確認します。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>iOSで音声認識 〜Speech Frameworkを試す〜</title>
      <link>http://blog.algolab.jp/post/2016/08/16/speech-framework/</link>
      <pubDate>Tue, 16 Aug 2016 19:34:57 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/08/16/speech-framework/</guid>
      <description>

&lt;p&gt;2016/08/16現在、まだβ版という位置づけですが、Apple謹製の音声認識エンジン (Speech Framework) が公開されています。今回は、下記のサンプルコードを動作させてみます。
&lt;a href=&#34;https://developer.apple.com/library/prerelease/content/samplecode/SpeakToMe/Introduction/Intro.html&#34;&gt;https://developer.apple.com/library/prerelease/content/samplecode/SpeakToMe/Introduction/Intro.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;動作を確認するにはXcode 8.0以降、iOS 10.0 以降が必要なので、環境を整えるところから始めます。&lt;/p&gt;

&lt;h2 id=&#34;apple-developer-program-へ登録&#34;&gt;Apple Developer Program へ登録&lt;/h2&gt;

&lt;p&gt;諸々インストールするためには、Developer登録が必須なので、以下より登録を行います。
&lt;a href=&#34;https://developer.apple.com/programs/jp/&#34;&gt;https://developer.apple.com/programs/jp/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;xcode-8-betaをmacにインストール&#34;&gt;Xcode 8 betaをMacにインストール&lt;/h2&gt;

&lt;p&gt;Macから下記ページへアクセスし、Xcode 8 betaをダウンロード、インストールします。&lt;br /&gt;
&lt;a href=&#34;https://developer.apple.com/download/&#34;&gt;https://developer.apple.com/download/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;ios-10-betaをiphone-にインストール&#34;&gt;iOS 10 betaをiPhone にインストール&lt;/h2&gt;

&lt;p&gt;iPhoneか下記ページへアクセスし、iOS 10 betaをダウンロード、インストールします。&lt;br /&gt;
&lt;a href=&#34;https://developer.apple.com/download/&#34;&gt;https://developer.apple.com/download/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;サンプルコードをダウンロード&#34;&gt;サンプルコードをダウンロード&lt;/h2&gt;

&lt;p&gt;下記ページより、サンプルコードをダウンロードします。
&lt;a href=&#34;https://developer.apple.com/library/prerelease/content/samplecode/SpeakToMe/Introduction/Intro.html&#34;&gt;https://developer.apple.com/library/prerelease/content/samplecode/SpeakToMe/Introduction/Intro.html&lt;/a&gt;&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/08/16/speech-framework//speak_to_me.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;h2 id=&#34;署名の確認&#34;&gt;署名の確認&lt;/h2&gt;

&lt;p&gt;サンプルコードを開き、&lt;code&gt;TARGETS -&amp;gt; General -&amp;gt; Signing&lt;/code&gt;にDeveloper登録を行っているTeamが選択されているか確認します。
(ここが正しく設定されていないと実機でのビルドに失敗します)&lt;/p&gt;

&lt;h2 id=&#34;日本語対応&#34;&gt;日本語対応&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;ViewController.swift&lt;/code&gt;の15行目、言語指定のコードを&lt;code&gt;en-US&lt;/code&gt;から&lt;code&gt;ja-JP&lt;/code&gt;に書き換えます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: &amp;quot;ja-JP&amp;quot;))!
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;認識精度を確認&#34;&gt;認識精度を確認&lt;/h2&gt;

&lt;p&gt;実機で動作させ、認識精度を確認してみます。まず、「吾輩は猫である」を認識させてみます。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;吾輩は猫である。名前はまだ無い。どこで生れたかとんと見当がつかぬ。何でも薄暗いじめじめした所でニャーニャー泣いていた事だけは記憶している。吾輩はここで始めて人間というものを見た。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;認識結果がこちら。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;吾輩は猫である。名前はまだない。どこで生まれたかとんと見当がつかん。何でも薄暗いじめじめした所でニャンニャン泣いていた事だけは記憶している。吾輩はここで始めて人間というものを見た。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ほぼ正解と言っていい認識精度です。ここまで精度が高いとは正直驚きです。&lt;/p&gt;

&lt;h2 id=&#34;次回予告&#34;&gt;次回予告&lt;/h2&gt;

&lt;p&gt;次回はサンプルコードの中身を追ってみようと思います。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Amazon Echoを6,000円で自作する 〜Raspberry Pi 3 &#43; Alexa Voice Services (AVS)〜</title>
      <link>http://blog.algolab.jp/post/2016/08/11/raspberry-pi-alexa/</link>
      <pubDate>Thu, 11 Aug 2016 19:08:44 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/08/11/raspberry-pi-alexa/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;http://blog.algolab.jp/post/2016/07/29/mary-meeker-2016/&#34;&gt;音声は新しいパラダイムシフトになる 〜2016年度版メアリー・ミーカー氏レポートまとめ〜&lt;/a&gt; でも触れたように、次世代デバイスとしてAmazon Echoは注目するべき存在です。&lt;/p&gt;

&lt;p&gt;しかしながら、日本では技適の関係で未だ使用できません。&lt;br /&gt;
ただ、Alexa Voice Services (AVS) というものが公開されており、Amazon Echoを様々なデバイスで動作させることが可能です。&lt;/p&gt;

&lt;p&gt;今回は、Raspberry Pi 3からAVSを利用できるようにしました。&lt;br /&gt;
セットアップについては下記にある通りですが、低予算での最低限の手順をまとめてみます。
&lt;a href=&#34;https://github.com/amzn/alexa-avs-raspberry-pi&#34;&gt;https://github.com/amzn/alexa-avs-raspberry-pi&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;完成したもの&#34;&gt;完成したもの&lt;/h2&gt;

&lt;p&gt;いきなり動画ですが、こんな感じで動きます。英語で話かけると、リクエストを解釈して実行してくれたり、音声で応答してくれて面白いです。

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/fWubPL5_YaU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34; allowfullscreen frameborder=&#34;0&#34;&gt;&lt;/iframe&gt;
 &lt;/div&gt;
&lt;/p&gt;

&lt;h2 id=&#34;用意したもの&#34;&gt;用意したもの&lt;/h2&gt;

&lt;p&gt;音声入力にUSBマイクロフォンが必要なので、Raspberry Pi 3と併せて購入。他はありあわせで用意しました。&lt;br /&gt;
Raspberry Pi用のディスプレイを用意してもよいですが、今回はVNC server (Linux版リモートデスクトップ) を使います。&lt;/p&gt;

&lt;h3 id=&#34;買ったもの&#34;&gt;買ったもの&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Raspberry Pi 3 (4,800円)

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.co.jp/gp/product/B01D1FR2WE/&#34;&gt;https://www.amazon.co.jp/gp/product/B01D1FR2WE/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;USBマイクロフォン (1,600円)

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.co.jp/gp/product/B0027WPY82&#34;&gt;https://www.amazon.co.jp/gp/product/B0027WPY82&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;ありあわせ&#34;&gt;ありあわせ&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Micro SDカード

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.co.jp/dp/B00CDJNOX6/&#34;&gt;https://www.amazon.co.jp/dp/B00CDJNOX6/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Micro-USB (A-MicroB) ケーブル&lt;/li&gt;
&lt;li&gt;スピーカー&lt;/li&gt;
&lt;li&gt;LANケーブル&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;raspberry-pi-を起動する&#34;&gt;Raspberry Pi を起動する&lt;/h2&gt;

&lt;h3 id=&#34;osイメージの準備&#34;&gt;OSイメージの準備&lt;/h3&gt;

&lt;p&gt;以下の記事を参考に進めました。&lt;br /&gt;
&lt;a href=&#34;http://qiita.com/onlyindreams/items/acc70807b69b43e176bf&#34;&gt;Raspberry Pi 3にRaspbianをインストール(Mac OS X を使用)&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Rasbian Jessie は &lt;code&gt;2016-05-27&lt;/code&gt; リリースのものを用いました&lt;/li&gt;
&lt;li&gt;ddコマンドのオプションで、ブロックサイズは大文字 (&lt;code&gt;bs=1M&lt;/code&gt;) で指定しました&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;起動手順&#34;&gt;起動手順&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;MicroSD、LAN、 USBマイクロフォン、スピーカーを接続しておきます。&lt;/li&gt;
&lt;li&gt;電源用としてUSBケーブルを挿すとBIOSが起動します。今回はOSであるRaspbian Jessieも自動で起動しました。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;必要なアカウント-ライブラリの準備&#34;&gt;必要なアカウント・ライブラリの準備&lt;/h2&gt;

&lt;p&gt;AVSを利用するために必要なものを諸々準備します。&lt;/p&gt;

&lt;h3 id=&#34;amazon-developer-アカウントの登録&#34;&gt;Amazon Developer アカウントの登録&lt;/h3&gt;

&lt;p&gt;下記よりアカウントを登録します。登録済みであれば不要です。&lt;br /&gt;
&lt;a href=&#34;https://developer.amazon.com/login.html&#34;&gt;https://developer.amazon.com/login.html&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;サンプルアプリのダウンロード&#34;&gt;サンプルアプリのダウンロード&lt;/h3&gt;

&lt;p&gt;公式のGithub上にある &lt;a href=&#34;https://github.com/amzn/alexa-avs-raspberry-pi/archive/master.zip&#34;&gt;Sample app&lt;/a&gt; をダウンロード&amp;amp;解凍して下記のようにデスクトップなどのパスに保存します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;/home/pi/Desktop/alexa-avs-raspberry-pi-master/
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;vnc-serverのインストール&#34;&gt;VNC Serverのインストール&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# install
$ sudo apt-get install tightvncserver
# run
$ tightvncserver
# auto run setup
$ vi /home/pi/.config/tightvnc.desktop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;tightvnc.desktop&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[Desktop Entry]
Type=Application
Name=TightVNC
Exec=vncserver :1
StartupNotify=false
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;vncでraspberry-piへアクセス&#34;&gt;VNCでRaspberry Piへアクセス&lt;/h3&gt;

&lt;p&gt;Macからアクセスする手順は &lt;a href=&#34;http://blog.algolab.jp/post/2016/08/22/ubuntu-tightvnc-server/#アクセス&#34;&gt;こちら&lt;/a&gt; をご参照ください。&lt;/p&gt;

&lt;h3 id=&#34;vlcのインストール&#34;&gt;VLCのインストール&lt;/h3&gt;

&lt;p&gt;VLC media playerをインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# install
$ sudo apt-get install vlc-nox vlc-data
# add env vars
$ echo &amp;quot;export LD_LIBRARY_PATH=/usr/lib/vlc:$LD_LIBRARY_PATH&amp;quot; &amp;gt;&amp;gt; ~/.bashrc
$ echo &amp;quot;export VLC_PLUGIN_PATH=/usr/lib/vlc/plugins&amp;quot; &amp;gt;&amp;gt; ~/.bashrc
$ soure ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;nodeとnpmのインストール&#34;&gt;NodeとNPMのインストール&lt;/h3&gt;

&lt;p&gt;後に出てくるサーバーの起動に必要なNodeとNPMをインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# apt-get update &amp;amp; upgrade. It takes about 15 min.
$ sudo apt-get update &amp;amp;&amp;amp; sudo apt-get upgrade
# install nodejs
$ curl -sL https://deb.nodesource.com/setup | sudo bash -
$ sudo apt-get install nodejs
$ cd /home/pi/Desktop/alexa-avs-raspberry-pi-master/samples/companionService
$ npm install
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;jdkとmavenのインストール&#34;&gt;JDKとMavenのインストール&lt;/h3&gt;

&lt;p&gt;公式DocはMavenの環境変数は &lt;code&gt;/etc/profile.d/maven.sh&lt;/code&gt; に追加する方法ですが、うまくいかなかったので手っ取り早く &lt;code&gt;bashrc&lt;/code&gt; に追加して進めました。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# java
$ cd /home/pi/Desktop/alexa-avs-raspberry-pi-master/samples/javaclient
$ ./install-java8.sh
# maven
$ wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz
$ sudo tar xvf apache-maven-3.3.9-bin.tar.gz  -C /opt
# add maven_vars
$ echo &amp;quot;export M2_HOME=/opt/apache-maven-3.3.9&amp;quot; &amp;gt;&amp;gt; ~/.bashrc
$ echo &amp;quot;export PATH=$PATH:$M2_HOME/bin&amp;quot; &amp;gt;&amp;gt; ~/.bashrc
$ source ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;証明書生成スクリプトを実行&#34;&gt;証明書生成スクリプトを実行&lt;/h3&gt;

&lt;p&gt;プロダクトID、シリアル番号、パスワードの3つを入力します。今回はパスワードは空のままで進めます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ /home/pi/Desktop/alexa-avs-raspberry-pi-master/samples/javaclient/generate.sh
&amp;gt; product ID: my_device
&amp;gt; Serial Number: 123456
&amp;gt; Password: [blank]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;クライアントidとclientsecretを発行&#34;&gt;クライアントIDとClientSecretを発行&lt;/h3&gt;

&lt;p&gt;ここは &lt;a href=&#34;https://github.com/amzn/alexa-avs-raspberry-pi#user-content-6---getting-started-with-alexa-voice-service&#34;&gt;公式Doc&lt;/a&gt; の画像のとおり進めればよいです。&lt;/p&gt;

&lt;h3 id=&#34;サーバとクライアントを起動&#34;&gt;サーバとクライアントを起動&lt;/h3&gt;

&lt;p&gt;下記のとおりサーバを起動します。 &lt;code&gt;config.js&lt;/code&gt; には先ほど発行したクライアントIDとClientSecretを入力しておきます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# setup clientId and ClientSecret
$ vi /home/pi/Desktop/alexa-avs-raspberry-pi-master/samples/companionService/config.js
$ cd /home/pi/Desktop/alexa-avs-raspberry-pi-master/samples/companionService
$ npm start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;続いてクライアントも起動します。起動するとGUIも一緒に立ち上がります。 &lt;code&gt;DISPLAY=:1.0&lt;/code&gt; はVNC経由の場合の指定です。外部ディスプレイを使う場合は &lt;code&gt;DISPLAY=:0.0&lt;/code&gt; です。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ cd /home/pi/Desktop/alexa-avs-raspberry-pi-master/samples/javaclient
$ mvn install
$ export DISPLAY=:1.0
$ mvn exec:exec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;GUIに出てくるURLにアクセスしてデバイスの登録になります。ここも &lt;a href=&#34;https://github.com/amzn/alexa-avs-raspberry-pi#user-content-10---obtain-authorsization-from-login-with-amazon&#34;&gt;公式Doc&lt;/a&gt; の画像のとおりです。以上が終わると、AVSを利用できます。&lt;/p&gt;

&lt;h2 id=&#34;次回予告&#34;&gt;次回予告&lt;/h2&gt;

&lt;p&gt;次回はAlexa Skillsを登録して使ってみようと思います。乞うご期待。Don&amp;rsquo;t miss out!&lt;br /&gt;
(2016/08/25 公開しました &amp;rarr; &lt;a href=&#34;http://blog.algolab.jp/post/2016/08/29/alexa-skills-kit/&#34;&gt;Alexa Skills KitをAWS Lamdaから使う&lt;/a&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>音声は新しいパラダイムシフトになる 〜2016年度版メアリー・ミーカー氏レポートまとめ〜</title>
      <link>http://blog.algolab.jp/post/2016/07/29/mary-meeker-2016/</link>
      <pubDate>Fri, 29 Jul 2016 11:47:41 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/07/29/mary-meeker-2016/</guid>
      <description>

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/D0N5V1PjTsIasR&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;p&gt;&lt;strong&gt;「音声インターフェースは新しいパラダイムシフトになる」&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;伝説のアナリスト、メアリー・ミーカー氏は、 &lt;a href=&#34;http://www.kpcb.com/internet-trends&#34;&gt;インターネット・トレンド&lt;/a&gt; 2016年度版の中で述べています。&lt;br /&gt;
ここでは、レポートの中から、音声に関するものをまとめていきます。&lt;/p&gt;

&lt;h2 id=&#34;インターフェースの技術革新は10年毎に起きる&#34;&gt;インターフェースの技術革新は10年毎に起きる&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//114.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;ヒューマンインターフェースの歴史を振り返ってみると、ここ半世紀においては10年単位で技術革新が起きていることが分かります。&lt;br /&gt;
iPhoneによる、タッチ + カメラインターフェースが登場したのが 2007年。&lt;br /&gt;
次の10年では、SiriやAmazon Echoに代表される音声インターフェースが技術革新を起こすだろう、と予測しています。&lt;/p&gt;

&lt;h2 id=&#34;音声は最も効率の良い入力方法である&#34;&gt;音声は最も効率の良い入力方法である&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//116.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;なぜ音声か、という問いに対して、メリットと独自性の観点から理由を述べています。&lt;br /&gt;
何より、「早い」「簡単」というのが音声インターフェースのメリットでしょう。&lt;br /&gt;
また、煩雑なGUIを必要とせず、低コストで場所をとらないことから、IoTとも相性が良い、としています。&lt;/p&gt;

&lt;h2 id=&#34;音声認識は人間並みに進歩&#34;&gt;音声認識は人間並みに進歩&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//118.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;Googleの研究成果によると、語彙数、認識精度ともに年々向上しています。&lt;/p&gt;

&lt;h2 id=&#34;特に-認識精度はここ数年で急激に向上&#34;&gt;特に、認識精度はここ数年で急激に向上&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//119.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;数年前までは良くて80%程度だったものが、最近は90%を優に超えてきているのが分かります。&lt;br /&gt;
人工知能研究の権威である、BaiduのAndrew Ng氏は、精度が 99% を超えるとゲームチェンジャーになる (= 世界が変わる) と述べています。&lt;br /&gt;
技術進歩の鍵となるのはディープラーニングで、音声認識分野においては、Baiduが一歩リードしている印象です。&lt;br /&gt;
Baidu の論文については、下記の記事内でも取り上げていますのでご参照ください。&lt;br /&gt;
&lt;a href=&#34;http://blog.algolab.jp/post/2016/07/21/icml-2016-reading/#deep-speech-2-end-to-end-speech-recognition-in-english-and-mandarin&#34;&gt;ICML2016読み会 まとめ&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;音声アシスタントの利用は技術の進歩が牽引&#34;&gt;音声アシスタントの利用は技術の進歩が牽引&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//121.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;入力インターフェースがキーボードから音声に置き換わるのはまだ少し早いと前置きをしながら、利用状況についてまとめています。&lt;br /&gt;
音声アシスタントの利用者は2015年時点で65%で、使い始めるきっかけとしては、ソフトウェア技術の進歩の理由が一番とのことです。&lt;/p&gt;

&lt;h2 id=&#34;音声検索の利用は開始時点の35倍に&#34;&gt;音声検索の利用は開始時点の35倍に&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//122.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;iPhoneおよびGoogleが音声検索を開始したのが2008年ですが、その時に比べ、利用回数は右肩あがりに伸びています。&lt;/p&gt;

&lt;h2 id=&#34;タイピングが難しい中国語ではさらに伸長&#34;&gt;タイピングが難しい中国語ではさらに伸長&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//123.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;Baiduの利用状況を見ると、音声入力、音声読み上げともに伸びています。&lt;br /&gt;
スマートフォンにおける言語のタイピングのしやすさ、も音声入力への利用へ影響を与えそうです。&lt;/p&gt;

&lt;h2 id=&#34;1日に6-8回音声検索する&#34;&gt;1日に6-8回音声検索する&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//124.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;&lt;a href=&#34;http://www.soundhound.com/hound&#34;&gt;Hound&lt;/a&gt; (音声アシスタントアプリ) のデータによると、1日で6-8回音声検索を行うようです。&lt;br /&gt;
カテゴリとしては、「一般情報」「エンターテイメント」「地域情報」「アシスタント」の4つにまたがる、とのことです。&lt;/p&gt;

&lt;h2 id=&#34;2020年には音声検索が50-を超える&#34;&gt;2020年には音声検索が50%を超える&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//125.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;音声検索の利用について、過去、現在、未来をまとめています。&lt;br /&gt;
Adrew Ng氏は、2020年には検索の半分以上が音声か画像になる、と予測しています。&lt;/p&gt;

&lt;h2 id=&#34;ハンズフリー-画面フリー&#34;&gt;ハンズフリー &amp;amp; 画面フリー&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//127.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;音声インターフェースを使う理由のトップが、「手 (もしくは画面) がふさがっている時に便利だから」で、&lt;br /&gt;
利用シチュエーションとしては、「家」「車」「移動中」が大部分を占めています。&lt;/p&gt;

&lt;h2 id=&#34;プラットフォームは構築され-サードパーティの動きも速い&#34;&gt;プラットフォームは構築され、サードパーティの動きも速い&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//129.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;前のスライドで述べた「家」「車」「移動中」において、Amazon Alexaは様々なOEMを提供しています。&lt;br /&gt;
また、Alexaを拡張できるAlexa Skills Kitの開発も盛んになっています。&lt;/p&gt;

&lt;h2 id=&#34;ショッピングも迅速に&#34;&gt;ショッピングも迅速に&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//130.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;Amazonは、ショッピングをモバイルアプリから音声入力へ置き換えることを目指しています。&lt;/p&gt;

&lt;h2 id=&#34;amazon-echoの所有率は5&#34;&gt;Amazon Echoの所有率は5%&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//131.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;CIRPによると、AmazonEchoの所有者は5%で、認知度は61%とのことです。&lt;/p&gt;

&lt;h2 id=&#34;2016年は産業の変わり目となる&#34;&gt;2016年は産業の変わり目となる&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/29/mary-meeker-2016//133.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;コンピュータ産業の変わり目は後から振り返ると明確なものであると前置きした上で、iPhoneの売上が2015年にピークを迎えたことを分岐点と捉え、今後はAmazon Echoの売上が急激に伸びるのではないか、と締めくくっています。&lt;/p&gt;

&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;

&lt;p&gt;まとめは以上となります。&lt;br /&gt;
未来を見据えたときに、やはり「音声」は外せないキーワードとなってくるのではないでしょうか。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>