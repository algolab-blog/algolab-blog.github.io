<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Study Session on ALGO GEEKS</title>
    <link>http://blog.algolab.jp/tags/study-session/</link>
    <description>Recent content in Study Session on ALGO GEEKS</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Thu, 08 Sep 2016 17:58:07 +0900</lastBuildDate>
    <atom:link href="http://blog.algolab.jp/tags/study-session/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>データサイエンスLT祭り 資料まとめ</title>
      <link>http://blog.algolab.jp/post/2016/09/08/dslt/</link>
      <pubDate>Thu, 08 Sep 2016 17:58:07 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/09/08/dslt/</guid>
      <description>

&lt;p&gt;「データサイエンスLT祭り」の資料を確認できたものから随時まとめていきます。&lt;br /&gt;
&lt;a href=&#34;http://data-science-lt.connpass.com/event/35289/&#34;&gt;http://data-science-lt.connpass.com/event/35289/&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://data-science-lt.connpass.com/event/35412/&#34;&gt;http://data-science-lt.connpass.com/event/35412/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;rise-jupyter-notebookでお手軽スライド作成-hugoshortcode-1&#34;&gt;RISE: Jupyter Notebookでお手軽スライド作成 / &lt;a href=&#34;https://twitter.com/sfchaos&#34;&gt;@sfchaos&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/sfchaos/dslt20160907&#34;&gt;https://github.com/sfchaos/dslt20160907&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;low-bias-high-varianceとfilter-bubbleな私-hugoshortcode-2&#34;&gt;Low Bias High VarianceとFilter Bubbleな私 / &lt;a href=&#34;https://twitter.com/tetsuroito&#34;&gt;@tetsuroito&lt;/a&gt;
&lt;/h2&gt;

&lt;script async class=&#39;speakerdeck-embed&#39; data-id=&#39;050358760ef54eaea5de43bde2dd18c8&#39; data-ratio=&#39;1.33333333333333&#39; src=&#39;//speakerdeck.com/assets/embed.js&#39;&gt;&lt;/script&gt;

&lt;h2 id=&#34;実務家のためのデータサイエンス速習法-hugoshortcode-4&#34;&gt;実務家のためのデータサイエンス速習法 / &lt;a href=&#34;https://twitter.com/shakezo_&#34;&gt;@shakezo_&lt;/a&gt;
&lt;/h2&gt;

&lt;h2 id=&#34;面倒くさいこと考えたくないあなたへ-tpotと機械学習-hugoshortcode-5&#34;&gt;面倒くさいこと考えたくないあなたへ〜TPOTと機械学習〜 / &lt;a href=&#34;https://twitter.com/tereka114&#34;&gt;@tereka114&lt;/a&gt;
&lt;/h2&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/xKDz487fnppbKk&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;h2 id=&#34;ご注文はスパムですか-hugoshortcode-7&#34;&gt;ご注文はスパムですか / &lt;a href=&#34;https://twitter.com/k66dango&#34;&gt;@k66dango&lt;/a&gt;
&lt;/h2&gt;

&lt;h2 id=&#34;データ分析者だけどリアルタイム分析がしたい-hugoshortcode-8&#34;&gt;データ分析者だけどリアルタイム分析がしたい / &lt;a href=&#34;https://twitter.com/kos59125&#34;&gt;@kos59125&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.com/kos59125/2122?c=Tqf7Mh&#34;&gt;https://docs.com/kos59125/2122?c=Tqf7Mh&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;チャンプがやまかすを一撃する方法-hugoshortcode-9&#34;&gt;チャンプがやまかすを一撃する方法 / &lt;a href=&#34;https://twitter.com/millionsmile&#34;&gt;@millionsmile&lt;/a&gt;
&lt;/h2&gt;

&lt;h2 id=&#34;階層ベイズモデルで割安mobile-pcを探す-hugoshortcode-10&#34;&gt;階層ベイズモデルで割安mobile PCを探す / &lt;a href=&#34;https://twitter.com/berobero11&#34;&gt;@berobero11&lt;/a&gt;
&lt;/h2&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/kOMYOJ6KywOYC8&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;h2 id=&#34;サラリーマンのための計算社会科学-hugoshortcode-12&#34;&gt;サラリーマンのための計算社会科学 / &lt;a href=&#34;https://twitter.com/mtknnktm&#34;&gt;@mtknnktm&lt;/a&gt;
&lt;/h2&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/41adcDWlVcTicj&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;h2 id=&#34;aucが0-01改善したってどういうことですか-hugoshortcode-14&#34;&gt;AUCが0.01改善したってどういうことですか？ / &lt;a href=&#34;https://twitter.com/Kenmatsu4&#34;&gt;@Kenmatsu4&lt;/a&gt;
&lt;/h2&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/8zufiekGQcGspR&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;h2 id=&#34;特徴量抽出するつもりが-夜な夜なクリーチャーを生み出してしまっている話-hugoshortcode-16&#34;&gt;特徴量抽出するつもりが、夜な夜なクリーチャーを生み出してしまっている話 / &lt;a href=&#34;https://twitter.com/Ryosuke0624&#34;&gt;@Ryosuke0624&lt;/a&gt;
&lt;/h2&gt;

&lt;h2 id=&#34;初心者向けに機械学習のハンズオンセミナーをしてみてわかったこと-hugoshortcode-17&#34;&gt;初心者向けに機械学習のハンズオンセミナーをしてみてわかったこと / &lt;a href=&#34;https://twitter.com/__john__smith__&#34;&gt;@__john__smith__&lt;/a&gt;
&lt;/h2&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/ji3tlWlYjusRKp&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;h2 id=&#34;reproducebility-100倍-dockerマン-hugoshortcode-19&#34;&gt;Reproducebility 100倍 Dockerマン / &lt;a href=&#34;https://twitter.com/teramonagi&#34;&gt;@teramonagi&lt;/a&gt;
&lt;/h2&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/lopySOx7zWRxHf&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;h2 id=&#34;時と場所で選ぶ機械学習-hugoshortcode-21&#34;&gt;時と場所で選ぶ機械学習 / &lt;a href=&#34;https://twitter.com/vaaaaanquish&#34;&gt;@vaaaaanquish&lt;/a&gt;
&lt;/h2&gt;

&lt;h2 id=&#34;鋼鉄の錬金術師あらため出会い系錬金術師による-マリオａｉから分かる人生をクリアするのに大事なn個の共有-hugoshortcode-22&#34;&gt;鋼鉄の錬金術師あらため出会い系錬金術師による、マリオＡＩから分かる人生をクリアするのに大事なn個の共有 / &lt;a href=&#34;https://twitter.com/tomomoto_LV3&#34;&gt;@tomomoto_LV3&lt;/a&gt;
&lt;/h2&gt;

&lt;h2 id=&#34;1回くらいやってみよう-kaggle初挑戦-hugoshortcode-23&#34;&gt;1回くらいやってみよう: Kaggle初挑戦 / &lt;a href=&#34;https://twitter.com/siero5335&#34;&gt;@siero5335&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/E0IEcrUIu8pAJt&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;

&lt;a href=&#34;http://statchiraura.blog.fc2.com/blog-entry-31.html&#34;&gt;http://statchiraura.blog.fc2.com/blog-entry-31.html&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;俺たちの戦いはこれからだ-hugoshortcode-25&#34;&gt;俺たちの戦いはこれからだ / &lt;a href=&#34;https://twitter.com/gepuro&#34;&gt;@gepuro&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/cBPffWqv3tp1zJ&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;

&lt;a href=&#34;http://blog.gepuro.net/archives/159&#34;&gt;http://blog.gepuro.net/archives/159&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;グラフ作成-3分クッキング-hugoshortcode-27&#34;&gt;グラフ作成 3分クッキング / &lt;a href=&#34;https://twitter.com/wn_seko&#34;&gt;@wn_seko&lt;/a&gt;
&lt;/h2&gt;

&lt;h2 id=&#34;gbdtを使ったfeature-transformationの適用例-hugoshortcode-28&#34;&gt;GBDTを使ったfeature transformationの適用例 / &lt;a href=&#34;https://twitter.com/Quasi_quant2010&#34;&gt;@Quasi_quant2010&lt;/a&gt;
&lt;/h2&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/7PBZv0Enkb05Gg&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;h2 id=&#34;機械学習するな機会学習しろ-hugoshortcode-30&#34;&gt;機械学習するな機会学習しろ / &lt;a href=&#34;https://twitter.com/wonder_zone&#34;&gt;@wonder_zone&lt;/a&gt;
&lt;/h2&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/mPSXhvjQW6QQba&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;h2 id=&#34;cdtv-o2o最前線情報をお伝えします-hugoshortcode-32&#34;&gt;CDTV『O2O最前線情報をお伝えします』/ &lt;a href=&#34;https://github.com/yumebayashi&#34;&gt;@yumebayashi&lt;/a&gt;
&lt;/h2&gt;

&lt;h2 id=&#34;データサイエンティストの憂鬱-hugoshortcode-33&#34;&gt;データサイエンティストの憂鬱 / &lt;a href=&#34;https://twitter.com/shoe116&#34;&gt;@shoe116&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/4oyoc61JGEaqbl&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;

&lt;a href=&#34;http://shoe116.hatenablog.com/entry/2016/04/04/084113&#34;&gt;http://shoe116.hatenablog.com/entry/2016/04/04/084113&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;otonaの夏休み課題-hugoshortcode-35&#34;&gt;Otonaの夏休み課題 / &lt;a href=&#34;https://twitter.com/u_ribo&#34;&gt;@u_ribo&lt;/a&gt;
&lt;/h2&gt;

&lt;h2 id=&#34;rcpp-が支えるパッケージ-hugoshortcode-36&#34;&gt;{Rcpp}が支えるパッケージ / &lt;a href=&#34;https://twitter.com/yamano357&#34;&gt;@yamano357&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;&lt;script async class=&#39;speakerdeck-embed&#39; data-id=&#39;9278831c5202498d935eab1dd403845c&#39; data-ratio=&#39;1.33333333333333&#39; src=&#39;//speakerdeck.com/assets/embed.js&#39;&gt;&lt;/script&gt;
&lt;a href=&#34;http://qiita.com/yamano357/items/9846adac05b0c11f5cac&#34;&gt;http://qiita.com/yamano357/items/9846adac05b0c11f5cac&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://rpubs.com/yamano357/202054&#34;&gt;http://rpubs.com/yamano357/202054&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;最後に喋るのはこの俺だ-hugoshortcode-38&#34;&gt;最後に喋るのはこの俺だ / &lt;a href=&#34;https://twitter.com/yamakatu&#34;&gt;@yamakatu&lt;/a&gt;
&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>ICML2016読み会まとめ</title>
      <link>http://blog.algolab.jp/post/2016/07/21/icml-2016-reading/</link>
      <pubDate>Fri, 22 Jul 2016 10:14:41 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/07/21/icml-2016-reading/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;http://connpass.com/event/34960/&#34;&gt;ICML2016読み会&lt;/a&gt; の内容をまとめました。&lt;br /&gt;
ニコ生配信URL：&lt;a href=&#34;http://live.nicovideo.jp/watch/lv268597918&#34;&gt;http://live.nicovideo.jp/watch/lv268597918&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;icml概要&#34;&gt;ICML概要&lt;/h2&gt;

&lt;p&gt;林浩平さん (&lt;a href=&#34;https://twitter.com/hayasick&#34;&gt;@hayasick&lt;/a&gt;
) / 産業技術総合研究所&lt;/p&gt;

&lt;p&gt;&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/2OrjGekQEu2Bgr&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;

&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ICMLはNIPSに次ぐ機械学習の国際会議&lt;/li&gt;
&lt;li&gt;ディープラーニングと、それに伴う最適化がトレンド&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;dropout-distillation&#34;&gt;Dropout distillation&lt;/h2&gt;

&lt;p&gt;佐野正太郎さん (&lt;a href=&#34;https://twitter.com/g_votte&#34;&gt;@g_votte&lt;/a&gt;
) / リクルートコミュニケーションズ&lt;/p&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/GSKKG9nwXe83zR&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;p&gt;論文: &lt;a href=&#34;http://jmlr.org/proceedings/papers/v48/bulo16.html&#34;&gt;http://jmlr.org/proceedings/papers/v48/bulo16.html&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Dropout is a popular stochastic regularization technique for deep neural networks that works by randomly dropping (i.e. zeroing) units from the network during training. This randomization process allows to implicitly train an ensemble of exponentially many networks sharing the same parametrization, which should be averaged at test time to deliver the final prediction. A typical workaround for this intractable averaging operation consists in scaling the layers undergoing dropout randomization. This simple rule called ’standard dropout’ is efficient, but might degrade the accuracy of the prediction. In this work we introduce a novel approach, coined ’dropout distillation’, that allows us to train a predictor in a way to better approximate the intractable, but preferable, averaging process, while keeping under control its computational efficiency. We are thus able to construct models that are as efficient as standard dropout, or even more efficient, while being more accurate. Experiments on standard benchmark datasets demonstrate the validity of our method, yielding consistent improvements over conventional dropout.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;Dropoutを学習に用いた場合、その予測において、時間と精度を両立することが難しかった&lt;/li&gt;
&lt;li&gt;Distillation (蒸留法) を応用することで、短時間で精度よく予測を行うモデルを構築した&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;「Dropout Distillation」Dropout を使って学習したモデルを利用して予測をするときの話。本当は Dropout の平均を取ると精度が上がるが遅い。そこで Distillation を使ったらよかったよという話。 &lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; Takuya Akiba (@iwiwi) &lt;a href=&#34;https://twitter.com/iwiwi/status/756002188770410496&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt; 来た。間違えて歌舞伎座行ってしまった（汗&lt;br&gt;dropout distillation、普通のドロップアウトより良いという話だが、学習時間のオーダーじゃなくて学習時間が同じ、で比べないとフェアじゃない気がする。平均化SGDあたりと実は等価だったりしないのかなあ&lt;/p&gt;&amp;mdash; Nakatani Shuyo (@shuyo) &lt;a href=&#34;https://twitter.com/shuyo/status/756002470573121536&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&#34;learning-convolutional-neural-networks-for-graphs&#34;&gt;Learning Convolutional Neural Networks for Graphs&lt;/h2&gt;

&lt;p&gt;秋葉拓哉さん (&lt;a href=&#34;https://twitter.com/iwiwi&#34;&gt;@iwiwi&lt;/a&gt;
) / Preferred Networks&lt;/p&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/3htE46MNnSfNQy&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;p&gt;論文: &lt;a href=&#34;http://jmlr.org/proceedings/papers/v48/niepert16.html&#34;&gt;http://jmlr.org/proceedings/papers/v48/niepert16.html&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Numerous important problems can be framed as learning from graph data. We propose a framework for learning convolutional neural networks for arbitrary graphs. These graphs may be undirected, directed, and with both discrete and continuous node and edge attributes. Analogous to image-based convolutional networks that operate on locally connected regions of the input, we present a general approach to extracting locally connected regions from graphs. Using established benchmark data sets, we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;グラフ構造 (化学化合物など) をCNNで学習させたいが、そのまま突っ込むことは難しい&lt;/li&gt;
&lt;li&gt;WL カーネルを応用したアルゴリズムを用いて、グラフ構造をテンソルに変換した
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;グラフをCNNに突っ込む話、これはCNNになってない的結論になってたが、k方向は近傍の展開なので、ちゃんとCNNの一般化になってる。CNNじゃなくてもいいよね、というのは確かに思ったが、最初に選んだ頂点数wをグラフごとに変えたいならCNNにするしか &lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; Nakatani Shuyo (@shuyo) &lt;a href=&#34;https://twitter.com/shuyo/status/756011698117419008&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;estimating-structured-vector-autoregressive-models&#34;&gt;Estimating Structured Vector Autoregressive Models&lt;/h2&gt;

&lt;p&gt;谷本啓さん (&lt;a href=&#34;https://twitter.com/akira_dev&#34;&gt;@akira_dev&lt;/a&gt;
) / NEC&lt;/p&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/uzp8O2l8b8LyZK&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;p&gt;論文: &lt;a href=&#34;http://jmlr.org/proceedings/papers/v48/melnyk16.html&#34;&gt;http://jmlr.org/proceedings/papers/v48/melnyk16.html&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Numerous important problems can be framed as learning from graph data. We propose a framework for learning convolutional neural networks for arbitrary graphs. These graphs may be undirected, directed, and with both discrete and continuous node and edge attributes. Analogous to image-based convolutional networks that operate on locally connected regions of the input, we present a general approach to extracting locally connected regions from graphs. Using established benchmark data sets, we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;一般のノルムでの正規化を用いた VARモデルの推定の非漸近的な解析を行った&lt;/li&gt;
&lt;li&gt;収束ルートはi.i.dと同じだった&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;meta-learning-with-memory-augmented-neural-networks&#34;&gt;Meta-Learning with Memory-Augmented Neural Networks&lt;/h2&gt;

&lt;p&gt;渡辺有祐さん / SONY&lt;/p&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/tj8ML4cdtq2M7s&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;p&gt;論文: &lt;a href=&#34;http://jmlr.org/proceedings/papers/v48/santoro16.html&#34;&gt;http://jmlr.org/proceedings/papers/v48/santoro16.html&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;Neural Turing Machine を One-Shot Learning に応用し、高い精度を得た&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt; transfer learning + contextual bandit てところか．この後，multi-step 行動最適化にするんだろうな．&lt;/p&gt;&amp;mdash; Ugo-Nama (@movingsloth) &lt;a href=&#34;https://twitter.com/movingsloth/status/756028694196396032&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;「Meta-Learning with Memory-Augmented Neural Networks」Neural Turing Machine の拡張を提案。One-Shot Learning のアルゴリズムを学習させると k 近傍法よりも高い性能。 &lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; Takuya Akiba (@iwiwi) &lt;a href=&#34;https://twitter.com/iwiwi/status/756031082303152130&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&#34;pixel-recurrent-neural-networks&#34;&gt;Pixel Recurrent Neural Networks&lt;/h2&gt;

&lt;p&gt;得居誠也さん (&lt;a href=&#34;https://twitter.com/beam2d&#34;&gt;@beam2d&lt;/a&gt;
) / Preferred Networks&lt;/p&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/3bJ5TFJmX7Bk1K&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;p&gt;論文: &lt;a href=&#34;http://jmlr.org/proceedings/papers/v48/oord16.html&#34;&gt;http://jmlr.org/proceedings/papers/v48/oord16.html&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;画像生成において、VAEやGANとは異なる自己回帰 (RNNに似たもの) のアプローチを用いたところ、綺麗な画像を生成できた&lt;/li&gt;
&lt;li&gt;並列化により、高速な勾配計算も両立した&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;PixelRNNの話、これに似てるなぁ &lt;a href=&#34;https://t.co/oI8oDBUm1e&#34;&gt;https://t.co/oI8oDBUm1e&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; snneko (@snneko) &lt;a href=&#34;https://twitter.com/snneko/status/756037448879136768&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;PixelRNN、出力は各チャンネル 256値の多クラス分類なのか。すげー。思いついてもやらない系だなあ。でもそれでくっきりした画像が生成できる可能性があるなら今度試してみるかなあ。&lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; Nakatani Shuyo (@shuyo) &lt;a href=&#34;https://twitter.com/shuyo/status/756038954550304768&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;定性的ってレベルじゃない単なる個人的な印象だけど、GAN系の生成する画像はノイジーでぼやっとしがちだが、何が写っているかはわかる。PixelRNN はくっきりした画像を生成するが、よく見たら何が写っているのかさっぱりわからん。 &lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; Nakatani Shuyo (@shuyo) &lt;a href=&#34;https://twitter.com/shuyo/status/756042609060040705&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&#34;dynamic-memory-networks-for-visual-and-textual-question-answering&#34;&gt;Dynamic Memory Networks for Visual and Textual Question Answering&lt;/h2&gt;

&lt;p&gt;花木健太郎さん &lt;a href=&#34;https://twitter.com/csstudyabroad&#34;&gt;@csstudyabroad&lt;/a&gt;
 / IBM&lt;/p&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/efLeWADDGXz14D&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;p&gt;論文: &lt;a href=&#34;http://jmlr.org/proceedings/papers/v48/xiong16.html&#34;&gt;http://jmlr.org/proceedings/papers/v48/xiong16.html&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Neural network architectures with memory and attention mechanisms exhibit certain reason- ing capabilities required for question answering. One such architecture, the dynamic memory net- work (DMN), obtained high accuracy on a variety of language tasks. However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images. Based on an analysis of the DMN, we propose several improvements to its memory and input modules. Together with these changes we introduce a novel input module for images in order to be able to answer visual questions. Our new DMN+ model improves the state of the art on both the Visual Question Answering dataset and the bAbI-10k text question-answering dataset without supporting fact supervision.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;Memory Network内のMemory Moduleを改良した&lt;/li&gt;
&lt;li&gt;テキストではなく、画像を用いたQAタスクでも精度が出た&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;Memory Network，なんかピンとこないんだよなあ。これでもかってくらいヒューリスティックを積んで積んで積みまくってくるからかなあ。 &lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; Nakatani Shuyo (@shuyo) &lt;a href=&#34;https://twitter.com/shuyo/status/756047518291460096&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Memory Networks for Language Understanding, ICML Tutorial 2016 &lt;a href=&#34;https://t.co/ssPEmP1Pf9&#34;&gt;https://t.co/ssPEmP1Pf9&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; Pnz (@penzant) &lt;a href=&#34;https://twitter.com/penzant/status/756050102670598144&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Theano Implementation of Dynamic Memory Networks for Visual and Textual Question Answering &lt;a href=&#34;https://t.co/a8ijZKcnwS&#34;&gt;https://t.co/a8ijZKcnwS&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; Pnz (@penzant) &lt;a href=&#34;https://twitter.com/penzant/status/756050917686816768&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt; &lt;a href=&#34;https://t.co/EOaOfKLH40&#34;&gt;https://t.co/EOaOfKLH40&lt;/a&gt; 黒魔術はここね🎵&lt;/p&gt;&amp;mdash; prototechno (@prototechno) &lt;a href=&#34;https://twitter.com/prototechno/status/756051774436356096&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&#34;generative-adversarial-text-to-image-synthesis&#34;&gt;Generative Adversarial Text to Image Synthesis&lt;/h2&gt;

&lt;p&gt;廣芝和之さん (&lt;a href=&#34;https://twitter.com/hiho_karuta&#34;&gt;@hiho_karuta&lt;/a&gt;
) / ドワンゴ&lt;/p&gt;

&lt;iframe allowfullscreen=&#34;allowfullscreen&#34; frameborder=&#34;0&#34; scrolling=&#34;no&#34; src=&#34;http://niconare.nicovideo.jp/embed_works/kn1626&#34; style=&#34;max-width: 100%;&#34; width=&#34;485&#34; height=&#34;413&#34;&gt;&lt;/iframe&gt;


&lt;p&gt;論文: &lt;a href=&#34;http://jmlr.org/proceedings/papers/v48/reed16.html&#34;&gt;http://jmlr.org/proceedings/papers/v48/reed16.html&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories such as faces, album covers, room interiors and flowers. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image modeling, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;GANのアーキテクチャを応用し、 文章から画像を生成することができた&lt;/li&gt;
&lt;li&gt;文章だけでは表せない情報は、スタイル、という概念を用いて吸収した&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;文字情報ではない情報（スタイル）を転写（ex. 背景を継承）&lt;br&gt;画像からスタイルを求める関数を学習&lt;br&gt;スタイルZと文章をGに与える&lt;br&gt;スライド &lt;a href=&#34;https://t.co/c5lB0JIjgg&#34;&gt;https://t.co/c5lB0JIjgg&lt;/a&gt;&lt;br&gt;実装 &lt;a href=&#34;https://t.co/JpFDDp7bJo&#34;&gt;https://t.co/JpFDDp7bJo&lt;/a&gt;&lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; Ryobot | りょぼっと (@_Ryobot) &lt;a href=&#34;https://twitter.com/_Ryobot/status/756057271877050369&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;「Generative Adversarial Text to Image Synthesis」GAN を応用して文章から画像を生成。生成側に画像と文章を入れ、判定側は画像が文章と一致しているかも学習させる。 &lt;a href=&#34;https://t.co/ipQyxAsInL&#34;&gt;https://t.co/ipQyxAsInL&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt;&lt;/p&gt;&amp;mdash; Takuya Akiba (@iwiwi) &lt;a href=&#34;https://twitter.com/iwiwi/status/756058225657511936&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&#34;deep-speech-2-end-to-end-speech-recognition-in-english-and-mandarin&#34;&gt;Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin&lt;/h2&gt;

&lt;p&gt;西鳥羽二郎さん (&lt;a href=&#34;https://twitter.com/jnishi&#34;&gt;@jnishi&lt;/a&gt;
) / Preferred Infrastructure&lt;/p&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/z0e4lCCp712kue&#34; class=&#34;slideshare&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;


&lt;p&gt;論文: &lt;a href=&#34;http://jmlr.org/proceedings/papers/v48/amodei16.html&#34;&gt;http://jmlr.org/proceedings/papers/v48/amodei16.html&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech–two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, enabling experiments that previously took weeks to now run in days. This allows us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;End to Endの音声認識モデルを構築し、人手による書き起こしよりも高い精度を得た&lt;/li&gt;
&lt;li&gt;異なる言語や、雑音のあるなしにも対応可能なモデルとなった&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://twitter.com/hashtag/icml_yomi?src=hash&#34;&gt;#icml_yomi&lt;/a&gt; これ試そ / “WING » Downloads » Chinese Microtext Dataset” &lt;a href=&#34;https://t.co/tnkejvjfZ2&#34;&gt;https://t.co/tnkejvjfZ2&lt;/a&gt;&lt;/p&gt;&amp;mdash; prototechno (@prototechno) &lt;a href=&#34;https://twitter.com/prototechno/status/756065132116008961&#34;&gt;July 21, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;

&lt;p&gt;まとめは以上となります。&lt;br /&gt;
貴重な場を提供していただいた関係者の方々、ありがとうございました。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>R-env:連舞® ハンズオンで RoBoHoN（ロボホン）・ Sota（ソータ）と戯れてきた</title>
      <link>http://blog.algolab.jp/post/2016/07/14/r-env-hands-on/</link>
      <pubDate>Thu, 14 Jul 2016 17:40:56 +0900</pubDate>
      
      <guid>http://blog.algolab.jp/post/2016/07/14/r-env-hands-on/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://r-env.doorkeeper.jp/events/48273&#34;&gt;[R-env:連舞® Innovation Hub] R-env:連舞®×RoBoHoNでロボットサービス開発体験ハンズオン&lt;/a&gt; に参加してきましたので、その模様をレポートします。&lt;/p&gt;

&lt;h2 id=&#34;r-env-連舞-とは&#34;&gt;R-env:連舞® とは&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/14/r-env-hands-on//r-env.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;NTTが開発を行っている、様々なロボットやデバイスと連携し、 ビジュアルプログラミング環境を用いて誰でも簡単に開発を行うことのできるプラットフォームです。&lt;/p&gt;

&lt;h2 id=&#34;robohon-とは&#34;&gt;RoBoHoN とは&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/14/r-env-hands-on//robohon.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;2016/06/29にSDKが一般公開されたのが記憶に新しいですが、SHARPが開発しているロボット電話です。&lt;br /&gt;
Androidをベースとしており、標準機能の他に、HVML (Hyper Voice Markup Language) という独自の言語を用いてシナリオを作成することができるようです。&lt;br /&gt;
SDKは &lt;a href=&#34;https://robohon.com/&#34;&gt;公式ページ&lt;/a&gt; 内のマイページよりダウンロード可能です。&lt;/p&gt;

&lt;h2 id=&#34;r-env-と-robohon-の連携&#34;&gt;R-env と RoBoHoN の連携&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/14/r-env-hands-on//r-env-robohon.jpg&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;R-envとRoBoHoNなどの機器の間でWebSocketによる通信を行うことでインタラクションを実現しています。&lt;br /&gt;
上記の図は、R-envからRoBoHonに処理を依頼する例ですが、RoBoHoN側からイベント (持ち上げられた、など) をR-envへ通知することができます。&lt;br /&gt;
また、R-envには幾つもの機器を登録することができるので、例えばRoBoHoNに「エアコンつけて！」と頼むとエアコンが起動する、といったことも実現することが可能です。&lt;/p&gt;

&lt;h2 id=&#34;ハンズオン&#34;&gt;ハンズオン&lt;/h2&gt;

&lt;p&gt;今回のハンズオンでは、R-envおよびRoBoHoNの紹介の後、30分程度の自由な開発時間がありました。&lt;br /&gt;
Sotaも動かしても良い、とのことだったので、RoBoHonとSotaに簡単な会話をさせてみることにしました。&lt;/p&gt;

&lt;h2 id=&#34;シナリオを作成する&#34;&gt;シナリオを作成する&lt;/h2&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/14/r-env-hands-on//screen_1.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;GUIでフローを定義していく形となります。&lt;br /&gt;
ボックスにイベントの定義、矢印にイベント遷移条件の定義を行います。&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://blog.algolab.jp/images//post/2016/07/14/r-env-hands-on//screen_2.png&#34;/&gt;
  
&lt;/figure&gt;


&lt;p&gt;イベントの詳細を定義する画面です。&lt;br /&gt;
ここでは、RoBoHonのプロジェクタを起動するイベントを定義しています。&lt;/p&gt;

&lt;h2 id=&#34;動いた&#34;&gt;動いた！&lt;/h2&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/A-ArkARJUpQ&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34; allowfullscreen frameborder=&#34;0&#34;&gt;&lt;/iframe&gt;
 &lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;RoBoHoN:&lt;/strong&gt; はじめまして、ロボホンです。&lt;br /&gt;
&lt;strong&gt;Sota:&lt;/strong&gt; はじめまして、ソータです。ロボホン、プロジェクタ写して！&lt;br /&gt;
&lt;strong&gt;RoBoHoN:&lt;/strong&gt; （プロジェクタを写す）&lt;br /&gt;
&lt;strong&gt;Sota:&lt;/strong&gt; ありがとう！僕からは音楽を流すね！（音楽を流す）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;最終的に上記のようなシナリオが完成しました！リアルにモノが動くので、感動もひとしおです！！&lt;/p&gt;

&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;

&lt;p&gt;手軽にロボットプログラミングが体験でき、非常に貴重な経験となりました。&lt;br /&gt;
ハンズオンは定期的に開催しているようなので、みなさんも体験してみてはいかがでしょうか？&lt;br /&gt;
&lt;a href=&#34;https://r-env.doorkeeper.jp/&#34;&gt;R-env:連舞® Innovation Hub&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>